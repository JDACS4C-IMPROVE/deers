{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEERS training 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn-r1AC_Xz8w"
      },
      "source": [
        "# Evaluate single model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc4oxJwfX6Vr"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLJuaf-toZHq"
      },
      "source": [
        "#### Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-pGQLu0XuZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e7a7a2-913e-4b1e-a250-2bdffbe99428"
      },
      "source": [
        "# Install packages\n",
        "!pip install ray[rllib]==0.8.1  # also recommended: ray[debug]\n",
        "!pip uninstall -y pyarrow\n",
        "!pip uninstall -y pickle5\n",
        "!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ray[rllib]==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/37/0e9877a2729d31881d9bb2cad1f9aedd2f451602af67706df6faaef33e7f/ray-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (74.3MB)\n",
            "\u001b[K     |████████████████████████████████| 74.3MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (20.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.0.12)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.6.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.13)\n",
            "Collecting redis>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/7c/24fb0511df653cf1a5d938d8f5d19802a88cef255706fdda242ff97e91b7/redis-3.5.3-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.15.0)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (3.12.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (2.6.0)\n",
            "Requirement already satisfied: gym[atari]; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (0.17.3)\n",
            "Requirement already satisfied: tabulate; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (0.8.9)\n",
            "Collecting lz4; extra == \"rllib\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/52/151c815a486290608e4dc6699a0cfd74141dc5191f8fe928e7d1b28b569e/lz4-3.1.3-cp37-cp37m-manylinux2010_x86_64.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 30.1MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless; extra == \"rllib\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/35/bfc76533f2274cd3da4e2cf255cd13ab9d7f6fc8990c06911e7f8fcc2130/opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy; extra == \"rllib\" in /usr/local/lib/python3.7/dist-packages (from ray[rllib]==0.8.1) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ray[rllib]==0.8.1) (2.4.7)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (57.0.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (8.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (1.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->ray[rllib]==0.8.1) (21.2.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.1) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.1) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.7/dist-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.1) (0.2.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.1) (0.16.0)\n",
            "Installing collected packages: redis, funcsigs, colorama, lz4, opencv-python-headless, ray\n",
            "Successfully installed colorama-0.4.4 funcsigs-1.0.2 lz4-3.1.3 opencv-python-headless-4.5.2.54 ray-0.8.1 redis-3.5.3\n",
            "Uninstalling pyarrow-3.0.0:\n",
            "  Successfully uninstalled pyarrow-3.0.0\n",
            "\u001b[33mWARNING: Skipping pickle5 as it is not installed.\u001b[0m\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch===1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 22kB/s \n",
            "\u001b[?25hCollecting torchvision===0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision===0.5.0) (1.15.0)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMdqSUw3nZhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bbce34-d06f-4e30-a03e-246acfa30f3e"
      },
      "source": [
        "# Mount Google Drive if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3OpK8f6ohhv"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A03zOOurnjGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9311934c-22eb-448f-82fd-848566caac13"
      },
      "source": [
        "# Setup some constants\n",
        "project_dir = \"...\"\n",
        "global_response_metric = \"LN_IC50\"\n",
        "# Standard imports\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune import track\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import dill\n",
        "import warnings\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "# Custom utilities imports\n",
        "sys.path.append(project_dir + \"Scripts/Modules\")\n",
        "from modeling import Dataset\n",
        "\n",
        "print(ray.__version__)\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.1\n",
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFxBzuQXnlYA"
      },
      "source": [
        "## Network definitions and helper classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p5SOjHAnnlP"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "\n",
        "# Network definitions\n",
        "# Linear model\n",
        "class LinearMatrixFactorizationWithFeatures(torch.nn.Module):\n",
        "    def __init__(self, drug_input_dim, cell_line_input_dim, output_dim, \n",
        "                 out_activation_func=None,\n",
        "                 drug_bias=True,\n",
        "                 cell_line_bias=True):\n",
        "        super(LinearMatrixFactorizationWithFeatures, self).__init__()\n",
        "        self.drug_linear = torch.nn.Linear(drug_input_dim, output_dim, bias=drug_bias)\n",
        "        self.cell_line_linear = torch.nn.Linear(cell_line_input_dim, output_dim, bias=cell_line_bias)\n",
        "        self.out_activation = out_activation_func\n",
        "        \n",
        "    def forward(self, drug_features, cell_line_features):\n",
        "        drug_outputs = self.drug_linear(drug_features)\n",
        "        cell_line_outputs = self.cell_line_linear(cell_line_features)\n",
        "        \n",
        "        final_outputs = torch.sum(torch.mul(drug_outputs, cell_line_outputs), dim=1).view(-1, 1)\n",
        "        if self.out_activation:\n",
        "            return self.out_activation(final_outputs)\n",
        "        return final_outputs\n",
        "    \n",
        "# Deep autoencoder with one hidden layer\n",
        "class DeepAutoencoderOneHiddenLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, code_dim, activation_func=nn.ReLU, \n",
        "                 code_activation=True, dropout=False, dropout_rate=0.5):\n",
        "        super(DeepAutoencoderOneHiddenLayer, self).__init__()\n",
        "        # Establish encoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(input_dim, hidden_dim))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim, code_dim))\n",
        "        if code_activation:\n",
        "            modules.append(activation_func())\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        # Establish decoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(code_dim, hidden_dim))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        code = x\n",
        "        x = self.decoder(x)\n",
        "        return code, x\n",
        "    \n",
        "# Deep autoencoder with two hidden layers\n",
        "class DeepAutoencoderTwoHiddenLayers(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, code_dim, activation_func=nn.ReLU,\n",
        "                 code_activation=True, dropout=False, dropout_rate=0.5):\n",
        "        super(DeepAutoencoderTwoHiddenLayers, self).__init__()\n",
        "        # Establish encoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(input_dim, hidden_dim1))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim1, hidden_dim2))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim2, code_dim))\n",
        "        if code_activation:\n",
        "            modules.append(activation_func())\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        \n",
        "        # Establish decoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(code_dim, hidden_dim2))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim2, hidden_dim1))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim1, input_dim))\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        code = x\n",
        "        x = self.decoder(x)\n",
        "        return code, x\n",
        "    \n",
        "# Deep autoencoder with three hidden layers\n",
        "class DeepAutoencoderThreeHiddenLayers(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, code_dim, activation_func=nn.ReLU,\n",
        "                 code_activation=True, dropout=False, dropout_rate=0.5):\n",
        "        super(DeepAutoencoderThreeHiddenLayers, self).__init__()\n",
        "        # Establish encoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(input_dim, hidden_dim1))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim1, hidden_dim2))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim2, hidden_dim3))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim3, code_dim))\n",
        "        if code_activation:\n",
        "            modules.append(activation_func())\n",
        "        self.encoder = nn.Sequential(*modules)\n",
        "        \n",
        "        # Establish decoder\n",
        "        modules = []\n",
        "        modules.append(nn.Linear(code_dim, hidden_dim3))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim3, hidden_dim2))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim2, hidden_dim1))\n",
        "        modules.append(activation_func())\n",
        "        if dropout:\n",
        "            modules.append(nn.Dropout(dropout_rate))\n",
        "        modules.append(nn.Linear(hidden_dim1, input_dim))\n",
        "        self.decoder = nn.Sequential(*modules)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        code = x\n",
        "        x = self.decoder(x)\n",
        "        return code, x\n",
        "\n",
        "class ForwardNetworkOneHiddenLayer(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, activation_func=nn.ReLU,\n",
        "                out_activation=None):\n",
        "        super(ForwardNetworkOneHiddenLayer, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim1),\n",
        "            activation_func(),\n",
        "            nn.Linear(hidden_dim1, 1))\n",
        "        self.out_activation = out_activation\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.out_activation:\n",
        "            return self.out_activation(self.layers(x))\n",
        "        else:\n",
        "            return self.layers(x)\n",
        "\n",
        "# Rec system with incorporated autoencoders\n",
        "class RecSystemWithAutoencoders(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 drug_autoencoder,\n",
        "                 cell_line_autoencoder,\n",
        "                 out_activation=None):\n",
        "        \n",
        "        super(RecSystemWithAutoencoders, self).__init__()\n",
        "        self.drug_autoencoder = drug_autoencoder\n",
        "        self.cell_line_autoencoder = cell_line_autoencoder\n",
        "        self.out_activation = out_activation\n",
        "        \n",
        "    def forward(self, drug_features, cell_line_features):\n",
        "        drug_code, drug_reconstruction = self.drug_autoencoder(drug_features)\n",
        "        cell_line_code, cell_line_reconstruction = self.cell_line_autoencoder(cell_line_features)\n",
        "        \n",
        "        final_outputs = torch.sum(torch.mul(drug_code, cell_line_code), dim=1).view(-1, 1)\n",
        "        if self.out_activation:\n",
        "            return self.out_activation(final_outputs), drug_reconstruction, cell_line_reconstruction\n",
        "        return final_outputs, drug_reconstruction, cell_line_reconstruction\n",
        "\n",
        "class ForwardLinearRegression(torch.nn.Module):\n",
        "    def __init__(self, input_dim, out_activation=None):\n",
        "        super(ForwardLinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.out_activation = out_activation\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.out_activation:\n",
        "            return self.out_activation(self.linear(x))\n",
        "        return self.linear(x)\n",
        "\n",
        "class ForwardNetworkTwoHiddenLayers(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, activation_func=nn.ReLU,\n",
        "                out_activation=None, dropout_rate=0.0):\n",
        "        super(ForwardNetworkTwoHiddenLayers, self).__init__()\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "             nn.Linear(input_dim, hidden_dim1),\n",
        "             activation_func(),\n",
        "             nn.Dropout(dropout_rate),\n",
        "             nn.Linear(hidden_dim1, hidden_dim2),\n",
        "             activation_func(),\n",
        "             nn.Linear(hidden_dim2, 1))\n",
        "        \n",
        "        self.out_activation = out_activation\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.out_activation:\n",
        "            return self.out_activation(self.layers(x))\n",
        "        else:\n",
        "            return self.layers(x)\n",
        "\n",
        "class RecSystemCodeConcatenation(torch.nn.Module):\n",
        "    def __init__(self, drug_autoencoder, cell_line_autoencoder, \n",
        "                 forward_network, \n",
        "                 code_interactions=False):\n",
        "        super(RecSystemCodeConcatenation, self).__init__()\n",
        "        self.drug_autoencoder = drug_autoencoder\n",
        "        self.cell_line_autoencoder = cell_line_autoencoder\n",
        "        self.forward_network = forward_network\n",
        "        self.code_interactions = code_interactions\n",
        "        \n",
        "    def forward(self, drug_features, cell_line_features):\n",
        "        drug_code, drug_reconstruction = self.drug_autoencoder(drug_features)\n",
        "        cell_line_code, cell_line_reconstruction = self.cell_line_autoencoder(cell_line_features)\n",
        "                \n",
        "        if self.code_interactions:\n",
        "            drug_code_t = drug_code.view(drug_code.shape[0], drug_code.shape[1], 1)\n",
        "            cell_line_code_t = cell_line_code.view(cell_line_code.shape[0], 1, cell_line_code.shape[1])\n",
        "            x = torch.bmm(drug_code_t, cell_line_code_t)\n",
        "            x = x.view(cell_line_code.shape[0], x.shape[1] * x.shape[2])\n",
        "            x = torch.cat((drug_code, cell_line_code, x), axis=1)\n",
        "            return self.forward_network(x), drug_reconstruction, cell_line_reconstruction\n",
        "\n",
        "        else:\n",
        "            # Concatenate codes without interactions\n",
        "            x = torch.cat((drug_code, cell_line_code), axis=1)\n",
        "            return self.forward_network(x), drug_reconstruction, cell_line_reconstruction\n",
        "\n",
        "### MODEL CLASSES\n",
        "class Model:\n",
        "    \"\"\"Wrapper around PyTorch model.\n",
        "\n",
        "    Contains helper functions for training and evaluating the underlying network.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, network):\n",
        "        \"\"\"Instance initializer.\n",
        "\n",
        "        Args:\n",
        "            name (str): Custom name of the model.\n",
        "            network (PyTorch model): Underlying PyTorch model.\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.network = network\n",
        "        \n",
        "    def train(self, train_samples, cell_line_features, drug_features,\n",
        "             batch_size, optimizer, criterion, reg_lambda=0, log=True, response_metric=\"AUC\"):\n",
        "        \"\"\"Perform one epoch of training of the underlying network.\n",
        "\n",
        "        Args:\n",
        "            train_samples (DataFrame): Table containing drug-cell line training pairs and corresponding response metric.\n",
        "            cell_line_features (DataFrame): Cell line features data.\n",
        "            drug_features (DataFrame): Drug features data.\n",
        "            batch_size (int): Batch size.\n",
        "            optimizer (PyTorch optimizer): Optimizer to use.\n",
        "            criterion (PyTorch cost function): Cost function to optimize.\n",
        "            reg_lambda (float): Weight of the L2 regularization, defaults to 0.\n",
        "            log (bool): If to print some information during training, defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            loss (float): Value of the loss drug response loss after one epoch of training.\n",
        "\n",
        "        \"\"\"\n",
        "        no_batches = train_samples.shape[0] // batch_size + 1\n",
        "        # Establish the device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if log:\n",
        "            print(device)\n",
        "        # Move the network into device\n",
        "        self.network.to(device)\n",
        "        # Training the model\n",
        "        self.network.train()\n",
        "        for batch in range(no_batches):\n",
        "            # Separate response variable batch\n",
        "            if batch != no_batches:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
        "            else:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:]\n",
        "\n",
        "            # Extract output variable batch\n",
        "            y_batch = torch.from_numpy(samples_batch[response_metric].values).view(-1, 1).to(device)\n",
        "\n",
        "            # Extract cell lines IDs for which data shall be extracted\n",
        "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
        "            # Extract corresponding cell line data\n",
        "            cell_line_input_batch = cell_line_features.loc[cl_ids].values\n",
        "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch).to(device)\n",
        "\n",
        "            # Extract drug IDs for which data shall be extracted\n",
        "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
        "            # Extract corresponding drug data\n",
        "            drug_input_batch = drug_features.loc[drug_ids].values\n",
        "            drug_input_batch = torch.from_numpy(drug_input_batch).to(device)\n",
        "\n",
        "            # Clear gradient buffers because we don't want to accummulate gradients \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            batch_output = self.network(drug_input_batch.float(), cell_line_input_batch.float())\n",
        "\n",
        "            # L2 regularization\n",
        "            reg_sum = 0\n",
        "            for param in self.network.parameters():\n",
        "                reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
        "\n",
        "            # Compute the loss for this batch\n",
        "            loss = criterion(batch_output, y_batch.float()) + reg_lambda * reg_sum\n",
        "            # Get the gradients w.r.t. the parameters\n",
        "            loss.backward()\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "        return loss\n",
        "    \n",
        "    def predict(self, samples, cell_line_features, drug_features, response_metric=\"AUC\"):\n",
        "        \"\"\"Predict response for a given set of samples.\n",
        "\n",
        "        Args:\n",
        "            samples (DataFrame): Table containing drug-cell line pairs and corresponding response metric.\n",
        "            cell_line_features (DataFrame): Cell line features data.\n",
        "            drug_features (DataFrame): Drug features data.\n",
        "\n",
        "        Returns:\n",
        "            predicted (torch.Tensor): Model's predictions for provided samples.\n",
        "            y_true (np.array): True response values for provided samples.\n",
        "        \"\"\"\n",
        "        # Establish the device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # Extract true target values\n",
        "        y_true = samples[response_metric].values\n",
        "\n",
        "        cl_input = cell_line_features.loc[samples[\"COSMIC_ID\"].values].values\n",
        "        drug_input = drug_features.loc[samples[\"DRUG_ID\"].values].values\n",
        "\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            predicted = self.network(torch.from_numpy(drug_input).to(device).float(), \n",
        "                             torch.from_numpy(cl_input).to(device).float())\n",
        "        return predicted, y_true\n",
        "    \n",
        "    @staticmethod\n",
        "    def per_drug_performance_df(samples, predicted, mean_training_auc=None, response_metric=\"AUC\"):\n",
        "        \"\"\"Compute evaluation metrics per drug and return them in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            samples (DataFrame): Table containing drug-cell line pairs and corresponding response metric.\n",
        "            predicted (torch.Tensor): Model's predictions for considered samples.\n",
        "            mean_training_auc (float): Mean of drug-response in training data for calculating dummy values,\n",
        "                defaults to None. If None, mean of true AUC for a given drug is considered, resulting in\n",
        "                dummy RMSE being the standard deviation of the AUC for a given drug.\n",
        "\n",
        "        Returns:\n",
        "            performance_per_drug (DataFrame): Table containing per-drug model and dummy performance metrics.\n",
        "        \"\"\"\n",
        "        sample_with_predictions = samples.copy()\n",
        "        sample_with_predictions[\"Predicted \" + str(response_metric)] = predicted.numpy()\n",
        "\n",
        "        drugs = []\n",
        "        model_corrs = []\n",
        "        model_rmses = []\n",
        "        dummy_corrs = []\n",
        "        dummy_rmses = []\n",
        "        no_samples = []\n",
        "\n",
        "        for drug in sample_with_predictions.DRUG_ID.unique():\n",
        "            df = sample_with_predictions[sample_with_predictions.DRUG_ID == drug]\n",
        "            if df.shape[0] < 2:\n",
        "                continue\n",
        "            if mean_training_auc:\n",
        "                dummy_preds = [mean_training_auc] * df.shape[0]\n",
        "            else:\n",
        "                dummy_preds = [df[response_metric].mean()] * df.shape[0]\n",
        "            dummy_rmse = metrics.mean_squared_error(df[response_metric], dummy_preds) ** 0.5\n",
        "            dummy_corr = pearsonr(df[response_metric], dummy_preds)\n",
        "\n",
        "            try:\n",
        "                model_rmse = metrics.mean_squared_error(df[response_metric], df[\"Predicted \" + str(response_metric)]) ** 0.5\n",
        "                model_corr = pearsonr(df[response_metric], df[\"Predicted \" + str(response_metric)])\n",
        "            except ValueError:\n",
        "                model_rmse, model_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "            drugs.append(drug)\n",
        "            dummy_rmses.append(dummy_rmse)\n",
        "            dummy_corrs.append(dummy_corr[0])\n",
        "\n",
        "            model_rmses.append(model_rmse)\n",
        "            model_corrs.append(model_corr[0])\n",
        "\n",
        "            no_samples.append(df.COSMIC_ID.nunique())\n",
        "\n",
        "        performance_per_drug = pd.DataFrame()\n",
        "        performance_per_drug[\"Drug ID\"] = drugs\n",
        "        performance_per_drug[\"Model RMSE\"] = model_rmses\n",
        "        performance_per_drug[\"Model correlation\"] = model_corrs\n",
        "\n",
        "        performance_per_drug[\"Dummy RMSE\"] = dummy_rmses\n",
        "        performance_per_drug[\"Dummy correlation\"] = dummy_corrs\n",
        "        performance_per_drug[\"No. samples\"] = no_samples\n",
        "\n",
        "        return performance_per_drug\n",
        "\n",
        "    @staticmethod\n",
        "    def per_entity_performance_df(samples, predicted, entity_type=\"DRUG_ID\", mean_training_auc=None,\n",
        "                                 response_metric=\"AUC\"):\n",
        "        \"\"\"Compute evaluation metrics per entity (drug or cell line) and return them in a DataFrame.\n",
        "\n",
        "        Args:\n",
        "            samples (DataFrame): Table containing drug-cell line pairs and corresponding response metric.\n",
        "            predicted (torch.Tensor): Model's predictions for considered samples.\n",
        "            mean_training_auc (float): Mean of drug-response in training data for calculating dummy values,\n",
        "                defaults to None. If None, mean of true AUC for a given drug is considered, resulting in\n",
        "                dummy RMSE being the standard deviation of the AUC for a given drug.\n",
        "\n",
        "        Returns:\n",
        "            performance_per_entity (DataFrame): Table containing per-entity model and dummy performance metrics.\n",
        "        \"\"\"\n",
        "        sample_with_predictions = samples.copy()\n",
        "        sample_with_predictions[\"Predicted \" + str(response_metric)] = predicted.numpy()\n",
        "\n",
        "        entities = []\n",
        "        model_corrs = []\n",
        "        model_rmses = []\n",
        "        dummy_corrs = []\n",
        "        dummy_rmses = []\n",
        "        no_samples = []\n",
        "\n",
        "        for entity in sample_with_predictions[entity_type].unique():\n",
        "            df = sample_with_predictions[sample_with_predictions[entity_type] == entity]\n",
        "            if df.shape[0] < 2:\n",
        "                continue\n",
        "            if mean_training_auc:\n",
        "                dummy_preds = [mean_training_auc] * df.shape[0]\n",
        "            else:\n",
        "                dummy_preds = [df[response_metric].mean()] * df.shape[0]\n",
        "            dummy_rmse = metrics.mean_squared_error(df[response_metric], dummy_preds) ** 0.5\n",
        "            dummy_corr = pearsonr(df[response_metric], dummy_preds)\n",
        "\n",
        "            try:\n",
        "                model_rmse = metrics.mean_squared_error(df[response_metric], df[\"Predicted \" + str(response_metric)]) ** 0.5\n",
        "                model_corr = pearsonr(df[response_metric], df[\"Predicted \" + str(response_metric)])\n",
        "            except ValueError:\n",
        "                model_rmse, model_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "            entities.append(entity)\n",
        "            dummy_rmses.append(dummy_rmse)\n",
        "            dummy_corrs.append(dummy_corr[0])\n",
        "\n",
        "            model_rmses.append(model_rmse)\n",
        "            model_corrs.append(model_corr[0])\n",
        "\n",
        "            no_samples.append(df.shape[0])\n",
        "\n",
        "        performance_per_entity = pd.DataFrame()\n",
        "        performance_per_entity[entity_type] = entities\n",
        "        performance_per_entity[\"Model RMSE\"] = model_rmses\n",
        "        performance_per_entity[\"Model correlation\"] = model_corrs\n",
        "\n",
        "        performance_per_entity[\"Dummy RMSE\"] = dummy_rmses\n",
        "        performance_per_entity[\"Dummy correlation\"] = dummy_corrs\n",
        "        performance_per_entity[\"No. samples\"] = no_samples\n",
        "\n",
        "        return performance_per_entity\n",
        "        \n",
        "    @staticmethod\n",
        "    def evaluate_predictions(y_true, preds):\n",
        "        \"\"\"Compute RMSE and correlation with true values for model predictions.\"\"\"\n",
        "        return metrics.mean_squared_error(y_true, preds) ** 0.5, pearsonr(y_true, preds)\n",
        "    \n",
        "       \n",
        "\n",
        "    \n",
        "class ModelWithAutoencoders(Model):\n",
        "    \"\"\" Wrapper around PyTorch model involving autoencoders.\n",
        "\n",
        "    Inherits from Model class. Train and predict methods are adjusted for optimizing\n",
        "    drug sensitivity predictions as well as drug and cell line reconstructions.\n",
        "\n",
        "    \"\"\"\n",
        "    def train(self, train_samples, cell_line_features, drug_features,\n",
        "             batch_size, optimizer, criterion, reconstruction_term_drug=0.0,\n",
        "              reconstruction_term_cl=0.0, reg_lambda=0.0, log=True, response_metric=\"AUC\"):\n",
        "        \"\"\"Perform one epoch of training of the underlying network with autoencoders.\n",
        "\n",
        "        Rather than only drug-reponse prediction losss, also optimize for difference in drug and cell line\n",
        "        input data and their corresponding reconstructions.\n",
        "\n",
        "        Args:\n",
        "            train_samples (DataFrame): Table containing drug-cell line training pairs and corresponding response metric.\n",
        "            cell_line_features (DataFrame): Cell line features data.\n",
        "            drug_features (DataFrame): Drug features data.\n",
        "            batch_size (int): Batch size.\n",
        "            optimizer (PyTorch optimizer): Optimizer to use.\n",
        "            criterion (PyTorch cost function): Cost function to optimize.\n",
        "            reconstruction_term_drug (float): Weight of reconstruction of input data in\n",
        "                drug autoencoder, defaults to 0.\n",
        "            reconstruction_term_cl (float): Weight of reconstruction of input data in\n",
        "                cell line autoencoder, defaults to 0.\n",
        "            reg_lambda (float): Weight of the L2 regularization, defaults to 0.\n",
        "            log (bool): If to print some information during training, defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            loss (float): Value of the loss drug response loss after one epoch of training.\n",
        "            drug_recounstruction_loss (float): Loss between drug input and drug reconstruction.\n",
        "            cl_reconstruction_loss (float): Loss between cell line input and cell line reconstruction.\n",
        "\n",
        "        \"\"\"\n",
        "        no_batches = train_samples.shape[0] // batch_size + 1\n",
        "        # Establish the device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if log:\n",
        "          print(device)\n",
        "        # Move the network into device\n",
        "        self.network.to(device)\n",
        "        # Training the model\n",
        "        self.network.train()\n",
        "        for batch in range(no_batches):\n",
        "            # Separate response variable batch\n",
        "            if batch != no_batches:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
        "            else:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:]\n",
        "\n",
        "            # Extract output variable batch\n",
        "            y_batch = torch.from_numpy(samples_batch[response_metric].values).view(-1, 1).to(device)\n",
        "\n",
        "            # Extract cell lines IDs for which data shall be extracted\n",
        "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
        "            # Extract corresponding cell line data\n",
        "            cell_line_input_batch = cell_line_features.loc[cl_ids].values\n",
        "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch).to(device)\n",
        "\n",
        "            # Extract drug IDs for which data shall be extracted\n",
        "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
        "            # Extract corresponding drug data\n",
        "            drug_input_batch = drug_features.loc[drug_ids].values\n",
        "            drug_input_batch = torch.from_numpy(drug_input_batch).to(device)\n",
        "\n",
        "            # Clear gradient buffers because we don't want to accummulate gradients \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            batch_output, batch_drug_reconstruction, batch_cl_reconstruction = self.network(\n",
        "                drug_input_batch.float(), cell_line_input_batch.float())\n",
        "\n",
        "            # L2 regularization\n",
        "            reg_sum = 0\n",
        "            for param in self.network.parameters():\n",
        "                reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
        "\n",
        "            # Compute the loss for this batch, including the drug and cell line reconstruction losses\n",
        "            output_loss = criterion(batch_output, y_batch.float()) + reg_lambda * reg_sum\n",
        "            drug_recounstruction_loss = criterion(batch_drug_reconstruction, drug_input_batch.float())\n",
        "            cl_reconstruction_loss = criterion(batch_cl_reconstruction, cell_line_input_batch.float())\n",
        "\n",
        "            # Combine the losses in the final cost function\n",
        "            loss = output_loss + reconstruction_term_drug * drug_recounstruction_loss + reconstruction_term_cl * cl_reconstruction_loss\n",
        "            # Get the gradients w.r.t. the parameters\n",
        "            loss.backward()\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "        return loss, drug_recounstruction_loss, cl_reconstruction_loss\n",
        "    \n",
        "    def predict(self, samples, cell_line_features, drug_features, response_metric=\"AUC\"):\n",
        "        \"\"\"Predict response along with drug anc cell line reconstructions for a given set of samples.\n",
        "\n",
        "        Args:\n",
        "            samples (DataFrame): Table containing drug-cell line pairs and corresponding response metric.\n",
        "            cell_line_features (DataFrame): Cell line features data.\n",
        "            drug_features (DataFrame): Drug features data.\n",
        "\n",
        "        Returns:\n",
        "            predicted (torch.Tensor): Model's predictions for provided samples.\n",
        "            y_true (np.array): True response values for provided samples.\n",
        "            drug_input (np.array): Drug input data for provided samples.\n",
        "            cl_input (np.array): Cell line input data for provided samples.\n",
        "\n",
        "        \"\"\"\n",
        "        # Establish the device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        y_true = samples[response_metric].values\n",
        "\n",
        "        cl_input = cell_line_features.loc[samples[\"COSMIC_ID\"].values].values\n",
        "        drug_input = drug_features.loc[samples[\"DRUG_ID\"].values].values\n",
        "\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            predicted = self.network(torch.from_numpy(drug_input).to(device).float(), \n",
        "                             torch.from_numpy(cl_input).to(device).float())\n",
        "        return predicted, y_true, drug_input, cl_input\n",
        "        \n",
        "    def train_with_independence_penalty(self, train_samples, cell_line_features, drug_features,\n",
        "             batch_size, optimizer, criterion, reconstruction_term_drug=0.0,\n",
        "              reconstruction_term_cl=0.0, independence_term_drug=0.0, independence_term_cl=0.0, reg_lambda=0.0, log=True,\n",
        "              response_metric=\"AUC\"):\n",
        "        \"\"\"Perform one epoch of training of the underlying network with autoencoders.\n",
        "\n",
        "        Rather than only drug-reponse prediction losss, also optimize for difference in drug and cell line\n",
        "        input data and their corresponding reconstructions.\n",
        "\n",
        "        Args:\n",
        "            train_samples (DataFrame): Table containing drug-cell line training pairs and corresponding response metric.\n",
        "            cell_line_features (DataFrame): Cell line features data.\n",
        "            drug_features (DataFrame): Drug features data.\n",
        "            batch_size (int): Batch size.\n",
        "            optimizer (PyTorch optimizer): Optimizer to use.\n",
        "            criterion (PyTorch cost function): Cost function to optimize.\n",
        "            reconstruction_term_drug (float): Weight of reconstruction of input data in\n",
        "                drug autoencoder, defaults to 0.\n",
        "            reconstruction_term_cl (float): Weight of reconstruction of input data in\n",
        "                cell line autoencoder, defaults to 0.\n",
        "            reg_lambda (float): Weight of the L2 regularization, defaults to 0.\n",
        "            log (bool): If to print some information during training, defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            loss (float): Value of the loss drug response loss after one epoch of training.\n",
        "            drug_recounstruction_loss (float): Loss between drug input and drug reconstruction.\n",
        "            cl_reconstruction_loss (float): Loss between cell line input and cell line reconstruction.\n",
        "\n",
        "        \"\"\"\n",
        "        no_batches = train_samples.shape[0] // batch_size + 1\n",
        "        # Establish the device\n",
        "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if log:\n",
        "          print(device)\n",
        "        # Move the network into device\n",
        "        self.network.to(device)\n",
        "        # Training the model\n",
        "        self.network.train()\n",
        "        for batch in range(no_batches):\n",
        "            # Separate response variable batch\n",
        "            if batch != no_batches:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
        "            else:\n",
        "                samples_batch = train_samples.iloc[batch * batch_size:]\n",
        "\n",
        "            # Extract output variable batch\n",
        "            y_batch = torch.from_numpy(samples_batch[response_metric].values).view(-1, 1).to(device)\n",
        "\n",
        "            # Extract cell lines IDs for which data shall be extracted\n",
        "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
        "            # Extract corresponding cell line data\n",
        "            cell_line_input_batch = cell_line_features.loc[cl_ids].values\n",
        "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch).to(device)\n",
        "\n",
        "            # Extract drug IDs for which data shall be extracted\n",
        "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
        "            # Extract corresponding drug data\n",
        "            drug_input_batch = drug_features.loc[drug_ids].values\n",
        "            drug_input_batch = torch.from_numpy(drug_input_batch).to(device)\n",
        "\n",
        "            # Clear gradient buffers because we don't want to accummulate gradients \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Perform forward pass\n",
        "            batch_output, batch_drug_reconstruction, batch_cl_reconstruction = self.network(\n",
        "                drug_input_batch.float(), cell_line_input_batch.float())\n",
        "\n",
        "            # L2 regularization\n",
        "            reg_sum = 0\n",
        "            for param in self.network.parameters():\n",
        "                reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
        "\n",
        "            # Compute the loss for this batch, including the drug and cell line reconstruction losses\n",
        "            output_loss = criterion(batch_output, y_batch.float()) + reg_lambda * reg_sum\n",
        "            drug_recounstruction_loss = criterion(batch_drug_reconstruction, drug_input_batch.float())\n",
        "            cl_reconstruction_loss = criterion(batch_cl_reconstruction, cell_line_input_batch.float())\n",
        "\n",
        "            # Compute independence loss\n",
        "            # Covariance matrices\n",
        "            t0 = time.time()\n",
        "            drug_codes_batch = self.network.drug_autoencoder.encoder(drug_input_batch.float())\n",
        "            cl_codes_batch = self.network.cell_line_autoencoder.encoder(cell_line_input_batch.float())\n",
        "            \n",
        "            drug_cov = self.__class__.covariance_matrix_torch(drug_codes_batch)\n",
        "            cl_cov = self.__class__.covariance_matrix_torch(cl_codes_batch)\n",
        "      \n",
        "            drug_independence_loss = 0\n",
        "            cl_independence_loss = 0\n",
        "            drug_independence_loss = (drug_cov * drug_cov).sum() - torch.trace(drug_cov * drug_cov)\n",
        "            cl_independence_loss = (cl_cov * cl_cov).sum() - torch.trace(cl_cov * cl_cov)\n",
        "\n",
        "            # Combine the losses in the final cost function\n",
        "            loss = output_loss + reconstruction_term_drug * drug_recounstruction_loss + \\\n",
        "                    reconstruction_term_cl * cl_reconstruction_loss + \\\n",
        "                    independence_term_drug * drug_independence_loss + independence_term_cl * cl_independence_loss\n",
        "            # Get the gradients w.r.t. the parameters\n",
        "            loss.backward()\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "        return loss, drug_recounstruction_loss, cl_reconstruction_loss, drug_independence_loss, cl_independence_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def covariance_matrix_torch(m, rowvar=False):\n",
        "        '''Estimate a covariance matrix given data.\n",
        "\n",
        "        Covariance indicates the level to which two variables vary together.\n",
        "        If we examine N-dimensional samples, `X = [x_1, x_2, ... x_N]^T`,\n",
        "        then the covariance matrix element `C_{ij}` is the covariance of\n",
        "        `x_i` and `x_j`. The element `C_{ii}` is the variance of `x_i`.\n",
        "\n",
        "        Args:\n",
        "            m: A 1-D or 2-D array containing multiple variables and observations.\n",
        "                Each row of `m` represents a variable, and each column a single\n",
        "                observation of all those variables.\n",
        "            rowvar: If `rowvar` is True, then each row represents a\n",
        "                variable, with observations in the columns. Otherwise, the\n",
        "                relationship is transposed: each column represents a variable,\n",
        "                while the rows contain observations.\n",
        "\n",
        "        Returns:\n",
        "            The covariance matrix of the variables.\n",
        "        '''\n",
        "        if m.dim() > 2:\n",
        "            raise ValueError('m has more than 2 dimensions')\n",
        "        if m.dim() < 2:\n",
        "            m = m.view(1, -1)\n",
        "        if not rowvar and m.size(0) != 1:\n",
        "            m = m.t()\n",
        "        # m = m.type(torch.double)  # uncomment this line if desired\n",
        "        fact = 1.0 / (m.size(1) - 1)\n",
        "        m -= torch.mean(m, dim=1, keepdim=True)\n",
        "        mt = m.t()  # if complex: mt = m.t().conj()\n",
        "        return fact * m.matmul(mt).squeeze()\n",
        "\n",
        "def min_max_series(s, minimum=None, maximum=None):\n",
        "    \"\"\"Perform min-max scaling on a one-dimensional Series or array.\"\"\"\n",
        "    if minimum and maximum:\n",
        "        return (s - minimum) / (maximum - minimum)\n",
        "    return (s - s.min()) / (s.max() - s.min())\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "def instantiate_system(specs, state_dict=None):\n",
        "        \"\"\"Create a recommender system in accordance with provided specs.\"\"\"\n",
        "        # Linear model case\n",
        "        if specs[\"architecture_type\"] == \"linear\":\n",
        "            # Establish out activation\n",
        "            network = LinearMatrixFactorizationWithFeatures(specs[\"drug_dim\"],\n",
        "                                        specs[\"cell_line_dim\"], specs[\"code_dim\"],\n",
        "                                        out_activation_func=specs[\"out_activation\"],\n",
        "                                        drug_bias=specs[\"drug_bias\"],\n",
        "                                        cell_line_bias=specs[\"cell_line_bias\"])\n",
        "        # Autoencoders case\n",
        "        elif \"autoencoder\" in specs[\"architecture_type\"].lower():\n",
        "            if specs[\"num_layers\"] == 1:\n",
        "                # Establish autoencoders\n",
        "                drug_autoencoder = DeepAutoencoderOneHiddenLayer(specs[\"drug_dim\"],\n",
        "                                                    specs[\"drug_hidden_dim1\"], specs[\"code_dim\"],\n",
        "                                                    activation_func=specs[\"activation_func\"],\n",
        "                                                    code_activation=specs[\"code_activation\"],\n",
        "                                                    dropout=specs[\"dropout\"],\n",
        "                                                    dropout_rate=specs[\"dropout_rate\"])\n",
        "                cell_line_autoencoder = DeepAutoencoderOneHiddenLayer(specs[\"cell_line_dim\"],\n",
        "                                                    specs[\"cell_line_hidden_dim1\"], specs[\"code_dim\"],\n",
        "                                                    activation_func=specs[\"activation_func\"],\n",
        "                                                    code_activation=specs[\"code_activation\"],\n",
        "                                                    dropout=specs[\"dropout\"],\n",
        "                                                    dropout_rate=specs[\"dropout_rate\"])\n",
        "        \n",
        "            elif specs[\"num_layers\"] == 2:\n",
        "                # Setup autoencoders\n",
        "                drug_autoencoder = DeepAutoencoderTwoHiddenLayers(specs[\"drug_dim\"],\n",
        "                                                 specs[\"drug_hidden_dim1\"],\n",
        "                                                 specs[\"drug_hidden_dim2\"],\n",
        "                                                 specs[\"code_dim\"],\n",
        "                                                 activation_func=specs[\"activation_func\"],\n",
        "                                                 code_activation=specs[\"code_activation\"],\n",
        "                                                 dropout=specs[\"dropout\"],\n",
        "                                                 dropout_rate=specs[\"dropout_rate\"])\n",
        "                \n",
        "                cell_line_autoencoder = DeepAutoencoderTwoHiddenLayers(specs[\"cell_line_dim\"],\n",
        "                                                 specs[\"cell_line_hidden_dim1\"],\n",
        "                                                 specs[\"cell_line_hidden_dim2\"],\n",
        "                                                 specs[\"code_dim\"],\n",
        "                                                 activation_func=specs[\"activation_func\"],\n",
        "                                                 code_activation=specs[\"code_activation\"],\n",
        "                                                 dropout=specs[\"dropout\"],\n",
        "                                                 dropout_rate=specs[\"dropout_rate\"])\n",
        "            elif specs[\"num_layers\"] == 3:\n",
        "                drug_autoencoder = DeepAutoencoderThreeHiddenLayers(specs[\"drug_dim\"],\n",
        "                                                  specs[\"drug_hidden_dim1\"],\n",
        "                                                  specs[\"drug_hidden_dim2\"],\n",
        "                                                  specs[\"drug_hidden_dim3\"],\n",
        "                                                  specs[\"code_dim\"],\n",
        "                                                  activation_func=specs[\"activation_func\"],\n",
        "                                                  code_activation=specs[\"code_activation\"],\n",
        "                                                  dropout=specs[\"dropout\"],\n",
        "                                                  dropout_rate=specs[\"dropout_rate\"])\n",
        "                \n",
        "                cell_line_autoencoder = DeepAutoencoderThreeHiddenLayers(specs[\"cell_line_dim\"],\n",
        "                                                  specs[\"cell_line_hidden_dim1\"],\n",
        "                                                  specs[\"cell_line_hidden_dim2\"],\n",
        "                                                  specs[\"cell_line_hidden_dim3\"],\n",
        "                                                  specs[\"code_dim\"],\n",
        "                                                  activation_func=specs[\"activation_func\"],\n",
        "                                                  code_activation=specs[\"code_activation\"],\n",
        "                                                  dropout=specs[\"dropout\"],\n",
        "                                                  dropout_rate=specs[\"dropout_rate\"])\n",
        "            # Setup whole system\n",
        "            network = RecSystemWithAutoencoders(drug_autoencoder,\n",
        "                                                                  cell_line_autoencoder,\n",
        "                                                                  specs[\"out_activation\"])\n",
        "        # If state dict is provided, load the weights\n",
        "        if state_dict:\n",
        "            network.load_state_dict(state_dict)\n",
        "        return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elhIj508oqop"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSo_uUNcos7N"
      },
      "source": [
        "with open(project_dir + \"Data/Preprocessed Datasets/GDSC-KINOMEscan_proteins_intersection_+_remaining_GDSC_target_genes_dataset_with_IC50.pkl\", \"rb\") as f:\n",
        "    full_dataset = dill.load(f)\n",
        "\n",
        "## Data preprocessing\n",
        "#### Establish response data for samples (drug-cell line pairs)\n",
        "\n",
        "response_df = full_dataset.response_data.copy()\n",
        "\n",
        "#### Establish cell line features data\n",
        "cell_line_data_original_df = full_dataset.full_cell_lines_data.copy()\n",
        "\n",
        "# Search for cell lines present in response data, but missing the genomic features\n",
        "missing_cell_lines = []\n",
        "for cosmic_id in response_df.COSMIC_ID.unique():\n",
        "    if cosmic_id not in cell_line_data_original_df.cell_line_id.unique():\n",
        "        missing_cell_lines.append(cosmic_id)\n",
        "# Put cell line IDs into index and drop cell line IDs columns\n",
        "cell_line_data_original_df.index = cell_line_data_original_df.cell_line_id\n",
        "cell_line_data_original_df = cell_line_data_original_df.drop(\"cell_line_id\", axis=1)\n",
        "\n",
        "# Extract response only for cell lines for which features are present\n",
        "response_df = response_df[~response_df.COSMIC_ID.isin(missing_cell_lines)]\n",
        "\n",
        "#### Establish drug features data\n",
        "drug_data_original_df = full_dataset.drugs_data.copy()\n",
        "\n",
        "# Convert drug index from LINCS name to GDSC drug ID\n",
        "drug_data_original_df.index = drug_data_original_df.index.map(full_dataset.kinomescan_name_to_gdsc_id_mapper)\n",
        "\n",
        "print(drug_data_original_df.shape, cell_line_data_original_df.shape, response_df.shape)\n",
        "\n",
        "# Establish input data dimensionalities\n",
        "drug_dim = drug_data_original_df.shape[1]\n",
        "cell_line_dim = cell_line_data_original_df.shape[1]\n",
        "\n",
        "# Modify response data if needed\n",
        "response_df = response_df[[\"DRUG_ID\", \"COSMIC_ID\", global_response_metric]]\n",
        "print(response_df.shape)\n",
        "response_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdYGdowBrTca"
      },
      "source": [
        "## Trainable function for Ray"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3DAhW9psF85"
      },
      "source": [
        "def trainable_autoencoders(config):\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    # Setup seed\n",
        "    global samples_train\n",
        "    torch.manual_seed(split_seeds[experiment - 1])\n",
        "    print(samples_train.shape)\n",
        "    # Initiate DEERS\n",
        "    # Autoencoders\n",
        "    drug_autoencoder = DeepAutoencoderOneHiddenLayer(fixed_model_specs[\"drug_dim\"],\n",
        "                                                        fixed_model_specs[\"drug_hidden_dim\"], fixed_model_specs[\"code_dim\"],\n",
        "                                                        activation_func=fixed_model_specs[\"activation_func\"],\n",
        "                                                        code_activation=fixed_model_specs[\"code_activation\"],\n",
        "                                                        dropout=fixed_model_specs[\"auto_dropout\"])\n",
        "\n",
        "    cell_line_autoencoder = DeepAutoencoderOneHiddenLayer(fixed_model_specs[\"cell_line_dim\"],\n",
        "                                        fixed_model_specs[\"cell_line_hidden_dim\"], fixed_model_specs[\"code_dim\"],\n",
        "                                        activation_func=fixed_model_specs[\"activation_func\"],\n",
        "                                        code_activation=fixed_model_specs[\"code_activation\"],\n",
        "                                        dropout=fixed_model_specs[\"auto_dropout\"])\n",
        "    # Forward network\n",
        "    net = ForwardNetworkTwoHiddenLayers(2 * fixed_model_specs[\"code_dim\"], \n",
        "                                            fixed_model_specs[\"forward_net_hidden_dim1\"],\n",
        "                                            fixed_model_specs[\"forward_net_hidden_dim2\"],\n",
        "                                            dropout_rate=config[\"forward_net_drop_rate\"],\n",
        "                                            out_activation=fixed_model_specs[\"forward_net_out_act\"])\n",
        "    # Make the model together\n",
        "    network = RecSystemCodeConcatenation(drug_autoencoder, cell_line_autoencoder, forward_network=net,\n",
        "                                              code_interactions=fixed_model_specs[\"code_interactions\"])\n",
        "    print(type(network))\n",
        "    # Compute number of mini-batches per epoch\n",
        "    no_batches = samples_train.shape[0] // fixed_training_specs[\"batch_size\"] + 1\n",
        "\n",
        "    # Establish the device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    # Move the network into device\n",
        "    network.to(device)\n",
        "    # Training the model\n",
        "    network.train()\n",
        "\n",
        "    # Establish early stopping\n",
        "    early_stopping = EarlyStopping(patience=fixed_training_specs[\"early_stopping_patience\"], delta=0.0)\n",
        "\n",
        "    # Set optimizer and criterions\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=config[\"learning_rate\"])\n",
        "    if fixed_training_specs[\"weigh_by_true_auc\"]:\n",
        "        output_criterion = nn.MSELoss(reduction=\"none\")\n",
        "    else:\n",
        "        output_criterion = nn.MSELoss()\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1, fixed_training_specs[\"num_epochs\"] + 1):\n",
        "        # Initialize lists for storing training losses during batches\n",
        "        epoch_main_losses = 0\n",
        "        epoch_y_losses = 0\n",
        "        epoch_drug_indep_losses = 0\n",
        "        epoch_cl_indep_losses = 0\n",
        "        t0 = time.time()\n",
        "        # Shuffle the data if specified\n",
        "        if fixed_training_specs[\"shuffle_train\"]:\n",
        "            samples_train = samples_train.sample(frac=1., random_state=11)\n",
        "        for batch in range(no_batches):\n",
        "            # Separate response variable batch\n",
        "            if batch != no_batches:\n",
        "                samples_batch = samples_train.iloc[batch * fixed_training_specs[\"batch_size\"]:(batch + 1) * fixed_training_specs[\"batch_size\"]]\n",
        "            else:\n",
        "                samples_batch = samples_train.iloc[batch * fixed_training_specs[\"batch_size\"]:]\n",
        "\n",
        "            # Extract output variable batch\n",
        "            y_batch = samples_batch[global_response_metric].values.reshape(-1, 1)\n",
        "            \n",
        "            # Extract cell lines IDs for which data shall be extracted\n",
        "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
        "            # Extract corresponding cell line data\n",
        "            cell_line_input_batch = cell_line_data_df.loc[cl_ids].values\n",
        "\n",
        "            # Extract drug IDs for which data shall be extracted\n",
        "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
        "            # Extract corresponding drug data\n",
        "            drug_input_batch = drug_data_df.loc[drug_ids].values\n",
        "\n",
        "            # Augment data if specified\n",
        "            if fixed_training_specs[\"augment_data\"]:\n",
        "                original_drug_batch = drug_input_batch.copy()\n",
        "                original_cell_line_batch = cell_line_input_batch.copy()\n",
        "                original_y_batch = y_batch.copy()\n",
        "\n",
        "                for i in range(fixed_training_specs[\"augm_data_factor\"]):\n",
        "                    augmented_cell_line_data = original_cell_line_batch[:, :cell_line_exp_idx]\n",
        "                    cell_line_noise = np.random.normal(loc=fixed_training_specs[\"cl_noise_mean\"],\n",
        "                                                      scale=fixed_training_specs[\"cl_noise_std\"],\n",
        "                                                      size=augmented_cell_line_data.shape)\n",
        "                    augmented_cell_line_data = augmented_cell_line_data + cell_line_noise\n",
        "                    augmented_cell_line_data = np.concatenate((augmented_cell_line_data, \n",
        "                                                              original_cell_line_batch[:, cell_line_exp_idx:]),\n",
        "                                                              axis=1)\n",
        "                    augmented_response_data = original_y_batch + np.random.normal(loc=fixed_training_specs[\"auc_noise_mean\"],\n",
        "                                                                scale=fixed_training_specs[\"auc_noise_std\"],\n",
        "                                                                size=original_y_batch.shape)\n",
        "                    \n",
        "                    cell_line_input_batch = np.concatenate((cell_line_input_batch, augmented_cell_line_data),\n",
        "                                                          axis=0)\n",
        "                    \n",
        "                    y_batch = np.concatenate((y_batch, augmented_response_data), axis=0)\n",
        "                    drug_input_batch = np.concatenate((drug_input_batch, original_drug_batch), axis=0)\n",
        "\n",
        "            # Clear gradient buffers because we don't want to accummulate gradients \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Put data batches into torch and device\n",
        "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch).to(device)\n",
        "            drug_input_batch = torch.from_numpy(drug_input_batch).to(device)\n",
        "            y_batch = torch.from_numpy(y_batch).to(device)\n",
        "\n",
        "            # Perform forward pass\n",
        "            batch_output, batch_drug_reconstruction, batch_cl_reconstruction = network(\n",
        "                drug_input_batch.float(), cell_line_input_batch.float())\n",
        "\n",
        "            # L2 regularization if needed\n",
        "            reg_sum = 0\n",
        "            for param in network.parameters():\n",
        "                reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
        "\n",
        "            # Compute the loss for this batch, including the drug and cell line reconstruction losses\n",
        "            # Variant wit weightning the samples\n",
        "            if fixed_training_specs[\"weigh_by_true_auc\"]:\n",
        "                output_loss = output_criterion(batch_output, y_batch.float())\n",
        "                output_loss = torch.mul(output_loss, y_batch.float())\n",
        "                output_loss = output_loss.mean()\n",
        "            # Variant with normal loss\n",
        "            else:\n",
        "                output_loss = output_criterion(batch_output, y_batch.float())\n",
        "            # Reconstruction losses\n",
        "            drug_reconstruction_loss = reconstruction_criterion(batch_drug_reconstruction, drug_input_batch.float())\n",
        "            cl_reconstruction_loss = reconstruction_criterion(batch_cl_reconstruction, cell_line_input_batch.float())\n",
        "\n",
        "            # Compute independence loss\n",
        "            # Covariance matrices\n",
        "            drug_codes_batch = network.drug_autoencoder.encoder(drug_input_batch.float())\n",
        "            cl_codes_batch = network.cell_line_autoencoder.encoder(cell_line_input_batch.float())\n",
        "            drug_cov = ModelWithAutoencoders.covariance_matrix_torch(drug_codes_batch)\n",
        "            cl_cov = ModelWithAutoencoders.covariance_matrix_torch(cl_codes_batch)\n",
        "            \n",
        "            # Actual dependence losses\n",
        "            drug_independence_loss = (drug_cov * drug_cov).sum() - torch.trace(drug_cov * drug_cov)\n",
        "            cl_independence_loss = (cl_cov * cl_cov).sum() - torch.trace(cl_cov * cl_cov)\n",
        "\n",
        "            # Combine the losses in the final cost function\n",
        "            loss = fixed_training_specs[\"y_loss_weight\"] * output_loss + \\\n",
        "                    fixed_training_specs[\"reconstruction_term_drug\"] * drug_reconstruction_loss + \\\n",
        "                    fixed_training_specs[\"reconstruction_term_cl\"] * cl_reconstruction_loss + \\\n",
        "                    fixed_training_specs[\"independence_term\"] * drug_independence_loss + fixed_training_specs[\"independence_term\"] * cl_independence_loss\n",
        "            \n",
        "            # Get the gradients w.r.t. the parameters\n",
        "            loss.backward()\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_main_losses += loss.item()\n",
        "            epoch_y_losses += output_loss.item()\n",
        "            epoch_drug_indep_losses += drug_independence_loss.item()\n",
        "            epoch_cl_indep_losses += cl_independence_loss.item()\n",
        "        \n",
        "        print(\"Epoch training time:\", time.time() - t0)\n",
        "        f = lambda x: np.round(x, 3)\n",
        "        print(\"Epoch: {}, main loss: {}, output loss: {} drug independence loss: {}, cell line independence loss: {}\".format(\n",
        "            epoch, f(epoch_main_losses / no_batches), f(epoch_y_losses / no_batches),\n",
        "            f(epoch_drug_indep_losses / no_batches), f(epoch_cl_indep_losses / no_batches)))\n",
        "        \n",
        "        # Evaluate on training data\n",
        "        model = ModelWithAutoencoders(\"Model\", network)\n",
        "        predicted, y_true, drug_input, cl_input = model.predict(samples_train, cell_line_data_df, drug_data_df,\n",
        "                                                                response_metric=global_response_metric)\n",
        "        preds, drug_reconstruction, cl_reconstruction = predicted\n",
        "        try:\n",
        "            train_rmse, train_corr = Model.evaluate_predictions(y_true, preds.cpu().numpy().reshape(-1))\n",
        "        except ValueError:\n",
        "            train_rmse, train_corr = np.nan, (np.nan, np.nan)\n",
        "        # Drug reconstruction training error\n",
        "        try:\n",
        "            train_drug_rec_rmse, train_drug_rec_corr = Model.evaluate_predictions(drug_input.flatten(),\n",
        "                                            drug_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            train_drug_rec_rmse, train_drug_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Cell line reconstruction training error\n",
        "        try:\n",
        "            train_cl_rec_rmse, train_cl_rec_corr = Model.evaluate_predictions(cl_input.flatten(),\n",
        "                                            cl_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            train_cl_rec_rmse, train_cl_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        performance_df_train = Model.per_drug_performance_df(samples_train, preds.cpu(), \n",
        "                                                             response_metric=global_response_metric)\n",
        "        train_median_rmse = performance_df_train[\"Model RMSE\"].median()\n",
        "        train_median_corr = performance_df_train[\"Model correlation\"].median()\n",
        "        \n",
        "        # Evaluate on validation data\n",
        "        predicted, y_true, drug_input, cl_input = model.predict(samples_val, cell_line_data_df, drug_data_df,\n",
        "                                                                response_metric=global_response_metric)\n",
        "        preds, drug_reconstruction, cl_reconstruction = predicted\n",
        "        \n",
        "        # Main validation error\n",
        "        try:\n",
        "            val_rmse, val_corr = Model.evaluate_predictions(y_true, preds.cpu().numpy().reshape(-1))\n",
        "        except ValueError:\n",
        "            val_rmse, val_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Drug reconstruction validation error\n",
        "        try:\n",
        "            val_drug_rec_rmse, val_drug_rec_corr = Model.evaluate_predictions(drug_input.flatten(),\n",
        "                                            drug_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            val_drug_rec_rmse, val_drug_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Cell line reconstruction validation error\n",
        "        try:\n",
        "            val_cl_rec_rmse, val_cl_rec_corr = Model.evaluate_predictions(cl_input.flatten(),\n",
        "                                            cl_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            val_cl_rec_rmse, val_cl_rec_corr = np.nan, (np.nan, np.nan)\n",
        "        # Per-drug validation evaluation\n",
        "        performance_df_val = Model.per_drug_performance_df(samples_val, preds.cpu(),\n",
        "                                                           response_metric=global_response_metric)\n",
        "        val_median_rmse = performance_df_val[\"Model RMSE\"].median()\n",
        "        val_median_corr = performance_df_val[\"Model correlation\"].median()\n",
        "          \n",
        "        early_stopping(val_rmse, network)\n",
        "            \n",
        "        # Save network's state dict at the end of the training\n",
        "        if early_stopping.early_stop or epoch == fixed_training_specs[\"num_epochs\"]:\n",
        "            performance_df_train.to_csv(\"./performance_per_drug_train.csv\", index=False)\n",
        "            performance_df_val.to_csv(\"./performance_per_drug_val.csv\", index=False)\n",
        "            torch.save(model.network.state_dict(), \"./network_end_state_dict.pth\")\n",
        "\n",
        "        track.log(train_loss=epoch_main_losses / no_batches,\n",
        "                  train_y_loss=epoch_y_losses / no_batches,\n",
        "                  drug_independence_train_loss=epoch_drug_indep_losses / no_batches,\n",
        "                  cl_independence_train_loss=epoch_cl_indep_losses / no_batches,\n",
        "                  train_rmse=train_rmse,\n",
        "                  train_corr=train_corr[0],\n",
        "                  train_median_rmse=train_median_rmse,\n",
        "                  train_median_corr=train_median_corr,\n",
        "                  train_drug_rec_corr=train_drug_rec_corr[0],\n",
        "                  train_cl_rec_corr=train_cl_rec_corr[0],\n",
        "                  val_rmse=val_rmse,\n",
        "                  val_corr=val_corr[0],\n",
        "                  val_drug_rec_corr=val_drug_rec_corr[0],\n",
        "                  val_cl_rec_corr=val_cl_rec_corr[0],\n",
        "                  val_median_rmse=val_median_rmse,\n",
        "                  val_median_corr=val_median_corr,\n",
        "                  early_stopping_stop=early_stopping.early_stop,\n",
        "                  early_stopping_counter=early_stopping.counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAHf5K7H2ZFm"
      },
      "source": [
        "## Specs for model and training, experiment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSLkWvkJaLK_"
      },
      "source": [
        "#### Experiment setup\n",
        "\n",
        "# Establish how many cell lines go to val and test set\n",
        "num_val_cell_lines = 100\n",
        "num_test_cell_lines = 100\n",
        "\n",
        "# Number of train/evaluation iterations\n",
        "num_experimental_iterations = 5\n",
        "\n",
        "# Number of parameter combinations to check in every iteration\n",
        "num_tuning_samples = 20\n",
        "\n",
        "# Establish data split seeds\n",
        "split_seeds = [40, 65, 31, 9, 27]\n",
        "\n",
        "# Metric for parameter search\n",
        "validation_metric = \"val_rmse\"\n",
        "validation_mode = \"min\"\n",
        "\n",
        "########################################################\n",
        "# SPECIFY RESULTS DIRECTORY\n",
        "########################################################\n",
        "global_experiment_name = \"...\"\n",
        "global_experiment_dir = project_dir + \"...\" + global_experiment_name\n",
        "print(global_experiment_dir)\n",
        "\n",
        "if not os.path.exists(global_experiment_dir):\n",
        "    os.makedirs(global_experiment_dir)\n",
        "\n",
        "# Create and save JSON with experiment setup\n",
        "setup = {\"experiment name\": global_experiment_name,\n",
        "         \"dataset name\": full_dataset.name,\n",
        "         \"dataset description\": full_dataset.description,\n",
        "         \"drug features dimension\": drug_dim,\n",
        "         \"cell lines features dimension\": cell_line_dim,\n",
        "         \"num experimental iterations\": num_experimental_iterations,\n",
        "         \"num val cell lines\": num_val_cell_lines,\n",
        "         \"num test cell lines\": num_test_cell_lines,\n",
        "         \"data split seeds\": str(split_seeds),\n",
        "         \"optimizer\": \"Adam\",\n",
        "         \"num tuning samples\": num_tuning_samples,\n",
        "         \"validation metric\": validation_metric,\n",
        "         \"model\": \"RecSysNN\"\n",
        "  }\n",
        "\n",
        "fixed_model_specs = {\"drug_dim\": drug_dim,\n",
        "         \"cell_line_dim\": cell_line_dim,\n",
        "         \"code_dim\": 10,\n",
        "         \"drug_hidden_dim\": 128,\n",
        "         \"cell_line_hidden_dim\": 128,\n",
        "         \"auto_dropout\": False,\n",
        "         \"activation_func\": nn.ReLU,\n",
        "         \"code_activation\": False,\n",
        "         \"forward_net_hidden_dim1\": 512,\n",
        "         \"forward_net_hidden_dim2\": 256,\n",
        "         \"forward_net_out_act\": None,\n",
        "         \"code_interactions\": False}\n",
        "\n",
        "fixed_training_specs = {\"batch_size\": 512, \n",
        "                        \"num_epochs\": 150,\n",
        "                        \"early_stopping_patience\": 20,\n",
        "                        \"weigh_by_true_auc\": False,\n",
        "                        \"augment_data\": True,\n",
        "                        \"augm_data_factor\": 2,\n",
        "                        \"cl_noise_mean\": 0.0,\n",
        "                        \"cl_noise_std\": 0.6,\n",
        "                        \"auc_noise_mean\": 0.0, \n",
        "                        \"auc_noise_std\": 0.15,\n",
        "                        \"reconstruction_term_drug\": 0.1,\n",
        "                        \"reconstruction_term_cl\": 0.25,\n",
        "                        \"independence_term\": 0.1,\n",
        "                        \"y_loss_weight\": 1,\n",
        "                        \"l2_lambda\": 0.0,\n",
        "                        \"shuffle_train\": False,\n",
        "                        \"shuffle_train_and_val\": True,\n",
        "                        \"scale_response_variable\": True}\n",
        "\n",
        "if fixed_training_specs[\"augment_data\"]:\n",
        "  cell_line_exp_idx = 202\n",
        "\n",
        "# Hyperparameter search space\n",
        "search_space = {\n",
        "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
        "    \"forward_net_drop_rate\": tune.grid_search([0.0, 0.5])\n",
        "  }\n",
        "\n",
        "# Save the experiment parameters\n",
        "import json\n",
        "with open(global_experiment_dir + \"/experiment description.json\", \"w\") as f:\n",
        "    json.dump(setup, f)\n",
        "\n",
        "fixed_model_specs_json = fixed_model_specs.copy()\n",
        "for k in fixed_model_specs_json:\n",
        "  fixed_model_specs_json[k] = str(fixed_model_specs_json[k])\n",
        "with open(global_experiment_dir + \"/fixed_model_specs.json\", \"w\") as f:\n",
        "    json.dump(fixed_model_specs_json, f)\n",
        "\n",
        "fixed_training_specs_json = fixed_training_specs.copy()\n",
        "for k in fixed_training_specs_json:\n",
        "  fixed_training_specs_json[k] = str(fixed_training_specs_json[k])\n",
        "with open(global_experiment_dir + \"/fixed_training_specs.json\", \"w\") as f:\n",
        "    json.dump(fixed_training_specs_json, f)\n",
        "\n",
        "with open(global_experiment_dir + \"/search_space.pkl\", \"wb\") as f:\n",
        "    dill.dump(search_space, f)\n",
        "  \n",
        "with open(global_experiment_dir + \"/search_space.json\", \"w\") as f:\n",
        "    json.dump(str(search_space), f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNIUBl7cn9QS"
      },
      "source": [
        "## Hyperparameter tuning loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xltg6G9PamUC"
      },
      "source": [
        "%%time\n",
        "#### Hyperparameter tuning and evaluation iterations\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "for experiment in range(1, num_experimental_iterations + 1):\n",
        "    # Data preprocessing\n",
        "    # Split data into train/val/test sets - unseen cell lines\n",
        "    samples_train, samples_val, samples_test, cell_lines_test, cell_lines_test = Dataset.samples_train_test_split(\n",
        "                                                                            response_df,\n",
        "                                                                            num_val_cell_lines,\n",
        "                                                                            num_test_cell_lines,\n",
        "                                                                            split_seeds[experiment - 1],\n",
        "                                                                            shuffle=True)\n",
        "\n",
        "    # Scale the response variabe if needed\n",
        "    if fixed_training_specs[\"scale_response_variable\"]:\n",
        "        minimum, maximum = samples_train[global_response_metric].min(), samples_train[global_response_metric].max()\n",
        "        samples_train[global_response_metric] = Dataset.min_max_series(samples_train[global_response_metric], minimum, maximum)\n",
        "        samples_val[global_response_metric] = Dataset.min_max_series(samples_val[global_response_metric], minimum, maximum)\n",
        "        samples_test[global_response_metric] = Dataset.min_max_series(samples_test[global_response_metric], minimum, maximum)\n",
        "\n",
        "    print(\"Train:\", samples_train[global_response_metric].mean(), samples_train[global_response_metric].min(), \n",
        "      samples_train[global_response_metric].max())\n",
        "    print(\"Val:\", samples_val[global_response_metric].mean(), samples_val[global_response_metric].min(), \n",
        "          samples_val[global_response_metric].max())\n",
        "    print(\"Test:\", samples_test[global_response_metric].mean(), samples_test[global_response_metric].min(), \n",
        "          samples_test[global_response_metric].max())\n",
        "                   \n",
        "    # Normalize the data\n",
        "    # Cell line data\n",
        "    cols_subset = [col for col in list(cell_line_data_original_df) if col.endswith(\"_exp\")]\n",
        "    rows_subset = list(samples_train[\"COSMIC_ID\"].unique())\n",
        "\n",
        "    cell_line_data_df = Dataset.standardize_data(cell_line_data_original_df, cols_subset=cols_subset,\n",
        "                                                rows_subset=rows_subset)\n",
        "    # Drug data\n",
        "    rows_subset = list(samples_train[\"DRUG_ID\"].unique())\n",
        "    drug_data_df = Dataset.standardize_data(drug_data_original_df, rows_subset=rows_subset)\n",
        "\n",
        "    # Resources to request by Tune\n",
        "    ray.shutdown()\n",
        "    ray.init(num_cpus=2, num_gpus=1)\n",
        "\n",
        "    time.sleep(5)\n",
        "\n",
        "    ################################################\n",
        "    # RUN THE MAIN TUNE ANALYSIS #\n",
        "    ################################################\n",
        "    experiment_name = \"Experiment \" + str(experiment)\n",
        "    # Randomized sampling\n",
        "    analysis = tune.run(trainable_autoencoders, \n",
        "                        name=experiment_name,\n",
        "                        config=search_space, \n",
        "                        num_samples=num_tuning_samples,\n",
        "                        local_dir=global_experiment_dir,\n",
        "                        resources_per_trial={\"cpu\": 1, \"gpu\": 0.48},\n",
        "                        max_failures=3,\n",
        "                        verbose=1,\n",
        "                        stop={\"early_stopping_stop\": True})\n",
        "    \n",
        "    # Save summary dataframe for this analysis\n",
        "    full_df = analysis.dataframe()\n",
        "    full_df.to_csv(analysis._experiment_dir + \"/\" + \"analysis_tuning_results.csv\", index=False)\n",
        "\n",
        "    # Merge training and validation samples\n",
        "    samples_train_and_val = pd.concat([samples_train, samples_val], axis=0)\n",
        "    \n",
        "    # Extract best parameter combination for this tuning analysis\n",
        "    best_config = analysis.get_best_config(metric=validation_metric, mode=validation_mode)\n",
        "    \n",
        "    # Save best config\n",
        "    with open(analysis._experiment_dir + \"/best_config.txt\", \"w\") as f:\n",
        "        for d in best_config:\n",
        "            line = str(d) + \": \" + str(analysis.get_best_config(metric=\"val_rmse\", mode=\"min\")[d]) + \"\\n\"\n",
        "            f.write(line)\n",
        "\n",
        "    # Setup seed\n",
        "    torch.manual_seed(split_seeds[experiment - 1])\n",
        "\n",
        "    # Setup the network\n",
        "    drug_autoencoder = DeepAutoencoderOneHiddenLayer(fixed_model_specs[\"drug_dim\"],\n",
        "                                                        fixed_model_specs[\"drug_hidden_dim\"], fixed_model_specs[\"code_dim\"],\n",
        "                                                        activation_func=fixed_model_specs[\"activation_func\"],\n",
        "                                                        code_activation=fixed_model_specs[\"code_activation\"],\n",
        "                                                        dropout=fixed_model_specs[\"auto_dropout\"])\n",
        "\n",
        "    cell_line_autoencoder = DeepAutoencoderOneHiddenLayer(fixed_model_specs[\"cell_line_dim\"],\n",
        "                                        fixed_model_specs[\"cell_line_hidden_dim\"], fixed_model_specs[\"code_dim\"],\n",
        "                                        activation_func=fixed_model_specs[\"activation_func\"],\n",
        "                                        code_activation=fixed_model_specs[\"code_activation\"],\n",
        "                                        dropout=fixed_model_specs[\"auto_dropout\"])\n",
        "    # Forward network\n",
        "    net = ForwardNetworkTwoHiddenLayers(2 * fixed_model_specs[\"code_dim\"], \n",
        "                                            fixed_model_specs[\"forward_net_hidden_dim1\"],\n",
        "                                            fixed_model_specs[\"forward_net_hidden_dim2\"],\n",
        "                                            dropout_rate=best_config[\"forward_net_drop_rate\"],\n",
        "                                            out_activation=fixed_model_specs[\"forward_net_out_act\"])\n",
        "    # Make the model together\n",
        "    network = RecSystemCodeConcatenation(drug_autoencoder, cell_line_autoencoder, forward_network=net,\n",
        "                                              code_interactions=fixed_model_specs[\"code_interactions\"])\n",
        "    print(type(network))\n",
        "    # Compute number of mini-batches per epoch\n",
        "    no_batches = samples_train_and_val.shape[0] // fixed_training_specs[\"batch_size\"] + 1\n",
        "    print(\"No. batches:\", no_batches)\n",
        "\n",
        "    # Establish the device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    # Move the network into device\n",
        "    network.to(device)\n",
        "    # Training the model\n",
        "    network.train()\n",
        "\n",
        "    # Establish early stopping\n",
        "    early_stopping = EarlyStopping(patience=fixed_training_specs[\"early_stopping_patience\"], delta=0.0)\n",
        "\n",
        "    # Set optimizer and criterions\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=best_config[\"learning_rate\"])\n",
        "    if fixed_training_specs[\"weigh_by_true_auc\"]:\n",
        "        output_criterion = nn.MSELoss(reduction=\"none\")\n",
        "    else:\n",
        "        output_criterion = nn.MSELoss()\n",
        "    reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "    results = {\"epoch\": [],\n",
        "               \"train_loss\": [],\n",
        "               \"train_y_loss\": [],\n",
        "               \"drug_independence_train_loss\": [],\n",
        "               \"cl_independence_train_loss\": [],\n",
        "               \"train_rmse\": [],\n",
        "               \"train_corr\": [],\n",
        "               \"train_median_rmse\": [],\n",
        "               \"train_median_corr\": [],\n",
        "               \"train_drug_rec_corr\": [],\n",
        "               \"train_cl_rec_corr\": [],\n",
        "               \"test_rmse\": [],\n",
        "               \"test_corr\": [],\n",
        "               \"test_median_rmse\": [],\n",
        "               \"test_median_corr\": [],\n",
        "               \"test_drug_rec_corr\": [],\n",
        "               \"test_cl_rec_corr\": []}\n",
        "\n",
        "    # Establish number of epochs\n",
        "    best_trials_logdir = analysis.get_best_logdir(metric=validation_metric, mode=validation_mode)\n",
        "    df = full_df[full_df.logdir == best_trials_logdir]\n",
        "    train_iters = df[\"training_iteration\"].iloc[0]\n",
        "    counter = df[\"early_stopping_counter\"].iloc[0]\n",
        "\n",
        "\n",
        "    for epoch in range(1, train_iters - counter + 2):\n",
        "        # Shuffle the data\n",
        "        if fixed_training_specs[\"shuffle_train_and_val\"]:\n",
        "            samples_train_and_val = samples_train_and_val.sample(frac=1., random_state=11)\n",
        "        # Initialize lists for storing training losses during batches\n",
        "        epoch_main_losses = 0\n",
        "        epoch_y_losses = 0\n",
        "        epoch_drug_indep_losses = 0\n",
        "        epoch_cl_indep_losses = 0\n",
        "        t0 = time.time()\n",
        "        for batch in range(no_batches):\n",
        "            # Separate response variable batch\n",
        "            if batch != no_batches:\n",
        "                samples_batch = samples_train_and_val.iloc[batch * fixed_training_specs[\"batch_size\"]:(batch + 1) * fixed_training_specs[\"batch_size\"]]\n",
        "            else:\n",
        "                samples_batch = samples_train_and_val.iloc[batch * fixed_training_specs[\"batch_size\"]:]\n",
        "\n",
        "            # Extract output variable batch\n",
        "            y_batch = samples_batch[global_response_metric].values.reshape(-1, 1)\n",
        "            \n",
        "            # Extract cell lines IDs for which data shall be extracted\n",
        "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
        "            # Extract corresponding cell line data\n",
        "            cell_line_input_batch = cell_line_data_df.loc[cl_ids].values\n",
        "\n",
        "            # Extract drug IDs for which data shall be extracted\n",
        "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
        "            # Extract corresponding drug data\n",
        "            drug_input_batch = drug_data_df.loc[drug_ids].values\n",
        "\n",
        "            # Augment data if specified\n",
        "            if fixed_training_specs[\"augment_data\"]:\n",
        "                original_drug_batch = drug_input_batch.copy()\n",
        "                original_cell_line_batch = cell_line_input_batch.copy()\n",
        "                original_y_batch = y_batch.copy()\n",
        "\n",
        "                for i in range(fixed_training_specs[\"augm_data_factor\"]):\n",
        "                    augmented_cell_line_data = original_cell_line_batch[:, :cell_line_exp_idx]\n",
        "                    cell_line_noise = np.random.normal(loc=fixed_training_specs[\"cl_noise_mean\"],\n",
        "                                                      scale=fixed_training_specs[\"cl_noise_std\"],\n",
        "                                                      size=augmented_cell_line_data.shape)\n",
        "                    augmented_cell_line_data = augmented_cell_line_data + cell_line_noise\n",
        "                    augmented_cell_line_data = np.concatenate((augmented_cell_line_data, \n",
        "                                                              original_cell_line_batch[:, cell_line_exp_idx:]),\n",
        "                                                              axis=1)\n",
        "                    augmented_response_data = original_y_batch + np.random.normal(loc=fixed_training_specs[\"auc_noise_mean\"],\n",
        "                                                                scale=fixed_training_specs[\"auc_noise_std\"],\n",
        "                                                                size=original_y_batch.shape)\n",
        "                    \n",
        "                    cell_line_input_batch = np.concatenate((cell_line_input_batch, augmented_cell_line_data),\n",
        "                                                          axis=0)\n",
        "                    \n",
        "                    y_batch = np.concatenate((y_batch, augmented_response_data), axis=0)\n",
        "                    drug_input_batch = np.concatenate((drug_input_batch, original_drug_batch), axis=0)\n",
        "\n",
        "            # Clear gradient buffers because we don't want to accummulate gradients \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Put data batches into torch and device\n",
        "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch).to(device)\n",
        "            drug_input_batch = torch.from_numpy(drug_input_batch).to(device)\n",
        "            y_batch = torch.from_numpy(y_batch).to(device)\n",
        "\n",
        "            # Perform forward pass\n",
        "            batch_output, batch_drug_reconstruction, batch_cl_reconstruction = network(\n",
        "                drug_input_batch.float(), cell_line_input_batch.float())\n",
        "\n",
        "\n",
        "            # Compute the loss for this batch, including the drug and cell line reconstruction losses\n",
        "            # Variant with weightning the samples\n",
        "            if fixed_training_specs[\"weigh_by_true_auc\"]:\n",
        "                output_loss = output_criterion(batch_output, y_batch.float())\n",
        "                output_loss = torch.mul(output_loss, y_batch.float())\n",
        "                output_loss = output_loss.mean()\n",
        "            # Variant with normal loss\n",
        "            else:\n",
        "                output_loss = output_criterion(batch_output, y_batch.float())\n",
        "            # Reconstruction losses\n",
        "            drug_reconstruction_loss = reconstruction_criterion(batch_drug_reconstruction, drug_input_batch.float())\n",
        "            cl_reconstruction_loss = reconstruction_criterion(batch_cl_reconstruction, cell_line_input_batch.float())\n",
        "\n",
        "            # Compute independence loss\n",
        "            # Covariance matrices\n",
        "            drug_codes_batch = network.drug_autoencoder.encoder(drug_input_batch.float())\n",
        "            cl_codes_batch = network.cell_line_autoencoder.encoder(cell_line_input_batch.float())\n",
        "            drug_cov = ModelWithAutoencoders.covariance_matrix_torch(drug_codes_batch)\n",
        "            cl_cov = ModelWithAutoencoders.covariance_matrix_torch(cl_codes_batch)\n",
        "            \n",
        "            # Actual dependence losses\n",
        "            drug_independence_loss = (drug_cov * drug_cov).sum() - torch.trace(drug_cov * drug_cov)\n",
        "            cl_independence_loss = (cl_cov * cl_cov).sum() - torch.trace(cl_cov * cl_cov)\n",
        "\n",
        "            # Combine the losses in the final cost function\n",
        "            # loss = output_loss\n",
        "            # Combine the losses in the final cost function\n",
        "            loss = fixed_training_specs[\"y_loss_weight\"] * output_loss + \\\n",
        "                    fixed_training_specs[\"reconstruction_term_drug\"] * drug_reconstruction_loss + \\\n",
        "                    fixed_training_specs[\"reconstruction_term_cl\"] * cl_reconstruction_loss + \\\n",
        "                    fixed_training_specs[\"independence_term\"] * drug_independence_loss + fixed_training_specs[\"independence_term\"] * cl_independence_loss\n",
        "            \n",
        "            # Get the gradients w.r.t. the parameters\n",
        "            loss.backward()\n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_main_losses += loss.item()\n",
        "            epoch_y_losses += output_loss.item()\n",
        "            epoch_drug_indep_losses += drug_independence_loss.item()\n",
        "            epoch_cl_indep_losses += cl_independence_loss.item()\n",
        "        \n",
        "        print(\"Epoch training time:\", time.time() - t0)\n",
        "        f = lambda x: np.round(x, 3)\n",
        "        print(\"Epoch: {}, main loss: {}, output loss: {} drug independence loss: {}, cell line independence loss: {}\".format(\n",
        "            epoch, f(epoch_main_losses / no_batches), f(epoch_y_losses / no_batches),\n",
        "            f(epoch_drug_indep_losses / no_batches), f(epoch_cl_indep_losses / no_batches)))\n",
        "        \n",
        "        # Update metrics for learning curve\n",
        "        results[\"train_loss\"].append(epoch_main_losses / no_batches)\n",
        "        results[\"train_y_loss\"].append(epoch_y_losses / no_batches)\n",
        "        results[\"drug_independence_train_loss\"].append(epoch_drug_indep_losses / no_batches)\n",
        "        results[\"cl_independence_train_loss\"].append(epoch_cl_indep_losses / no_batches)\n",
        "        \n",
        "        # Evaluate on training data\n",
        "        best_model = ModelWithAutoencoders(\"Best model\", network)\n",
        "        predicted, y_true, drug_input, cl_input = best_model.predict(samples_train_and_val, cell_line_data_df, drug_data_df,\n",
        "                                                                     response_metric=global_response_metric)\n",
        "        preds, drug_reconstruction, cl_reconstruction = predicted\n",
        "        try:\n",
        "            train_rmse, train_corr = Model.evaluate_predictions(y_true, preds.cpu().numpy().reshape(-1))\n",
        "        except ValueError:\n",
        "            train_rmse, train_corr = np.nan, (np.nan, np.nan)\n",
        "        # Drug reconstruction training error\n",
        "        try:\n",
        "            train_drug_rec_rmse, train_drug_rec_corr = Model.evaluate_predictions(drug_input.flatten(),\n",
        "                                            drug_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            train_drug_rec_rmse, train_drug_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Cell line reconstruction training error\n",
        "        try:\n",
        "            train_cl_rec_rmse, train_cl_rec_corr = Model.evaluate_predictions(cl_input.flatten(),\n",
        "                                            cl_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            train_cl_rec_rmse, train_cl_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        train_performance_df = Model.per_drug_performance_df(samples_train_and_val, preds.cpu(),\n",
        "                                                             response_metric=global_response_metric)\n",
        "        train_median_rmse = train_performance_df[\"Model RMSE\"].median()\n",
        "        train_median_corr = train_performance_df[\"Model correlation\"].median()\n",
        "        \n",
        "        results[\"epoch\"].append(epoch)\n",
        "        results[\"train_rmse\"].append(train_rmse)\n",
        "        results[\"train_corr\"].append(train_corr[0])\n",
        "        results[\"train_median_corr\"].append(train_median_corr)\n",
        "        results[\"train_median_rmse\"].append(train_median_rmse)\n",
        "        results[\"train_drug_rec_corr\"].append(train_drug_rec_corr[0])\n",
        "        results[\"train_cl_rec_corr\"].append(train_cl_rec_corr[0])\n",
        "        \n",
        "        # Evaluate on test data\n",
        "        predicted, y_true, drug_input, cl_input = best_model.predict(samples_test, cell_line_data_df, drug_data_df,\n",
        "                                                                     response_metric=global_response_metric)\n",
        "        preds, drug_reconstruction, cl_reconstruction = predicted\n",
        "        \n",
        "        # Main test error\n",
        "        try:\n",
        "            test_rmse, test_corr = Model.evaluate_predictions(y_true, preds.cpu().numpy().reshape(-1))\n",
        "        except ValueError:\n",
        "            test_rmse, test_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Drug reconstruction validation error\n",
        "        try:\n",
        "            test_drug_rec_rmse, test_drug_rec_corr = Model.evaluate_predictions(drug_input.flatten(),\n",
        "                                            drug_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            test_drug_rec_rmse, test_drug_rec_corr = np.nan, (np.nan, np.nan)\n",
        "\n",
        "        # Cell line reconstruction validation error\n",
        "        try:\n",
        "            test_cl_rec_rmse, test_cl_rec_corr = Model.evaluate_predictions(cl_input.flatten(),\n",
        "                                            cl_reconstruction.cpu().numpy().flatten())\n",
        "        except ValueError:\n",
        "            test_cl_rec_rmse, test_cl_rec_corr = np.nan, (np.nan, np.nan)\n",
        "     \n",
        "        # Per-drug test evaluation\n",
        "        test_performance_df = Model.per_drug_performance_df(samples_test, preds.cpu(),\n",
        "                                                            response_metric=global_response_metric)\n",
        "        test_median_rmse = test_performance_df[\"Model RMSE\"].median()\n",
        "        test_median_corr = test_performance_df[\"Model correlation\"].median()\n",
        "\n",
        "        # Per-cell line test evaluation\n",
        "        per_cl_test_performance_df = Model.per_entity_performance_df(\n",
        "                        samples_test, preds.cpu(), entity_type=\"COSMIC_ID\", response_metric=global_response_metric)\n",
        "            \n",
        "        results[\"test_rmse\"].append(test_rmse)\n",
        "        results[\"test_corr\"].append(test_corr[0])\n",
        "        results[\"test_drug_rec_corr\"].append(test_drug_rec_corr[0])\n",
        "        results[\"test_cl_rec_corr\"].append(test_cl_rec_corr[0])\n",
        "        results[\"test_median_corr\"].append(test_median_corr)\n",
        "        results[\"test_median_rmse\"].append(test_median_rmse)\n",
        "\n",
        "        best_model_results = pd.DataFrame(results)\n",
        "\n",
        "    best_model_results.to_csv(analysis._experiment_dir + \"/best_model_test_results.csv\", index=False)\n",
        "    train_performance_df.to_csv(analysis._experiment_dir + \"/best_model_per_drug_train_results.csv\", index=False)\n",
        "    test_performance_df.to_csv(analysis._experiment_dir + \"/best_model_per_drug_test_results.csv\", index=False)\n",
        "    \n",
        "    # Save trained best model\n",
        "    with open(analysis._experiment_dir + \"/best_trained_model.pkl\", \"wb\") as f:\n",
        "        dill.dump(best_model, f)\n",
        "    # Save trained best model's state dict\n",
        "    best_network = network\n",
        "    torch.save(best_network.state_dict(), analysis._experiment_dir + \"/best_network_state_dict.pth\")\n",
        "        \n",
        "    print(\"*\" * 50)\n",
        "    print(\"Experiment\", experiment)\n",
        "    print(best_config)\n",
        "    print(\"*\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH7omXcXusI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}