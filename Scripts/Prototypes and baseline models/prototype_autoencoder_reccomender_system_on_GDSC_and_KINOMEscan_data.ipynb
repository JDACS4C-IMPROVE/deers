{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype reccomender system with cell lines and drug data embedded via simple autoencoder using KINOMEscan drug data and GDSC cell line data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import dill\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom utilities imports\n",
    "sys.path.append(\"/media/krzysztof/Nowy/Doktorat - Modelling drug efficacy in cancer/Recommender System Approach/Scripts/Modules\")\n",
    "from modeling import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinases Dataset + Remaning GDSC drug's putative targets <class 'modeling.Dataset'>\n",
      "\n",
      "Dataset containing 74 common drugs of GDSC and HMS LINCS Kinome scan dataset.\n",
      "Cell lines data types: expression, coding variant and tissue type. Expressions and coding variants are \n",
      "present only for proteins present in both GDSC and KINOMEscan data, resulting in expression of 188 genes and\n",
      "nmutations in 18 genes.\n",
      "In addition, expressions and mutations (17 new features) of remaining target genes from GDSC are included\n",
      "Tissue types are dummy encoded GDSC Tissue Descriptions 1 (18 features).\n",
      "Drugs representation: inhibition scores (% control) of 294 proteins. Set of proteins is the intersection of \n",
      "proteins screened for each of 74 drugs.\n",
      "Drug response data: drug reponse data contains AUC metrics across cell lines for 74 drugs considered.\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../../Data/Preprocessed Datasets/\"\n",
    "with open(filepath + \"GDSC-KINOMEscan_proteins_intersection_+_remaining_GDSC_target_genes_dataset.pkl\", \"rb\") as f:\n",
    "    full_dataset = dill.load(f)\n",
    "print(full_dataset.name, type(full_dataset))\n",
    "print()\n",
    "print(full_dataset.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(922, 241) (74, 294) (52730, 3)\n"
     ]
    }
   ],
   "source": [
    "# Establish response data for samples (drug-cell line pairs)\n",
    "response_df = full_dataset.response_data.copy()\n",
    "\n",
    "# Establish cell line features data\n",
    "cell_line_data_df = full_dataset.full_cell_lines_data.copy()\n",
    "\n",
    "# Search for cell lines present in response data, but missing the genomic features\n",
    "missing_cell_lines = []\n",
    "for cosmic_id in response_df.COSMIC_ID.unique():\n",
    "    if cosmic_id not in cell_line_data_df.cell_line_id.unique():\n",
    "        missing_cell_lines.append(cosmic_id)\n",
    "# Put cell line IDs into index and drop cell line IDs columns\n",
    "cell_line_data_df.index = cell_line_data_df.cell_line_id\n",
    "cell_line_data_df = cell_line_data_df.drop(\"cell_line_id\", axis=1)\n",
    "\n",
    "# Extract response only for cell lines for which features are present\n",
    "response_df = response_df[~response_df.COSMIC_ID.isin(missing_cell_lines)]\n",
    "\n",
    "# Establish drug features data\n",
    "drug_data_df = full_dataset.drugs_data.copy()\n",
    "\n",
    "# Convert drug index from LINCS name to GDSC drug ID\n",
    "drug_data_df.index = drug_data_df.index.map(full_dataset.kinomescan_name_to_gdsc_id_mapper)\n",
    "print(cell_line_data_df.shape, drug_data_df.shape, response_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models' definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear autoencoder\n",
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinearAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh())\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Simple linear autoencoder\n",
    "class LinearAutoencoderWithoutActivation(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinearAutoencoderWithoutActivation, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Basic deep autoencoder\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, middle_dim, hidden_dim):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, middle_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(middle_dim, hidden_dim),\n",
    "            nn.ReLU())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, middle_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(middle_dim, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Sample network\n",
    "class SampleNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SampleNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "class RecSystemWithAutoencoders:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check basic autoencoder alone on drug data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instianianate the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which data to use\n",
    "data_train = drug_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "# data_train = data_train.sample(frac=1.)\n",
    "# print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.456547008749136e-15 294.0 -6.81641338878148 1.2330512316262772\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "means = data_train.mean()\n",
    "stds = data_train.std()\n",
    "data_train = (data_train - means) / stds\n",
    "print(data_train.mean().sum(), data_train.std().sum(), data_train.min().min(), data_train.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 10)\n"
     ]
    }
   ],
   "source": [
    "# Take a sample of training data\n",
    "data_train = data_train.iloc[:, :10]\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) 20.590186153530404\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.sample(n=10, random_state=11)\n",
    "print(data_train.shape, data_train.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=2, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (decoder): Linear(in_features=2, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instatianate the model\n",
    "hidden_dim = 2\n",
    "\n",
    "torch.manual_seed(11)\n",
    "autoencoder = LinearAutoencoder(10, 2)\n",
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "torch.Size([2])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for p in autoencoder.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "no_batches = data_train.shape[0] // batch_size + 1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    autoencoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch loss: 0.20876051485538483\n",
      "Epoch: 2, batch loss: 0.20875924825668335\n",
      "Epoch: 3, batch loss: 0.2087579369544983\n",
      "Epoch: 4, batch loss: 0.20875662565231323\n",
      "Epoch: 5, batch loss: 0.20875529944896698\n",
      "Epoch: 6, batch loss: 0.20875407755374908\n",
      "Epoch: 7, batch loss: 0.20875275135040283\n",
      "Epoch: 8, batch loss: 0.20875145494937897\n",
      "Epoch: 9, batch loss: 0.2087501734495163\n",
      "Epoch: 10, batch loss: 0.20874889194965363\n",
      "Epoch: 11, batch loss: 0.20874756574630737\n",
      "Epoch: 12, batch loss: 0.20874625444412231\n",
      "Epoch: 13, batch loss: 0.20874498784542084\n",
      "Epoch: 14, batch loss: 0.20874370634555817\n",
      "Epoch: 15, batch loss: 0.2087424099445343\n",
      "Epoch: 16, batch loss: 0.20874111354351044\n",
      "Epoch: 17, batch loss: 0.20873983204364777\n",
      "Epoch: 18, batch loss: 0.2087385207414627\n",
      "Epoch: 19, batch loss: 0.20873723924160004\n",
      "Epoch: 20, batch loss: 0.20873592793941498\n",
      "Epoch: 21, batch loss: 0.2087346464395523\n",
      "Epoch: 22, batch loss: 0.20873335003852844\n",
      "Epoch: 23, batch loss: 0.20873206853866577\n",
      "Epoch: 24, batch loss: 0.2087307721376419\n",
      "Epoch: 25, batch loss: 0.20872947573661804\n",
      "Epoch: 26, batch loss: 0.20872816443443298\n",
      "Epoch: 27, batch loss: 0.20872686803340912\n",
      "Epoch: 28, batch loss: 0.20872560143470764\n",
      "Epoch: 29, batch loss: 0.20872430503368378\n",
      "Epoch: 30, batch loss: 0.2087230086326599\n",
      "Epoch: 31, batch loss: 0.20872169733047485\n",
      "Epoch: 32, batch loss: 0.20872043073177338\n",
      "Epoch: 33, batch loss: 0.2087191343307495\n",
      "Epoch: 34, batch loss: 0.20871783792972565\n",
      "Epoch: 35, batch loss: 0.20871654152870178\n",
      "Epoch: 36, batch loss: 0.20871524512767792\n",
      "Epoch: 37, batch loss: 0.20871394872665405\n",
      "Epoch: 38, batch loss: 0.20871266722679138\n",
      "Epoch: 39, batch loss: 0.20871134102344513\n",
      "Epoch: 40, batch loss: 0.20871010422706604\n",
      "Epoch: 41, batch loss: 0.20870880782604218\n",
      "Epoch: 42, batch loss: 0.2087075114250183\n",
      "Epoch: 43, batch loss: 0.20870620012283325\n",
      "Epoch: 44, batch loss: 0.2087049037218094\n",
      "Epoch: 45, batch loss: 0.2087036371231079\n",
      "Epoch: 46, batch loss: 0.20870235562324524\n",
      "Epoch: 47, batch loss: 0.20870104432106018\n",
      "Epoch: 48, batch loss: 0.2086997628211975\n",
      "Epoch: 49, batch loss: 0.20869848132133484\n",
      "Epoch: 50, batch loss: 0.20869717001914978\n",
      "Epoch: 51, batch loss: 0.20869584381580353\n",
      "Epoch: 52, batch loss: 0.20869454741477966\n",
      "Epoch: 53, batch loss: 0.20869328081607819\n",
      "Epoch: 54, batch loss: 0.20869198441505432\n",
      "Epoch: 55, batch loss: 0.20869068801403046\n",
      "Epoch: 56, batch loss: 0.20868940651416779\n",
      "Epoch: 57, batch loss: 0.20868812501430511\n",
      "Epoch: 58, batch loss: 0.20868682861328125\n",
      "Epoch: 59, batch loss: 0.20868553221225739\n",
      "Epoch: 60, batch loss: 0.20868425071239471\n",
      "Epoch: 61, batch loss: 0.20868295431137085\n",
      "Epoch: 62, batch loss: 0.20868167281150818\n",
      "Epoch: 63, batch loss: 0.2086804062128067\n",
      "Epoch: 64, batch loss: 0.20867908000946045\n",
      "Epoch: 65, batch loss: 0.20867782831192017\n",
      "Epoch: 66, batch loss: 0.2086765170097351\n",
      "Epoch: 67, batch loss: 0.20867523550987244\n",
      "Epoch: 68, batch loss: 0.20867395401000977\n",
      "Epoch: 69, batch loss: 0.2086726576089859\n",
      "Epoch: 70, batch loss: 0.20867134630680084\n",
      "Epoch: 71, batch loss: 0.20867007970809937\n",
      "Epoch: 72, batch loss: 0.2086687684059143\n",
      "Epoch: 73, batch loss: 0.20866747200489044\n",
      "Epoch: 74, batch loss: 0.20866619050502777\n",
      "Epoch: 75, batch loss: 0.20866484940052032\n",
      "Epoch: 76, batch loss: 0.20866361260414124\n",
      "Epoch: 77, batch loss: 0.20866230130195618\n",
      "Epoch: 78, batch loss: 0.2086610198020935\n",
      "Epoch: 79, batch loss: 0.20865972340106964\n",
      "Epoch: 80, batch loss: 0.20865844190120697\n",
      "Epoch: 81, batch loss: 0.2086571753025055\n",
      "Epoch: 82, batch loss: 0.20865587890148163\n",
      "Epoch: 83, batch loss: 0.20865455269813538\n",
      "Epoch: 84, batch loss: 0.2086533159017563\n",
      "Epoch: 85, batch loss: 0.20865200459957123\n",
      "Epoch: 86, batch loss: 0.20865072309970856\n",
      "Epoch: 87, batch loss: 0.2086494266986847\n",
      "Epoch: 88, batch loss: 0.20864813029766083\n",
      "Epoch: 89, batch loss: 0.20864686369895935\n",
      "Epoch: 90, batch loss: 0.2086455374956131\n",
      "Epoch: 91, batch loss: 0.20864424109458923\n",
      "Epoch: 92, batch loss: 0.20864295959472656\n",
      "Epoch: 93, batch loss: 0.2086416780948639\n",
      "Epoch: 94, batch loss: 0.20864038169384003\n",
      "Epoch: 95, batch loss: 0.20863910019397736\n",
      "Epoch: 96, batch loss: 0.2086377739906311\n",
      "Epoch: 97, batch loss: 0.20863653719425201\n",
      "Epoch: 98, batch loss: 0.20863521099090576\n",
      "Epoch: 99, batch loss: 0.20863394439220428\n",
      "Epoch: 100, batch loss: 0.20863264799118042\n",
      "Epoch: 101, batch loss: 0.20863136649131775\n",
      "Epoch: 102, batch loss: 0.20863007009029388\n",
      "Epoch: 103, batch loss: 0.2086287885904312\n",
      "Epoch: 104, batch loss: 0.20862749218940735\n",
      "Epoch: 105, batch loss: 0.20862622559070587\n",
      "Epoch: 106, batch loss: 0.2086249142885208\n",
      "Epoch: 107, batch loss: 0.20862366259098053\n",
      "Epoch: 108, batch loss: 0.20862236618995667\n",
      "Epoch: 109, batch loss: 0.208621084690094\n",
      "Epoch: 110, batch loss: 0.20861974358558655\n",
      "Epoch: 111, batch loss: 0.20861849188804626\n",
      "Epoch: 112, batch loss: 0.2086171954870224\n",
      "Epoch: 113, batch loss: 0.20861591398715973\n",
      "Epoch: 114, batch loss: 0.20861460268497467\n",
      "Epoch: 115, batch loss: 0.2086133360862732\n",
      "Epoch: 116, batch loss: 0.20861202478408813\n",
      "Epoch: 117, batch loss: 0.20861072838306427\n",
      "Epoch: 118, batch loss: 0.2086094319820404\n",
      "Epoch: 119, batch loss: 0.20860815048217773\n",
      "Epoch: 120, batch loss: 0.20860689878463745\n",
      "Epoch: 121, batch loss: 0.20860555768013\n",
      "Epoch: 122, batch loss: 0.20860427618026733\n",
      "Epoch: 123, batch loss: 0.20860297977924347\n",
      "Epoch: 124, batch loss: 0.20860172808170319\n",
      "Epoch: 125, batch loss: 0.20860040187835693\n",
      "Epoch: 126, batch loss: 0.20859915018081665\n",
      "Epoch: 127, batch loss: 0.2085978388786316\n",
      "Epoch: 128, batch loss: 0.2085965871810913\n",
      "Epoch: 129, batch loss: 0.20859527587890625\n",
      "Epoch: 130, batch loss: 0.20859402418136597\n",
      "Epoch: 131, batch loss: 0.20859269797801971\n",
      "Epoch: 132, batch loss: 0.20859144628047943\n",
      "Epoch: 133, batch loss: 0.20859010517597198\n",
      "Epoch: 134, batch loss: 0.20858882367610931\n",
      "Epoch: 135, batch loss: 0.20858751237392426\n",
      "Epoch: 136, batch loss: 0.20858626067638397\n",
      "Epoch: 137, batch loss: 0.2085849642753601\n",
      "Epoch: 138, batch loss: 0.20858368277549744\n",
      "Epoch: 139, batch loss: 0.20858238637447357\n",
      "Epoch: 140, batch loss: 0.2085811048746109\n",
      "Epoch: 141, batch loss: 0.20857980847358704\n",
      "Epoch: 142, batch loss: 0.20857852697372437\n",
      "Epoch: 143, batch loss: 0.2085772305727005\n",
      "Epoch: 144, batch loss: 0.20857594907283783\n",
      "Epoch: 145, batch loss: 0.20857465267181396\n",
      "Epoch: 146, batch loss: 0.2085733562707901\n",
      "Epoch: 147, batch loss: 0.20857205986976624\n",
      "Epoch: 148, batch loss: 0.20857077836990356\n",
      "Epoch: 149, batch loss: 0.2085694819688797\n",
      "Epoch: 150, batch loss: 0.20856821537017822\n",
      "Epoch: 151, batch loss: 0.20856691896915436\n",
      "Epoch: 152, batch loss: 0.2085656374692917\n",
      "Epoch: 153, batch loss: 0.20856435596942902\n",
      "Epoch: 154, batch loss: 0.20856305956840515\n",
      "Epoch: 155, batch loss: 0.2085617631673813\n",
      "Epoch: 156, batch loss: 0.208560511469841\n",
      "Epoch: 157, batch loss: 0.20855918526649475\n",
      "Epoch: 158, batch loss: 0.20855790376663208\n",
      "Epoch: 159, batch loss: 0.2085566520690918\n",
      "Epoch: 160, batch loss: 0.20855534076690674\n",
      "Epoch: 161, batch loss: 0.20855405926704407\n",
      "Epoch: 162, batch loss: 0.2085527777671814\n",
      "Epoch: 163, batch loss: 0.20855149626731873\n",
      "Epoch: 164, batch loss: 0.20855019986629486\n",
      "Epoch: 165, batch loss: 0.2085488885641098\n",
      "Epoch: 166, batch loss: 0.20854759216308594\n",
      "Epoch: 167, batch loss: 0.20854635536670685\n",
      "Epoch: 168, batch loss: 0.2085450291633606\n",
      "Epoch: 169, batch loss: 0.20854376256465912\n",
      "Epoch: 170, batch loss: 0.20854243636131287\n",
      "Epoch: 171, batch loss: 0.20854118466377258\n",
      "Epoch: 172, batch loss: 0.2085399031639099\n",
      "Epoch: 173, batch loss: 0.20853859186172485\n",
      "Epoch: 174, batch loss: 0.2085372805595398\n",
      "Epoch: 175, batch loss: 0.2085360586643219\n",
      "Epoch: 176, batch loss: 0.20853471755981445\n",
      "Epoch: 177, batch loss: 0.20853346586227417\n",
      "Epoch: 178, batch loss: 0.2085321545600891\n",
      "Epoch: 179, batch loss: 0.20853085815906525\n",
      "Epoch: 180, batch loss: 0.20852960646152496\n",
      "Epoch: 181, batch loss: 0.2085282951593399\n",
      "Epoch: 182, batch loss: 0.20852701365947723\n",
      "Epoch: 183, batch loss: 0.20852571725845337\n",
      "Epoch: 184, batch loss: 0.2085244357585907\n",
      "Epoch: 185, batch loss: 0.20852315425872803\n",
      "Epoch: 186, batch loss: 0.20852185785770416\n",
      "Epoch: 187, batch loss: 0.2085205614566803\n",
      "Epoch: 188, batch loss: 0.20851927995681763\n",
      "Epoch: 189, batch loss: 0.20851801335811615\n",
      "Epoch: 190, batch loss: 0.20851673185825348\n",
      "Epoch: 191, batch loss: 0.2085154503583908\n",
      "Epoch: 192, batch loss: 0.20851412415504456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 193, batch loss: 0.20851285755634308\n",
      "Epoch: 194, batch loss: 0.2085115760564804\n",
      "Epoch: 195, batch loss: 0.20851030945777893\n",
      "Epoch: 196, batch loss: 0.20850902795791626\n",
      "Epoch: 197, batch loss: 0.2085077315568924\n",
      "Epoch: 198, batch loss: 0.20850643515586853\n",
      "Epoch: 199, batch loss: 0.20850513875484467\n",
      "Epoch: 200, batch loss: 0.2085038721561432\n",
      "Epoch: 201, batch loss: 0.20850256085395813\n",
      "Epoch: 202, batch loss: 0.20850127935409546\n",
      "Epoch: 203, batch loss: 0.2084999829530716\n",
      "Epoch: 204, batch loss: 0.20849870145320892\n",
      "Epoch: 205, batch loss: 0.20849743485450745\n",
      "Epoch: 206, batch loss: 0.20849616825580597\n",
      "Epoch: 207, batch loss: 0.2084948718547821\n",
      "Epoch: 208, batch loss: 0.20849357545375824\n",
      "Epoch: 209, batch loss: 0.20849229395389557\n",
      "Epoch: 210, batch loss: 0.2084909975528717\n",
      "Epoch: 211, batch loss: 0.20848970115184784\n",
      "Epoch: 212, batch loss: 0.20848841965198517\n",
      "Epoch: 213, batch loss: 0.2084871232509613\n",
      "Epoch: 214, batch loss: 0.20848588645458221\n",
      "Epoch: 215, batch loss: 0.20848457515239716\n",
      "Epoch: 216, batch loss: 0.2084832787513733\n",
      "Epoch: 217, batch loss: 0.20848199725151062\n",
      "Epoch: 218, batch loss: 0.20848074555397034\n",
      "Epoch: 219, batch loss: 0.20847944915294647\n",
      "Epoch: 220, batch loss: 0.20847810804843903\n",
      "Epoch: 221, batch loss: 0.20847684144973755\n",
      "Epoch: 222, batch loss: 0.20847558975219727\n",
      "Epoch: 223, batch loss: 0.2084743082523346\n",
      "Epoch: 224, batch loss: 0.20847299695014954\n",
      "Epoch: 225, batch loss: 0.20847170054912567\n",
      "Epoch: 226, batch loss: 0.2084704339504242\n",
      "Epoch: 227, batch loss: 0.20846916735172272\n",
      "Epoch: 228, batch loss: 0.20846787095069885\n",
      "Epoch: 229, batch loss: 0.20846658945083618\n",
      "Epoch: 230, batch loss: 0.2084653079509735\n",
      "Epoch: 231, batch loss: 0.20846401154994965\n",
      "Epoch: 232, batch loss: 0.20846273005008698\n",
      "Epoch: 233, batch loss: 0.2084614634513855\n",
      "Epoch: 234, batch loss: 0.20846013724803925\n",
      "Epoch: 235, batch loss: 0.20845885574817657\n",
      "Epoch: 236, batch loss: 0.2084576040506363\n",
      "Epoch: 237, batch loss: 0.20845629274845123\n",
      "Epoch: 238, batch loss: 0.20845502614974976\n",
      "Epoch: 239, batch loss: 0.20845377445220947\n",
      "Epoch: 240, batch loss: 0.2084524780511856\n",
      "Epoch: 241, batch loss: 0.20845118165016174\n",
      "Epoch: 242, batch loss: 0.20844988524913788\n",
      "Epoch: 243, batch loss: 0.2084486335515976\n",
      "Epoch: 244, batch loss: 0.20844730734825134\n",
      "Epoch: 245, batch loss: 0.20844604074954987\n",
      "Epoch: 246, batch loss: 0.2084447741508484\n",
      "Epoch: 247, batch loss: 0.20844344794750214\n",
      "Epoch: 248, batch loss: 0.20844219624996185\n",
      "Epoch: 249, batch loss: 0.208440899848938\n",
      "Epoch: 250, batch loss: 0.20843961834907532\n",
      "Epoch: 251, batch loss: 0.20843833684921265\n",
      "Epoch: 252, batch loss: 0.20843705534934998\n",
      "Epoch: 253, batch loss: 0.2084357887506485\n",
      "Epoch: 254, batch loss: 0.20843450725078583\n",
      "Epoch: 255, batch loss: 0.20843322575092316\n",
      "Epoch: 256, batch loss: 0.2084319144487381\n",
      "Epoch: 257, batch loss: 0.20843064785003662\n",
      "Epoch: 258, batch loss: 0.20842935144901276\n",
      "Epoch: 259, batch loss: 0.20842809975147247\n",
      "Epoch: 260, batch loss: 0.20842677354812622\n",
      "Epoch: 261, batch loss: 0.20842552185058594\n",
      "Epoch: 262, batch loss: 0.20842422544956207\n",
      "Epoch: 263, batch loss: 0.2084229439496994\n",
      "Epoch: 264, batch loss: 0.20842164754867554\n",
      "Epoch: 265, batch loss: 0.20842039585113525\n",
      "Epoch: 266, batch loss: 0.20841911435127258\n",
      "Epoch: 267, batch loss: 0.20841781795024872\n",
      "Epoch: 268, batch loss: 0.20841652154922485\n",
      "Epoch: 269, batch loss: 0.20841529965400696\n",
      "Epoch: 270, batch loss: 0.2084140032529831\n",
      "Epoch: 271, batch loss: 0.20841269195079803\n",
      "Epoch: 272, batch loss: 0.20841139554977417\n",
      "Epoch: 273, batch loss: 0.2084101289510727\n",
      "Epoch: 274, batch loss: 0.20840883255004883\n",
      "Epoch: 275, batch loss: 0.20840758085250854\n",
      "Epoch: 276, batch loss: 0.2084062695503235\n",
      "Epoch: 277, batch loss: 0.208405002951622\n",
      "Epoch: 278, batch loss: 0.20840373635292053\n",
      "Epoch: 279, batch loss: 0.20840241014957428\n",
      "Epoch: 280, batch loss: 0.2084011435508728\n",
      "Epoch: 281, batch loss: 0.20839989185333252\n",
      "Epoch: 282, batch loss: 0.20839856564998627\n",
      "Epoch: 283, batch loss: 0.20839731395244598\n",
      "Epoch: 284, batch loss: 0.20839601755142212\n",
      "Epoch: 285, batch loss: 0.20839473605155945\n",
      "Epoch: 286, batch loss: 0.20839345455169678\n",
      "Epoch: 287, batch loss: 0.2083921879529953\n",
      "Epoch: 288, batch loss: 0.20839090645313263\n",
      "Epoch: 289, batch loss: 0.20838961005210876\n",
      "Epoch: 290, batch loss: 0.2083883434534073\n",
      "Epoch: 291, batch loss: 0.2083870768547058\n",
      "Epoch: 292, batch loss: 0.20838576555252075\n",
      "Epoch: 293, batch loss: 0.20838451385498047\n",
      "Epoch: 294, batch loss: 0.2083832174539566\n",
      "Epoch: 295, batch loss: 0.20838198065757751\n",
      "Epoch: 296, batch loss: 0.20838063955307007\n",
      "Epoch: 297, batch loss: 0.20837938785552979\n",
      "Epoch: 298, batch loss: 0.20837810635566711\n",
      "Epoch: 299, batch loss: 0.20837682485580444\n",
      "Epoch: 300, batch loss: 0.20837554335594177\n",
      "Epoch: 301, batch loss: 0.2083742320537567\n",
      "Epoch: 302, batch loss: 0.20837299525737762\n",
      "Epoch: 303, batch loss: 0.20837168395519257\n",
      "Epoch: 304, batch loss: 0.2083704173564911\n",
      "Epoch: 305, batch loss: 0.20836912095546722\n",
      "Epoch: 306, batch loss: 0.20836782455444336\n",
      "Epoch: 307, batch loss: 0.20836657285690308\n",
      "Epoch: 308, batch loss: 0.2083652913570404\n",
      "Epoch: 309, batch loss: 0.20836400985717773\n",
      "Epoch: 310, batch loss: 0.20836269855499268\n",
      "Epoch: 311, batch loss: 0.20836141705513\n",
      "Epoch: 312, batch loss: 0.20836013555526733\n",
      "Epoch: 313, batch loss: 0.20835882425308228\n",
      "Epoch: 314, batch loss: 0.20835758745670319\n",
      "Epoch: 315, batch loss: 0.20835630595684052\n",
      "Epoch: 316, batch loss: 0.20835503935813904\n",
      "Epoch: 317, batch loss: 0.20835372805595398\n",
      "Epoch: 318, batch loss: 0.2083524465560913\n",
      "Epoch: 319, batch loss: 0.20835120975971222\n",
      "Epoch: 320, batch loss: 0.20834991335868835\n",
      "Epoch: 321, batch loss: 0.2083486169576645\n",
      "Epoch: 322, batch loss: 0.2083473801612854\n",
      "Epoch: 323, batch loss: 0.20834606885910034\n",
      "Epoch: 324, batch loss: 0.20834480226039886\n",
      "Epoch: 325, batch loss: 0.208343505859375\n",
      "Epoch: 326, batch loss: 0.20834222435951233\n",
      "Epoch: 327, batch loss: 0.20834094285964966\n",
      "Epoch: 328, batch loss: 0.20833970606327057\n",
      "Epoch: 329, batch loss: 0.20833837985992432\n",
      "Epoch: 330, batch loss: 0.20833711326122284\n",
      "Epoch: 331, batch loss: 0.20833580195903778\n",
      "Epoch: 332, batch loss: 0.20833458006381989\n",
      "Epoch: 333, batch loss: 0.20833326876163483\n",
      "Epoch: 334, batch loss: 0.20833200216293335\n",
      "Epoch: 335, batch loss: 0.2083306908607483\n",
      "Epoch: 336, batch loss: 0.2083294689655304\n",
      "Epoch: 337, batch loss: 0.20832815766334534\n",
      "Epoch: 338, batch loss: 0.20832687616348267\n",
      "Epoch: 339, batch loss: 0.20832559466362\n",
      "Epoch: 340, batch loss: 0.20832429826259613\n",
      "Epoch: 341, batch loss: 0.20832301676273346\n",
      "Epoch: 342, batch loss: 0.20832176506519318\n",
      "Epoch: 343, batch loss: 0.2083204686641693\n",
      "Epoch: 344, batch loss: 0.20831914246082306\n",
      "Epoch: 345, batch loss: 0.20831790566444397\n",
      "Epoch: 346, batch loss: 0.2083166092634201\n",
      "Epoch: 347, batch loss: 0.20831535756587982\n",
      "Epoch: 348, batch loss: 0.20831407606601715\n",
      "Epoch: 349, batch loss: 0.2083127796649933\n",
      "Epoch: 350, batch loss: 0.2083115130662918\n",
      "Epoch: 351, batch loss: 0.20831021666526794\n",
      "Epoch: 352, batch loss: 0.20830892026424408\n",
      "Epoch: 353, batch loss: 0.2083076536655426\n",
      "Epoch: 354, batch loss: 0.20830638706684113\n",
      "Epoch: 355, batch loss: 0.20830509066581726\n",
      "Epoch: 356, batch loss: 0.2083038091659546\n",
      "Epoch: 357, batch loss: 0.2083025425672531\n",
      "Epoch: 358, batch loss: 0.20830127596855164\n",
      "Epoch: 359, batch loss: 0.20829997956752777\n",
      "Epoch: 360, batch loss: 0.2082986980676651\n",
      "Epoch: 361, batch loss: 0.20829744637012482\n",
      "Epoch: 362, batch loss: 0.20829616487026215\n",
      "Epoch: 363, batch loss: 0.20829491317272186\n",
      "Epoch: 364, batch loss: 0.20829357206821442\n",
      "Epoch: 365, batch loss: 0.20829233527183533\n",
      "Epoch: 366, batch loss: 0.20829103887081146\n",
      "Epoch: 367, batch loss: 0.20828977227210999\n",
      "Epoch: 368, batch loss: 0.20828846096992493\n",
      "Epoch: 369, batch loss: 0.20828722417354584\n",
      "Epoch: 370, batch loss: 0.20828589797019958\n",
      "Epoch: 371, batch loss: 0.2082846611738205\n",
      "Epoch: 372, batch loss: 0.20828336477279663\n",
      "Epoch: 373, batch loss: 0.20828211307525635\n",
      "Epoch: 374, batch loss: 0.2082807868719101\n",
      "Epoch: 375, batch loss: 0.208279550075531\n",
      "Epoch: 376, batch loss: 0.20827823877334595\n",
      "Epoch: 377, batch loss: 0.20827697217464447\n",
      "Epoch: 378, batch loss: 0.2082756757736206\n",
      "Epoch: 379, batch loss: 0.20827442407608032\n",
      "Epoch: 380, batch loss: 0.20827314257621765\n",
      "Epoch: 381, batch loss: 0.20827186107635498\n",
      "Epoch: 382, batch loss: 0.2082706093788147\n",
      "Epoch: 383, batch loss: 0.20826931297779083\n",
      "Epoch: 384, batch loss: 0.20826803147792816\n",
      "Epoch: 385, batch loss: 0.2082667499780655\n",
      "Epoch: 386, batch loss: 0.2082654982805252\n",
      "Epoch: 387, batch loss: 0.20826421678066254\n",
      "Epoch: 388, batch loss: 0.20826292037963867\n",
      "Epoch: 389, batch loss: 0.208261638879776\n",
      "Epoch: 390, batch loss: 0.20826038718223572\n",
      "Epoch: 391, batch loss: 0.20825910568237305\n",
      "Epoch: 392, batch loss: 0.208257794380188\n",
      "Epoch: 393, batch loss: 0.2082565724849701\n",
      "Epoch: 394, batch loss: 0.20825524628162384\n",
      "Epoch: 395, batch loss: 0.20825399458408356\n",
      "Epoch: 396, batch loss: 0.2082526832818985\n",
      "Epoch: 397, batch loss: 0.20825143158435822\n",
      "Epoch: 398, batch loss: 0.20825013518333435\n",
      "Epoch: 399, batch loss: 0.20824888348579407\n",
      "Epoch: 400, batch loss: 0.208247572183609\n",
      "Epoch: 401, batch loss: 0.2082463502883911\n",
      "Epoch: 402, batch loss: 0.20824502408504486\n",
      "Epoch: 403, batch loss: 0.20824378728866577\n",
      "Epoch: 404, batch loss: 0.2082424908876419\n",
      "Epoch: 405, batch loss: 0.20824120938777924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 406, batch loss: 0.20823991298675537\n",
      "Epoch: 407, batch loss: 0.20823867619037628\n",
      "Epoch: 408, batch loss: 0.20823736488819122\n",
      "Epoch: 409, batch loss: 0.20823609828948975\n",
      "Epoch: 410, batch loss: 0.20823483169078827\n",
      "Epoch: 411, batch loss: 0.2082335501909256\n",
      "Epoch: 412, batch loss: 0.20823225378990173\n",
      "Epoch: 413, batch loss: 0.20823097229003906\n",
      "Epoch: 414, batch loss: 0.20822973549365997\n",
      "Epoch: 415, batch loss: 0.2082284390926361\n",
      "Epoch: 416, batch loss: 0.20822715759277344\n",
      "Epoch: 417, batch loss: 0.20822590589523315\n",
      "Epoch: 418, batch loss: 0.20822462439537048\n",
      "Epoch: 419, batch loss: 0.2082233428955078\n",
      "Epoch: 420, batch loss: 0.20822204649448395\n",
      "Epoch: 421, batch loss: 0.20822077989578247\n",
      "Epoch: 422, batch loss: 0.208219513297081\n",
      "Epoch: 423, batch loss: 0.20821824669837952\n",
      "Epoch: 424, batch loss: 0.20821696519851685\n",
      "Epoch: 425, batch loss: 0.20821566879749298\n",
      "Epoch: 426, batch loss: 0.20821437239646912\n",
      "Epoch: 427, batch loss: 0.20821313560009003\n",
      "Epoch: 428, batch loss: 0.20821183919906616\n",
      "Epoch: 429, batch loss: 0.20821060240268707\n",
      "Epoch: 430, batch loss: 0.20820929110050201\n",
      "Epoch: 431, batch loss: 0.20820806920528412\n",
      "Epoch: 432, batch loss: 0.20820674300193787\n",
      "Epoch: 433, batch loss: 0.20820549130439758\n",
      "Epoch: 434, batch loss: 0.20820418000221252\n",
      "Epoch: 435, batch loss: 0.20820292830467224\n",
      "Epoch: 436, batch loss: 0.20820163190364838\n",
      "Epoch: 437, batch loss: 0.2082003951072693\n",
      "Epoch: 438, batch loss: 0.20819911360740662\n",
      "Epoch: 439, batch loss: 0.20819784700870514\n",
      "Epoch: 440, batch loss: 0.20819656550884247\n",
      "Epoch: 441, batch loss: 0.208195298910141\n",
      "Epoch: 442, batch loss: 0.20819398760795593\n",
      "Epoch: 443, batch loss: 0.20819273591041565\n",
      "Epoch: 444, batch loss: 0.20819145441055298\n",
      "Epoch: 445, batch loss: 0.2081901878118515\n",
      "Epoch: 446, batch loss: 0.20818887650966644\n",
      "Epoch: 447, batch loss: 0.20818762481212616\n",
      "Epoch: 448, batch loss: 0.20818635821342468\n",
      "Epoch: 449, batch loss: 0.2081851065158844\n",
      "Epoch: 450, batch loss: 0.20818381011486053\n",
      "Epoch: 451, batch loss: 0.20818252861499786\n",
      "Epoch: 452, batch loss: 0.2081812620162964\n",
      "Epoch: 453, batch loss: 0.2081800252199173\n",
      "Epoch: 454, batch loss: 0.20817871391773224\n",
      "Epoch: 455, batch loss: 0.20817743241786957\n",
      "Epoch: 456, batch loss: 0.20817619562149048\n",
      "Epoch: 457, batch loss: 0.2081749141216278\n",
      "Epoch: 458, batch loss: 0.20817363262176514\n",
      "Epoch: 459, batch loss: 0.20817233622074127\n",
      "Epoch: 460, batch loss: 0.2081710696220398\n",
      "Epoch: 461, batch loss: 0.2081698179244995\n",
      "Epoch: 462, batch loss: 0.20816852152347565\n",
      "Epoch: 463, batch loss: 0.20816722512245178\n",
      "Epoch: 464, batch loss: 0.2081659734249115\n",
      "Epoch: 465, batch loss: 0.20816469192504883\n",
      "Epoch: 466, batch loss: 0.20816345512866974\n",
      "Epoch: 467, batch loss: 0.2081621289253235\n",
      "Epoch: 468, batch loss: 0.2081608772277832\n",
      "Epoch: 469, batch loss: 0.20815962553024292\n",
      "Epoch: 470, batch loss: 0.20815835893154144\n",
      "Epoch: 471, batch loss: 0.20815704762935638\n",
      "Epoch: 472, batch loss: 0.2081557661294937\n",
      "Epoch: 473, batch loss: 0.20815446972846985\n",
      "Epoch: 474, batch loss: 0.20815324783325195\n",
      "Epoch: 475, batch loss: 0.2081519365310669\n",
      "Epoch: 476, batch loss: 0.208150714635849\n",
      "Epoch: 477, batch loss: 0.20814941823482513\n",
      "Epoch: 478, batch loss: 0.20814813673496246\n",
      "Epoch: 479, batch loss: 0.2081468552350998\n",
      "Epoch: 480, batch loss: 0.20814557373523712\n",
      "Epoch: 481, batch loss: 0.20814432203769684\n",
      "Epoch: 482, batch loss: 0.20814304053783417\n",
      "Epoch: 483, batch loss: 0.2081417590379715\n",
      "Epoch: 484, batch loss: 0.20814049243927002\n",
      "Epoch: 485, batch loss: 0.20813919603824615\n",
      "Epoch: 486, batch loss: 0.20813797414302826\n",
      "Epoch: 487, batch loss: 0.208136647939682\n",
      "Epoch: 488, batch loss: 0.20813541114330292\n",
      "Epoch: 489, batch loss: 0.20813411474227905\n",
      "Epoch: 490, batch loss: 0.20813286304473877\n",
      "Epoch: 491, batch loss: 0.2081315815448761\n",
      "Epoch: 492, batch loss: 0.20813030004501343\n",
      "Epoch: 493, batch loss: 0.20812901854515076\n",
      "Epoch: 494, batch loss: 0.20812775194644928\n",
      "Epoch: 495, batch loss: 0.2081264704465866\n",
      "Epoch: 496, batch loss: 0.20812523365020752\n",
      "Epoch: 497, batch loss: 0.20812393724918365\n",
      "Epoch: 498, batch loss: 0.20812267065048218\n",
      "Epoch: 499, batch loss: 0.2081213891506195\n",
      "Epoch: 500, batch loss: 0.20812013745307922\n",
      "Epoch: 501, batch loss: 0.20811885595321655\n",
      "Epoch: 502, batch loss: 0.2081175595521927\n",
      "Epoch: 503, batch loss: 0.2081163078546524\n",
      "Epoch: 504, batch loss: 0.20811505615711212\n",
      "Epoch: 505, batch loss: 0.20811378955841064\n",
      "Epoch: 506, batch loss: 0.20811249315738678\n",
      "Epoch: 507, batch loss: 0.2081112265586853\n",
      "Epoch: 508, batch loss: 0.20810993015766144\n",
      "Epoch: 509, batch loss: 0.20810867846012115\n",
      "Epoch: 510, batch loss: 0.2081073820590973\n",
      "Epoch: 511, batch loss: 0.208106130361557\n",
      "Epoch: 512, batch loss: 0.20810486376285553\n",
      "Epoch: 513, batch loss: 0.20810359716415405\n",
      "Epoch: 514, batch loss: 0.20810231566429138\n",
      "Epoch: 515, batch loss: 0.20810100436210632\n",
      "Epoch: 516, batch loss: 0.20809976756572723\n",
      "Epoch: 517, batch loss: 0.20809850096702576\n",
      "Epoch: 518, batch loss: 0.20809723436832428\n",
      "Epoch: 519, batch loss: 0.2080959528684616\n",
      "Epoch: 520, batch loss: 0.20809468626976013\n",
      "Epoch: 521, batch loss: 0.20809341967105865\n",
      "Epoch: 522, batch loss: 0.2080921232700348\n",
      "Epoch: 523, batch loss: 0.2080908566713333\n",
      "Epoch: 524, batch loss: 0.20808960497379303\n",
      "Epoch: 525, batch loss: 0.20808833837509155\n",
      "Epoch: 526, batch loss: 0.2080870419740677\n",
      "Epoch: 527, batch loss: 0.20808576047420502\n",
      "Epoch: 528, batch loss: 0.20808453857898712\n",
      "Epoch: 529, batch loss: 0.20808327198028564\n",
      "Epoch: 530, batch loss: 0.20808197557926178\n",
      "Epoch: 531, batch loss: 0.2080806940793991\n",
      "Epoch: 532, batch loss: 0.20807941257953644\n",
      "Epoch: 533, batch loss: 0.20807816088199615\n",
      "Epoch: 534, batch loss: 0.20807689428329468\n",
      "Epoch: 535, batch loss: 0.2080755978822708\n",
      "Epoch: 536, batch loss: 0.20807431638240814\n",
      "Epoch: 537, batch loss: 0.20807307958602905\n",
      "Epoch: 538, batch loss: 0.20807179808616638\n",
      "Epoch: 539, batch loss: 0.2080705463886261\n",
      "Epoch: 540, batch loss: 0.20806923508644104\n",
      "Epoch: 541, batch loss: 0.20806798338890076\n",
      "Epoch: 542, batch loss: 0.2080666869878769\n",
      "Epoch: 543, batch loss: 0.2080654352903366\n",
      "Epoch: 544, batch loss: 0.20806416869163513\n",
      "Epoch: 545, batch loss: 0.20806293189525604\n",
      "Epoch: 546, batch loss: 0.2080616056919098\n",
      "Epoch: 547, batch loss: 0.2080603390932083\n",
      "Epoch: 548, batch loss: 0.20805910229682922\n",
      "Epoch: 549, batch loss: 0.20805780589580536\n",
      "Epoch: 550, batch loss: 0.20805653929710388\n",
      "Epoch: 551, batch loss: 0.2080552726984024\n",
      "Epoch: 552, batch loss: 0.20805399119853973\n",
      "Epoch: 553, batch loss: 0.20805270969867706\n",
      "Epoch: 554, batch loss: 0.20805147290229797\n",
      "Epoch: 555, batch loss: 0.2080501765012741\n",
      "Epoch: 556, batch loss: 0.20804890990257263\n",
      "Epoch: 557, batch loss: 0.20804765820503235\n",
      "Epoch: 558, batch loss: 0.20804639160633087\n",
      "Epoch: 559, batch loss: 0.2080451250076294\n",
      "Epoch: 560, batch loss: 0.20804384350776672\n",
      "Epoch: 561, batch loss: 0.20804256200790405\n",
      "Epoch: 562, batch loss: 0.20804128050804138\n",
      "Epoch: 563, batch loss: 0.2080400437116623\n",
      "Epoch: 564, batch loss: 0.20803874731063843\n",
      "Epoch: 565, batch loss: 0.20803745090961456\n",
      "Epoch: 566, batch loss: 0.20803619921207428\n",
      "Epoch: 567, batch loss: 0.20803497731685638\n",
      "Epoch: 568, batch loss: 0.20803368091583252\n",
      "Epoch: 569, batch loss: 0.20803238451480865\n",
      "Epoch: 570, batch loss: 0.20803111791610718\n",
      "Epoch: 571, batch loss: 0.2080298662185669\n",
      "Epoch: 572, batch loss: 0.2080286145210266\n",
      "Epoch: 573, batch loss: 0.20802734792232513\n",
      "Epoch: 574, batch loss: 0.20802602171897888\n",
      "Epoch: 575, batch loss: 0.208024799823761\n",
      "Epoch: 576, batch loss: 0.2080235332250595\n",
      "Epoch: 577, batch loss: 0.20802225172519684\n",
      "Epoch: 578, batch loss: 0.20802094042301178\n",
      "Epoch: 579, batch loss: 0.20801971852779388\n",
      "Epoch: 580, batch loss: 0.20801842212677002\n",
      "Epoch: 581, batch loss: 0.20801717042922974\n",
      "Epoch: 582, batch loss: 0.20801590383052826\n",
      "Epoch: 583, batch loss: 0.2080146223306656\n",
      "Epoch: 584, batch loss: 0.2080133557319641\n",
      "Epoch: 585, batch loss: 0.20801210403442383\n",
      "Epoch: 586, batch loss: 0.20801080763339996\n",
      "Epoch: 587, batch loss: 0.2080095410346985\n",
      "Epoch: 588, batch loss: 0.208008274435997\n",
      "Epoch: 589, batch loss: 0.20800703763961792\n",
      "Epoch: 590, batch loss: 0.20800572633743286\n",
      "Epoch: 591, batch loss: 0.20800447463989258\n",
      "Epoch: 592, batch loss: 0.2080031931400299\n",
      "Epoch: 593, batch loss: 0.20800194144248962\n",
      "Epoch: 594, batch loss: 0.20800070464611053\n",
      "Epoch: 595, batch loss: 0.20799940824508667\n",
      "Epoch: 596, batch loss: 0.207998126745224\n",
      "Epoch: 597, batch loss: 0.20799684524536133\n",
      "Epoch: 598, batch loss: 0.20799559354782104\n",
      "Epoch: 599, batch loss: 0.20799432694911957\n",
      "Epoch: 600, batch loss: 0.2079930454492569\n",
      "Epoch: 601, batch loss: 0.20799177885055542\n",
      "Epoch: 602, batch loss: 0.20799051225185394\n",
      "Epoch: 603, batch loss: 0.20798926055431366\n",
      "Epoch: 604, batch loss: 0.207987979054451\n",
      "Epoch: 605, batch loss: 0.2079867124557495\n",
      "Epoch: 606, batch loss: 0.20798544585704803\n",
      "Epoch: 607, batch loss: 0.20798417925834656\n",
      "Epoch: 608, batch loss: 0.2079828828573227\n",
      "Epoch: 609, batch loss: 0.2079816460609436\n",
      "Epoch: 610, batch loss: 0.20798036456108093\n",
      "Epoch: 611, batch loss: 0.20797911286354065\n",
      "Epoch: 612, batch loss: 0.20797781646251678\n",
      "Epoch: 613, batch loss: 0.2079765647649765\n",
      "Epoch: 614, batch loss: 0.20797529816627502\n",
      "Epoch: 615, batch loss: 0.20797404646873474\n",
      "Epoch: 616, batch loss: 0.20797273516654968\n",
      "Epoch: 617, batch loss: 0.2079715132713318\n",
      "Epoch: 618, batch loss: 0.20797020196914673\n",
      "Epoch: 619, batch loss: 0.20796896517276764\n",
      "Epoch: 620, batch loss: 0.20796766877174377\n",
      "Epoch: 621, batch loss: 0.2079664170742035\n",
      "Epoch: 622, batch loss: 0.20796515047550201\n",
      "Epoch: 623, batch loss: 0.20796385407447815\n",
      "Epoch: 624, batch loss: 0.20796263217926025\n",
      "Epoch: 625, batch loss: 0.2079613357782364\n",
      "Epoch: 626, batch loss: 0.2079600840806961\n",
      "Epoch: 627, batch loss: 0.20795878767967224\n",
      "Epoch: 628, batch loss: 0.20795755088329315\n",
      "Epoch: 629, batch loss: 0.2079562544822693\n",
      "Epoch: 630, batch loss: 0.207955002784729\n",
      "Epoch: 631, batch loss: 0.20795370638370514\n",
      "Epoch: 632, batch loss: 0.20795245468616486\n",
      "Epoch: 633, batch loss: 0.20795120298862457\n",
      "Epoch: 634, batch loss: 0.20794996619224548\n",
      "Epoch: 635, batch loss: 0.20794862508773804\n",
      "Epoch: 636, batch loss: 0.20794738829135895\n",
      "Epoch: 637, batch loss: 0.20794612169265747\n",
      "Epoch: 638, batch loss: 0.2079448252916336\n",
      "Epoch: 639, batch loss: 0.20794357359409332\n",
      "Epoch: 640, batch loss: 0.20794230699539185\n",
      "Epoch: 641, batch loss: 0.20794104039669037\n",
      "Epoch: 642, batch loss: 0.2079397588968277\n",
      "Epoch: 643, batch loss: 0.20793849229812622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 644, batch loss: 0.20793725550174713\n",
      "Epoch: 645, batch loss: 0.20793598890304565\n",
      "Epoch: 646, batch loss: 0.20793470740318298\n",
      "Epoch: 647, batch loss: 0.2079334706068039\n",
      "Epoch: 648, batch loss: 0.20793218910694122\n",
      "Epoch: 649, batch loss: 0.20793095231056213\n",
      "Epoch: 650, batch loss: 0.2079296112060547\n",
      "Epoch: 651, batch loss: 0.20792840421199799\n",
      "Epoch: 652, batch loss: 0.2079271376132965\n",
      "Epoch: 653, batch loss: 0.20792585611343384\n",
      "Epoch: 654, batch loss: 0.20792455971240997\n",
      "Epoch: 655, batch loss: 0.20792333781719208\n",
      "Epoch: 656, batch loss: 0.2079220414161682\n",
      "Epoch: 657, batch loss: 0.20792075991630554\n",
      "Epoch: 658, batch loss: 0.20791950821876526\n",
      "Epoch: 659, batch loss: 0.20791824162006378\n",
      "Epoch: 660, batch loss: 0.2079170197248459\n",
      "Epoch: 661, batch loss: 0.20791572332382202\n",
      "Epoch: 662, batch loss: 0.20791444182395935\n",
      "Epoch: 663, batch loss: 0.20791317522525787\n",
      "Epoch: 664, batch loss: 0.20791195333003998\n",
      "Epoch: 665, batch loss: 0.20791062712669373\n",
      "Epoch: 666, batch loss: 0.20790937542915344\n",
      "Epoch: 667, batch loss: 0.20790810883045197\n",
      "Epoch: 668, batch loss: 0.2079068422317505\n",
      "Epoch: 669, batch loss: 0.207905575633049\n",
      "Epoch: 670, batch loss: 0.20790432393550873\n",
      "Epoch: 671, batch loss: 0.20790305733680725\n",
      "Epoch: 672, batch loss: 0.20790180563926697\n",
      "Epoch: 673, batch loss: 0.2079005092382431\n",
      "Epoch: 674, batch loss: 0.207899272441864\n",
      "Epoch: 675, batch loss: 0.20789796113967896\n",
      "Epoch: 676, batch loss: 0.20789675414562225\n",
      "Epoch: 677, batch loss: 0.20789547264575958\n",
      "Epoch: 678, batch loss: 0.20789417624473572\n",
      "Epoch: 679, batch loss: 0.20789295434951782\n",
      "Epoch: 680, batch loss: 0.20789164304733276\n",
      "Epoch: 681, batch loss: 0.20789039134979248\n",
      "Epoch: 682, batch loss: 0.207889124751091\n",
      "Epoch: 683, batch loss: 0.20788785815238953\n",
      "Epoch: 684, batch loss: 0.20788660645484924\n",
      "Epoch: 685, batch loss: 0.20788532495498657\n",
      "Epoch: 686, batch loss: 0.2078840583562851\n",
      "Epoch: 687, batch loss: 0.2078828066587448\n",
      "Epoch: 688, batch loss: 0.20788154006004333\n",
      "Epoch: 689, batch loss: 0.20788027346134186\n",
      "Epoch: 690, batch loss: 0.2078789919614792\n",
      "Epoch: 691, batch loss: 0.2078777253627777\n",
      "Epoch: 692, batch loss: 0.20787648856639862\n",
      "Epoch: 693, batch loss: 0.20787520706653595\n",
      "Epoch: 694, batch loss: 0.20787394046783447\n",
      "Epoch: 695, batch loss: 0.207872673869133\n",
      "Epoch: 696, batch loss: 0.2078714519739151\n",
      "Epoch: 697, batch loss: 0.20787014067173004\n",
      "Epoch: 698, batch loss: 0.20786887407302856\n",
      "Epoch: 699, batch loss: 0.2078675776720047\n",
      "Epoch: 700, batch loss: 0.2078663855791092\n",
      "Epoch: 701, batch loss: 0.20786510407924652\n",
      "Epoch: 702, batch loss: 0.20786382257938385\n",
      "Epoch: 703, batch loss: 0.20786255598068237\n",
      "Epoch: 704, batch loss: 0.2078612744808197\n",
      "Epoch: 705, batch loss: 0.20786000788211823\n",
      "Epoch: 706, batch loss: 0.20785878598690033\n",
      "Epoch: 707, batch loss: 0.20785748958587646\n",
      "Epoch: 708, batch loss: 0.20785625278949738\n",
      "Epoch: 709, batch loss: 0.2078549712896347\n",
      "Epoch: 710, batch loss: 0.20785370469093323\n",
      "Epoch: 711, batch loss: 0.20785245299339294\n",
      "Epoch: 712, batch loss: 0.20785115659236908\n",
      "Epoch: 713, batch loss: 0.20784991979599\n",
      "Epoch: 714, batch loss: 0.2078486680984497\n",
      "Epoch: 715, batch loss: 0.20784738659858704\n",
      "Epoch: 716, batch loss: 0.20784609019756317\n",
      "Epoch: 717, batch loss: 0.20784486830234528\n",
      "Epoch: 718, batch loss: 0.2078435868024826\n",
      "Epoch: 719, batch loss: 0.20784233510494232\n",
      "Epoch: 720, batch loss: 0.20784105360507965\n",
      "Epoch: 721, batch loss: 0.20783977210521698\n",
      "Epoch: 722, batch loss: 0.2078385353088379\n",
      "Epoch: 723, batch loss: 0.2078372836112976\n",
      "Epoch: 724, batch loss: 0.20783597230911255\n",
      "Epoch: 725, batch loss: 0.20783476531505585\n",
      "Epoch: 726, batch loss: 0.20783349871635437\n",
      "Epoch: 727, batch loss: 0.2078321874141693\n",
      "Epoch: 728, batch loss: 0.20783095061779022\n",
      "Epoch: 729, batch loss: 0.20782966911792755\n",
      "Epoch: 730, batch loss: 0.20782843232154846\n",
      "Epoch: 731, batch loss: 0.2078271359205246\n",
      "Epoch: 732, batch loss: 0.20782586932182312\n",
      "Epoch: 733, batch loss: 0.20782461762428284\n",
      "Epoch: 734, batch loss: 0.20782335102558136\n",
      "Epoch: 735, batch loss: 0.20782209932804108\n",
      "Epoch: 736, batch loss: 0.2078208178281784\n",
      "Epoch: 737, batch loss: 0.20781955122947693\n",
      "Epoch: 738, batch loss: 0.20781832933425903\n",
      "Epoch: 739, batch loss: 0.20781706273555756\n",
      "Epoch: 740, batch loss: 0.2078157365322113\n",
      "Epoch: 741, batch loss: 0.20781449973583221\n",
      "Epoch: 742, batch loss: 0.20781326293945312\n",
      "Epoch: 743, batch loss: 0.20781196653842926\n",
      "Epoch: 744, batch loss: 0.20781069993972778\n",
      "Epoch: 745, batch loss: 0.2078094482421875\n",
      "Epoch: 746, batch loss: 0.20780815184116364\n",
      "Epoch: 747, batch loss: 0.20780692994594574\n",
      "Epoch: 748, batch loss: 0.20780561864376068\n",
      "Epoch: 749, batch loss: 0.20780439674854279\n",
      "Epoch: 750, batch loss: 0.2078031301498413\n",
      "Epoch: 751, batch loss: 0.20780187845230103\n",
      "Epoch: 752, batch loss: 0.20780061185359955\n",
      "Epoch: 753, batch loss: 0.20779931545257568\n",
      "Epoch: 754, batch loss: 0.2077980786561966\n",
      "Epoch: 755, batch loss: 0.2077968269586563\n",
      "Epoch: 756, batch loss: 0.20779553055763245\n",
      "Epoch: 757, batch loss: 0.20779427886009216\n",
      "Epoch: 758, batch loss: 0.20779301226139069\n",
      "Epoch: 759, batch loss: 0.2077917605638504\n",
      "Epoch: 760, batch loss: 0.20779046416282654\n",
      "Epoch: 761, batch loss: 0.20778922736644745\n",
      "Epoch: 762, batch loss: 0.20778794586658478\n",
      "Epoch: 763, batch loss: 0.2077866792678833\n",
      "Epoch: 764, batch loss: 0.20778542757034302\n",
      "Epoch: 765, batch loss: 0.20778419077396393\n",
      "Epoch: 766, batch loss: 0.20778292417526245\n",
      "Epoch: 767, batch loss: 0.20778164267539978\n",
      "Epoch: 768, batch loss: 0.2077803611755371\n",
      "Epoch: 769, batch loss: 0.20777910947799683\n",
      "Epoch: 770, batch loss: 0.20777784287929535\n",
      "Epoch: 771, batch loss: 0.20777660608291626\n",
      "Epoch: 772, batch loss: 0.2077753245830536\n",
      "Epoch: 773, batch loss: 0.2077740877866745\n",
      "Epoch: 774, batch loss: 0.20777280628681183\n",
      "Epoch: 775, batch loss: 0.20777152478694916\n",
      "Epoch: 776, batch loss: 0.20777028799057007\n",
      "Epoch: 777, batch loss: 0.2077689915895462\n",
      "Epoch: 778, batch loss: 0.20776775479316711\n",
      "Epoch: 779, batch loss: 0.20776648819446564\n",
      "Epoch: 780, batch loss: 0.20776522159576416\n",
      "Epoch: 781, batch loss: 0.20776395499706268\n",
      "Epoch: 782, batch loss: 0.2077627182006836\n",
      "Epoch: 783, batch loss: 0.20776143670082092\n",
      "Epoch: 784, batch loss: 0.20776014029979706\n",
      "Epoch: 785, batch loss: 0.20775890350341797\n",
      "Epoch: 786, batch loss: 0.20775766670703888\n",
      "Epoch: 787, batch loss: 0.2077564299106598\n",
      "Epoch: 788, batch loss: 0.20775510370731354\n",
      "Epoch: 789, batch loss: 0.20775386691093445\n",
      "Epoch: 790, batch loss: 0.20775261521339417\n",
      "Epoch: 791, batch loss: 0.2077513486146927\n",
      "Epoch: 792, batch loss: 0.20775006711483002\n",
      "Epoch: 793, batch loss: 0.20774884521961212\n",
      "Epoch: 794, batch loss: 0.20774757862091064\n",
      "Epoch: 795, batch loss: 0.20774628221988678\n",
      "Epoch: 796, batch loss: 0.2077450156211853\n",
      "Epoch: 797, batch loss: 0.2077437937259674\n",
      "Epoch: 798, batch loss: 0.20774249732494354\n",
      "Epoch: 799, batch loss: 0.20774126052856445\n",
      "Epoch: 800, batch loss: 0.2077399641275406\n",
      "Epoch: 801, batch loss: 0.2077387422323227\n",
      "Epoch: 802, batch loss: 0.2077374905347824\n",
      "Epoch: 803, batch loss: 0.20773620903491974\n",
      "Epoch: 804, batch loss: 0.20773494243621826\n",
      "Epoch: 805, batch loss: 0.20773369073867798\n",
      "Epoch: 806, batch loss: 0.2077324241399765\n",
      "Epoch: 807, batch loss: 0.20773115754127502\n",
      "Epoch: 808, batch loss: 0.20772989094257355\n",
      "Epoch: 809, batch loss: 0.20772863924503326\n",
      "Epoch: 810, batch loss: 0.20772738754749298\n",
      "Epoch: 811, batch loss: 0.2077261358499527\n",
      "Epoch: 812, batch loss: 0.20772485435009003\n",
      "Epoch: 813, batch loss: 0.20772360265254974\n",
      "Epoch: 814, batch loss: 0.20772233605384827\n",
      "Epoch: 815, batch loss: 0.20772109925746918\n",
      "Epoch: 816, batch loss: 0.2077198028564453\n",
      "Epoch: 817, batch loss: 0.20771856606006622\n",
      "Epoch: 818, batch loss: 0.20771729946136475\n",
      "Epoch: 819, batch loss: 0.20771604776382446\n",
      "Epoch: 820, batch loss: 0.2077147513628006\n",
      "Epoch: 821, batch loss: 0.20771348476409912\n",
      "Epoch: 822, batch loss: 0.2077122926712036\n",
      "Epoch: 823, batch loss: 0.20771099627017975\n",
      "Epoch: 824, batch loss: 0.20770972967147827\n",
      "Epoch: 825, batch loss: 0.207708477973938\n",
      "Epoch: 826, batch loss: 0.2077072262763977\n",
      "Epoch: 827, batch loss: 0.20770595967769623\n",
      "Epoch: 828, batch loss: 0.20770469307899475\n",
      "Epoch: 829, batch loss: 0.20770344138145447\n",
      "Epoch: 830, batch loss: 0.2077021598815918\n",
      "Epoch: 831, batch loss: 0.2077009230852127\n",
      "Epoch: 832, batch loss: 0.20769965648651123\n",
      "Epoch: 833, batch loss: 0.20769838988780975\n",
      "Epoch: 834, batch loss: 0.20769712328910828\n",
      "Epoch: 835, batch loss: 0.20769590139389038\n",
      "Epoch: 836, batch loss: 0.2076946198940277\n",
      "Epoch: 837, batch loss: 0.20769336819648743\n",
      "Epoch: 838, batch loss: 0.20769210159778595\n",
      "Epoch: 839, batch loss: 0.20769083499908447\n",
      "Epoch: 840, batch loss: 0.207689568400383\n",
      "Epoch: 841, batch loss: 0.2076883316040039\n",
      "Epoch: 842, batch loss: 0.20768707990646362\n",
      "Epoch: 843, batch loss: 0.20768579840660095\n",
      "Epoch: 844, batch loss: 0.20768456161022186\n",
      "Epoch: 845, batch loss: 0.2076832801103592\n",
      "Epoch: 846, batch loss: 0.2076820582151413\n",
      "Epoch: 847, batch loss: 0.20768077671527863\n",
      "Epoch: 848, batch loss: 0.20767949521541595\n",
      "Epoch: 849, batch loss: 0.20767824351787567\n",
      "Epoch: 850, batch loss: 0.2076769769191742\n",
      "Epoch: 851, batch loss: 0.2076757550239563\n",
      "Epoch: 852, batch loss: 0.20767448842525482\n",
      "Epoch: 853, batch loss: 0.20767320692539215\n",
      "Epoch: 854, batch loss: 0.20767198503017426\n",
      "Epoch: 855, batch loss: 0.2076706886291504\n",
      "Epoch: 856, batch loss: 0.2076694369316101\n",
      "Epoch: 857, batch loss: 0.20766818523406982\n",
      "Epoch: 858, batch loss: 0.20766693353652954\n",
      "Epoch: 859, batch loss: 0.20766566693782806\n",
      "Epoch: 860, batch loss: 0.2076644003391266\n",
      "Epoch: 861, batch loss: 0.2076631486415863\n",
      "Epoch: 862, batch loss: 0.20766189694404602\n",
      "Epoch: 863, batch loss: 0.20766061544418335\n",
      "Epoch: 864, batch loss: 0.20765936374664307\n",
      "Epoch: 865, batch loss: 0.20765814185142517\n",
      "Epoch: 866, batch loss: 0.2076568454504013\n",
      "Epoch: 867, batch loss: 0.20765560865402222\n",
      "Epoch: 868, batch loss: 0.20765432715415955\n",
      "Epoch: 869, batch loss: 0.20765306055545807\n",
      "Epoch: 870, batch loss: 0.20765182375907898\n",
      "Epoch: 871, batch loss: 0.2076505422592163\n",
      "Epoch: 872, batch loss: 0.20764929056167603\n",
      "Epoch: 873, batch loss: 0.20764805376529694\n",
      "Epoch: 874, batch loss: 0.20764678716659546\n",
      "Epoch: 875, batch loss: 0.20764555037021637\n",
      "Epoch: 876, batch loss: 0.2076442539691925\n",
      "Epoch: 877, batch loss: 0.20764298737049103\n",
      "Epoch: 878, batch loss: 0.20764178037643433\n",
      "Epoch: 879, batch loss: 0.20764051377773285\n",
      "Epoch: 880, batch loss: 0.20763921737670898\n",
      "Epoch: 881, batch loss: 0.2076379954814911\n",
      "Epoch: 882, batch loss: 0.20763669908046722\n",
      "Epoch: 883, batch loss: 0.20763547718524933\n",
      "Epoch: 884, batch loss: 0.20763422548770905\n",
      "Epoch: 885, batch loss: 0.20763294398784637\n",
      "Epoch: 886, batch loss: 0.20763170719146729\n",
      "Epoch: 887, batch loss: 0.2076304405927658\n",
      "Epoch: 888, batch loss: 0.20762915909290314\n",
      "Epoch: 889, batch loss: 0.20762792229652405\n",
      "Epoch: 890, batch loss: 0.20762667059898376\n",
      "Epoch: 891, batch loss: 0.20762543380260468\n",
      "Epoch: 892, batch loss: 0.2076241672039032\n",
      "Epoch: 893, batch loss: 0.20762288570404053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 894, batch loss: 0.20762163400650024\n",
      "Epoch: 895, batch loss: 0.20762036740779877\n",
      "Epoch: 896, batch loss: 0.20761914551258087\n",
      "Epoch: 897, batch loss: 0.2076178342103958\n",
      "Epoch: 898, batch loss: 0.20761661231517792\n",
      "Epoch: 899, batch loss: 0.20761536061763763\n",
      "Epoch: 900, batch loss: 0.20761407911777496\n",
      "Epoch: 901, batch loss: 0.20761284232139587\n",
      "Epoch: 902, batch loss: 0.2076115757226944\n",
      "Epoch: 903, batch loss: 0.2076103240251541\n",
      "Epoch: 904, batch loss: 0.20760908722877502\n",
      "Epoch: 905, batch loss: 0.20760780572891235\n",
      "Epoch: 906, batch loss: 0.20760653913021088\n",
      "Epoch: 907, batch loss: 0.2076053023338318\n",
      "Epoch: 908, batch loss: 0.20760402083396912\n",
      "Epoch: 909, batch loss: 0.20760279893875122\n",
      "Epoch: 910, batch loss: 0.20760154724121094\n",
      "Epoch: 911, batch loss: 0.20760025084018707\n",
      "Epoch: 912, batch loss: 0.20759902894496918\n",
      "Epoch: 913, batch loss: 0.2075977325439453\n",
      "Epoch: 914, batch loss: 0.20759649574756622\n",
      "Epoch: 915, batch loss: 0.20759522914886475\n",
      "Epoch: 916, batch loss: 0.20759397745132446\n",
      "Epoch: 917, batch loss: 0.20759271085262299\n",
      "Epoch: 918, batch loss: 0.2075914591550827\n",
      "Epoch: 919, batch loss: 0.2075902372598648\n",
      "Epoch: 920, batch loss: 0.20758894085884094\n",
      "Epoch: 921, batch loss: 0.20758770406246185\n",
      "Epoch: 922, batch loss: 0.20758643746376038\n",
      "Epoch: 923, batch loss: 0.20758521556854248\n",
      "Epoch: 924, batch loss: 0.20758390426635742\n",
      "Epoch: 925, batch loss: 0.20758268237113953\n",
      "Epoch: 926, batch loss: 0.20758138597011566\n",
      "Epoch: 927, batch loss: 0.20758016407489777\n",
      "Epoch: 928, batch loss: 0.2075788825750351\n",
      "Epoch: 929, batch loss: 0.207577645778656\n",
      "Epoch: 930, batch loss: 0.2075764238834381\n",
      "Epoch: 931, batch loss: 0.20757514238357544\n",
      "Epoch: 932, batch loss: 0.20757387578487396\n",
      "Epoch: 933, batch loss: 0.20757260918617249\n",
      "Epoch: 934, batch loss: 0.2075713723897934\n",
      "Epoch: 935, batch loss: 0.20757009088993073\n",
      "Epoch: 936, batch loss: 0.20756885409355164\n",
      "Epoch: 937, batch loss: 0.20756757259368896\n",
      "Epoch: 938, batch loss: 0.20756635069847107\n",
      "Epoch: 939, batch loss: 0.20756511390209198\n",
      "Epoch: 940, batch loss: 0.20756380259990692\n",
      "Epoch: 941, batch loss: 0.20756258070468903\n",
      "Epoch: 942, batch loss: 0.20756131410598755\n",
      "Epoch: 943, batch loss: 0.20756006240844727\n",
      "Epoch: 944, batch loss: 0.2075587809085846\n",
      "Epoch: 945, batch loss: 0.2075575590133667\n",
      "Epoch: 946, batch loss: 0.20755630731582642\n",
      "Epoch: 947, batch loss: 0.20755507051944733\n",
      "Epoch: 948, batch loss: 0.20755377411842346\n",
      "Epoch: 949, batch loss: 0.20755250751972198\n",
      "Epoch: 950, batch loss: 0.20755130052566528\n",
      "Epoch: 951, batch loss: 0.20755000412464142\n",
      "Epoch: 952, batch loss: 0.20754875242710114\n",
      "Epoch: 953, batch loss: 0.20754751563072205\n",
      "Epoch: 954, batch loss: 0.20754627883434296\n",
      "Epoch: 955, batch loss: 0.20754499733448029\n",
      "Epoch: 956, batch loss: 0.2075437307357788\n",
      "Epoch: 957, batch loss: 0.20754249393939972\n",
      "Epoch: 958, batch loss: 0.20754124224185944\n",
      "Epoch: 959, batch loss: 0.20753996074199677\n",
      "Epoch: 960, batch loss: 0.20753872394561768\n",
      "Epoch: 961, batch loss: 0.2075374722480774\n",
      "Epoch: 962, batch loss: 0.20753620564937592\n",
      "Epoch: 963, batch loss: 0.20753496885299683\n",
      "Epoch: 964, batch loss: 0.20753368735313416\n",
      "Epoch: 965, batch loss: 0.20753245055675507\n",
      "Epoch: 966, batch loss: 0.20753119885921478\n",
      "Epoch: 967, batch loss: 0.20752990245819092\n",
      "Epoch: 968, batch loss: 0.20752868056297302\n",
      "Epoch: 969, batch loss: 0.20752744376659393\n",
      "Epoch: 970, batch loss: 0.20752616226673126\n",
      "Epoch: 971, batch loss: 0.20752491056919098\n",
      "Epoch: 972, batch loss: 0.2075236737728119\n",
      "Epoch: 973, batch loss: 0.20752239227294922\n",
      "Epoch: 974, batch loss: 0.20752115547657013\n",
      "Epoch: 975, batch loss: 0.20751987397670746\n",
      "Epoch: 976, batch loss: 0.20751863718032837\n",
      "Epoch: 977, batch loss: 0.20751741528511047\n",
      "Epoch: 978, batch loss: 0.2075161188840866\n",
      "Epoch: 979, batch loss: 0.2075148969888687\n",
      "Epoch: 980, batch loss: 0.20751361548900604\n",
      "Epoch: 981, batch loss: 0.20751236379146576\n",
      "Epoch: 982, batch loss: 0.20751112699508667\n",
      "Epoch: 983, batch loss: 0.2075098156929016\n",
      "Epoch: 984, batch loss: 0.2075086086988449\n",
      "Epoch: 985, batch loss: 0.20750734210014343\n",
      "Epoch: 986, batch loss: 0.20750606060028076\n",
      "Epoch: 987, batch loss: 0.20750480890274048\n",
      "Epoch: 988, batch loss: 0.20750358700752258\n",
      "Epoch: 989, batch loss: 0.2075023204088211\n",
      "Epoch: 990, batch loss: 0.2075011134147644\n",
      "Epoch: 991, batch loss: 0.20749977231025696\n",
      "Epoch: 992, batch loss: 0.20749856531620026\n",
      "Epoch: 993, batch loss: 0.20749732851982117\n",
      "Epoch: 994, batch loss: 0.2074960470199585\n",
      "Epoch: 995, batch loss: 0.2074947953224182\n",
      "Epoch: 996, batch loss: 0.20749355852603912\n",
      "Epoch: 997, batch loss: 0.20749230682849884\n",
      "Epoch: 998, batch loss: 0.20749105513095856\n",
      "Epoch: 999, batch loss: 0.2074897587299347\n",
      "Epoch: 1000, batch loss: 0.2074885219335556\n"
     ]
    }
   ],
   "source": [
    "# Batch gradient descent\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Preprocess the training data\n",
    "    data_batch = data_train.values\n",
    "    batch_input = torch.from_numpy(data_batch)\n",
    "    # Perform forward pass\n",
    "    batch_output = autoencoder(batch_input.float())\n",
    "    loss = criterion(batch_output, batch_input.float())\n",
    "    losses.append(loss)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: {}, batch loss: {}\".format(\n",
    "        epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2737,  0.5788, -1.2772,  0.6633,  0.5301, -2.0779,  0.5586,  0.5047,\n",
       "         0.5440,  0.7036], dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1628,  0.2995, -0.2047,  0.7543,  0.3145, -0.8917,  0.7201,  0.0270,\n",
       "         1.2516,  1.2944], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7979613580926649, 2.826950390151984e-23)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(batch_input.numpy().flatten(), batch_output.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8030690570010074, 0.005149130754613381)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(batch_input[0].numpy(), batch_output[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[0].numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check deep autoencoder alone on drug data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instianianate the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which data to use\n",
    "data_train = drug_data_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "# data_train = data_train.sample(frac=1.)\n",
    "# print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.456547008749136e-15 294.0 -6.81641338878148 1.2330512316262772\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "means = data_train.mean()\n",
    "stds = data_train.std()\n",
    "data_train = (data_train - means) / stds\n",
    "print(data_train.mean().sum(), data_train.std().sum(), data_train.min().min(), data_train.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 10)\n"
     ]
    }
   ],
   "source": [
    "# Take a sample of training data\n",
    "data_train = data_train.iloc[:, :10]\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) 20.590186153530404\n"
     ]
    }
   ],
   "source": [
    "data_train = data_train.sample(n=10, random_state=11)\n",
    "print(data_train.shape, data_train.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 294) 4.777844786474361e-13\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape, data_train.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=294, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=294, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instatianate the model\n",
    "torch.manual_seed(11)\n",
    "autoencoder = DeepAutoencoder(data_train.shape[1], 128, 5)\n",
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 294])\n",
      "torch.Size([128])\n",
      "torch.Size([5, 128])\n",
      "torch.Size([5])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128])\n",
      "torch.Size([294, 128])\n",
      "torch.Size([294])\n"
     ]
    }
   ],
   "source": [
    "for p in autoencoder.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "no_batches = data_train.shape[0] // batch_size + 1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch loss: 0.6508458256721497, val loss: 0.900892436504364\n",
      "Epoch: 2, batch loss: 0.5716217756271362, val loss: 0.7733346223831177\n",
      "Epoch: 3, batch loss: 0.4439236521720886, val loss: 0.6871200203895569\n",
      "Epoch: 4, batch loss: 0.3903379440307617, val loss: 0.6516532301902771\n",
      "Epoch: 5, batch loss: 0.3820187449455261, val loss: 0.6448106169700623\n",
      "Epoch: 6, batch loss: 0.3750286102294922, val loss: 0.6418816447257996\n",
      "Epoch: 7, batch loss: 0.36380457878112793, val loss: 0.638709545135498\n",
      "Epoch: 8, batch loss: 0.3465559184551239, val loss: 0.6334574222564697\n",
      "Epoch: 9, batch loss: 0.30373111367225647, val loss: 0.6206653714179993\n",
      "Epoch: 10, batch loss: 0.2623368501663208, val loss: 0.6033711433410645\n",
      "Epoch: 11, batch loss: 0.23159562051296234, val loss: 0.5850980281829834\n",
      "Epoch: 12, batch loss: 0.21829161047935486, val loss: 0.5725172162055969\n",
      "Epoch: 13, batch loss: 0.21471241116523743, val loss: 0.5577261447906494\n",
      "Epoch: 14, batch loss: 0.21089133620262146, val loss: 0.5480517745018005\n",
      "Epoch: 15, batch loss: 0.20925331115722656, val loss: 0.533988356590271\n",
      "Epoch: 16, batch loss: 0.2094344049692154, val loss: 0.5261991620063782\n",
      "Epoch: 17, batch loss: 0.2035362273454666, val loss: 0.5151547193527222\n",
      "Epoch: 18, batch loss: 0.1953330785036087, val loss: 0.5050923228263855\n",
      "Epoch: 19, batch loss: 0.18724802136421204, val loss: 0.49408432841300964\n",
      "Epoch: 20, batch loss: 0.18275795876979828, val loss: 0.4852798283100128\n",
      "Epoch: 21, batch loss: 0.16982075572013855, val loss: 0.4776447117328644\n",
      "Epoch: 22, batch loss: 0.16267752647399902, val loss: 0.47117385268211365\n",
      "Epoch: 23, batch loss: 0.15253722667694092, val loss: 0.46558281779289246\n",
      "Epoch: 24, batch loss: 0.1402164101600647, val loss: 0.4580729901790619\n",
      "Epoch: 25, batch loss: 0.13121072947978973, val loss: 0.4537023901939392\n",
      "Epoch: 26, batch loss: 0.12413302809000015, val loss: 0.44570493698120117\n",
      "Epoch: 27, batch loss: 0.11262183636426926, val loss: 0.4394415318965912\n",
      "Epoch: 28, batch loss: 0.10024183988571167, val loss: 0.4324458837509155\n",
      "Epoch: 29, batch loss: 0.0925414189696312, val loss: 0.4261411130428314\n",
      "Epoch: 30, batch loss: 0.08433147519826889, val loss: 0.41981253027915955\n",
      "Epoch: 31, batch loss: 0.07621058821678162, val loss: 0.41472217440605164\n",
      "Epoch: 32, batch loss: 0.06799715012311935, val loss: 0.4090496599674225\n",
      "Epoch: 33, batch loss: 0.06137041002511978, val loss: 0.40459325909614563\n",
      "Epoch: 34, batch loss: 0.05673134699463844, val loss: 0.3995361626148224\n",
      "Epoch: 35, batch loss: 0.05256635695695877, val loss: 0.3952068090438843\n",
      "Epoch: 36, batch loss: 0.04832294210791588, val loss: 0.39132383465766907\n",
      "Epoch: 37, batch loss: 0.04600148648023605, val loss: 0.38845595717430115\n",
      "Epoch: 38, batch loss: 0.04058246687054634, val loss: 0.3879586160182953\n",
      "Epoch: 39, batch loss: 0.04528933763504028, val loss: 0.40196382999420166\n",
      "Epoch: 40, batch loss: 0.041198525577783585, val loss: 0.41031691431999207\n",
      "Epoch: 41, batch loss: 0.055124834179878235, val loss: 0.39156678318977356\n",
      "Epoch: 42, batch loss: 0.0557132326066494, val loss: 0.39811256527900696\n",
      "Epoch: 43, batch loss: 0.05788391828536987, val loss: 0.38773947954177856\n",
      "Epoch: 44, batch loss: 0.04567350819706917, val loss: 0.38923582434654236\n",
      "Epoch: 45, batch loss: 0.048897553235292435, val loss: 0.37809568643569946\n",
      "Epoch: 46, batch loss: 0.040789179503917694, val loss: 0.37476295232772827\n",
      "Epoch: 47, batch loss: 0.04020112007856369, val loss: 0.36581167578697205\n",
      "Epoch: 48, batch loss: 0.03790488839149475, val loss: 0.3613860607147217\n",
      "Epoch: 49, batch loss: 0.03559132292866707, val loss: 0.3555951416492462\n",
      "Epoch: 50, batch loss: 0.03589922562241554, val loss: 0.3521227538585663\n",
      "Epoch: 51, batch loss: 0.029976530000567436, val loss: 0.3499189615249634\n",
      "Epoch: 52, batch loss: 0.03178852051496506, val loss: 0.34799715876579285\n",
      "Epoch: 53, batch loss: 0.032684724777936935, val loss: 0.3451387286186218\n",
      "Epoch: 54, batch loss: 0.02877439185976982, val loss: 0.34064748883247375\n",
      "Epoch: 55, batch loss: 0.027815384790301323, val loss: 0.3378358483314514\n",
      "Epoch: 56, batch loss: 0.0305518489331007, val loss: 0.33544832468032837\n",
      "Epoch: 57, batch loss: 0.029967689886689186, val loss: 0.3313237726688385\n",
      "Epoch: 58, batch loss: 0.029443679377436638, val loss: 0.3311767280101776\n",
      "Epoch: 59, batch loss: 0.030581001192331314, val loss: 0.3267062306404114\n",
      "Epoch: 60, batch loss: 0.026286037638783455, val loss: 0.32509711384773254\n",
      "Epoch: 61, batch loss: 0.02842806465923786, val loss: 0.3203728199005127\n",
      "Epoch: 62, batch loss: 0.02655000612139702, val loss: 0.3185535669326782\n",
      "Epoch: 63, batch loss: 0.030899031087756157, val loss: 0.3169119358062744\n",
      "Epoch: 64, batch loss: 0.028265774250030518, val loss: 0.316260427236557\n",
      "Epoch: 65, batch loss: 0.030580643564462662, val loss: 0.314657062292099\n",
      "Epoch: 66, batch loss: 0.02983907423913479, val loss: 0.3119795024394989\n",
      "Epoch: 67, batch loss: 0.0315902940928936, val loss: 0.30785083770751953\n",
      "Epoch: 68, batch loss: 0.02879101224243641, val loss: 0.3038013279438019\n",
      "Epoch: 69, batch loss: 0.030043059960007668, val loss: 0.3014574646949768\n",
      "Epoch: 70, batch loss: 0.02732023596763611, val loss: 0.2981356680393219\n",
      "Epoch: 71, batch loss: 0.03043932467699051, val loss: 0.29449036717414856\n",
      "Epoch: 72, batch loss: 0.02715722844004631, val loss: 0.29144197702407837\n",
      "Epoch: 73, batch loss: 0.029345950111746788, val loss: 0.2873707711696625\n",
      "Epoch: 74, batch loss: 0.024632008746266365, val loss: 0.28430554270744324\n",
      "Epoch: 75, batch loss: 0.027330083772540092, val loss: 0.28114089369773865\n",
      "Epoch: 76, batch loss: 0.02493877522647381, val loss: 0.2798483669757843\n",
      "Epoch: 77, batch loss: 0.02894633449614048, val loss: 0.277431458234787\n",
      "Epoch: 78, batch loss: 0.025066858157515526, val loss: 0.27730971574783325\n",
      "Epoch: 79, batch loss: 0.025531278923153877, val loss: 0.27372801303863525\n",
      "Epoch: 80, batch loss: 0.02465902641415596, val loss: 0.2714070677757263\n",
      "Epoch: 81, batch loss: 0.02721763774752617, val loss: 0.26483049988746643\n",
      "Epoch: 82, batch loss: 0.023617440834641457, val loss: 0.2631126642227173\n",
      "Epoch: 83, batch loss: 0.024264568462967873, val loss: 0.25934451818466187\n",
      "Epoch: 84, batch loss: 0.025470806285738945, val loss: 0.25824442505836487\n",
      "Epoch: 85, batch loss: 0.02568797394633293, val loss: 0.25414425134658813\n",
      "Epoch: 86, batch loss: 0.024486906826496124, val loss: 0.25327199697494507\n",
      "Epoch: 87, batch loss: 0.028081849217414856, val loss: 0.2486930936574936\n",
      "Epoch: 88, batch loss: 0.02073412947356701, val loss: 0.24633337557315826\n",
      "Epoch: 89, batch loss: 0.023557379841804504, val loss: 0.24236977100372314\n",
      "Epoch: 90, batch loss: 0.02012576535344124, val loss: 0.2427377551794052\n",
      "Epoch: 91, batch loss: 0.02336137741804123, val loss: 0.24151423573493958\n",
      "Epoch: 92, batch loss: 0.022179443389177322, val loss: 0.24121563136577606\n",
      "Epoch: 93, batch loss: 0.026124687865376472, val loss: 0.23472778499126434\n",
      "Epoch: 94, batch loss: 0.022678261622786522, val loss: 0.23110269010066986\n",
      "Epoch: 95, batch loss: 0.022863510996103287, val loss: 0.2273397594690323\n",
      "Epoch: 96, batch loss: 0.019692759960889816, val loss: 0.22571922838687897\n",
      "Epoch: 97, batch loss: 0.023586545139551163, val loss: 0.2225952446460724\n",
      "Epoch: 98, batch loss: 0.022017229348421097, val loss: 0.22140951454639435\n",
      "Epoch: 99, batch loss: 0.02162640169262886, val loss: 0.21957071125507355\n",
      "Epoch: 100, batch loss: 0.022149743512272835, val loss: 0.2203361988067627\n",
      "Epoch: 101, batch loss: 0.02571086771786213, val loss: 0.21768933534622192\n",
      "Epoch: 102, batch loss: 0.02542748861014843, val loss: 0.21514108777046204\n",
      "Epoch: 103, batch loss: 0.026067420840263367, val loss: 0.21617361903190613\n",
      "Epoch: 104, batch loss: 0.023080945014953613, val loss: 0.2146502435207367\n",
      "Epoch: 105, batch loss: 0.027455605566501617, val loss: 0.2121586799621582\n",
      "Epoch: 106, batch loss: 0.02914765104651451, val loss: 0.2158317118883133\n",
      "Epoch: 107, batch loss: 0.030729228630661964, val loss: 0.20762334764003754\n",
      "Epoch: 108, batch loss: 0.024306969717144966, val loss: 0.20294077694416046\n",
      "Epoch: 109, batch loss: 0.02728400193154812, val loss: 0.20495006442070007\n",
      "Epoch: 110, batch loss: 0.02259693294763565, val loss: 0.19928114116191864\n",
      "Epoch: 111, batch loss: 0.02255912497639656, val loss: 0.19675303995609283\n",
      "Epoch: 112, batch loss: 0.023041926324367523, val loss: 0.1977786421775818\n",
      "Epoch: 113, batch loss: 0.02433052659034729, val loss: 0.1942167431116104\n",
      "Epoch: 114, batch loss: 0.020165005698800087, val loss: 0.19060984253883362\n",
      "Epoch: 115, batch loss: 0.020240463316440582, val loss: 0.19063524901866913\n",
      "Epoch: 116, batch loss: 0.01918100379407406, val loss: 0.18799607455730438\n",
      "Epoch: 117, batch loss: 0.02073153853416443, val loss: 0.18581831455230713\n",
      "Epoch: 118, batch loss: 0.019417526200413704, val loss: 0.1860012710094452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119, batch loss: 0.02147871069610119, val loss: 0.18544691801071167\n",
      "Epoch: 120, batch loss: 0.02592608705163002, val loss: 0.18157969415187836\n",
      "Epoch: 121, batch loss: 0.018921369686722755, val loss: 0.1789424866437912\n",
      "Epoch: 122, batch loss: 0.020701857283711433, val loss: 0.18012721836566925\n",
      "Epoch: 123, batch loss: 0.03025292418897152, val loss: 0.176066592335701\n",
      "Epoch: 124, batch loss: 0.02524167113006115, val loss: 0.18112923204898834\n",
      "Epoch: 125, batch loss: 0.024260610342025757, val loss: 0.17969195544719696\n",
      "Epoch: 126, batch loss: 0.0332634411752224, val loss: 0.17708835005760193\n",
      "Epoch: 127, batch loss: 0.023056194186210632, val loss: 0.17891374230384827\n",
      "Epoch: 128, batch loss: 0.030866840854287148, val loss: 0.18010304868221283\n",
      "Epoch: 129, batch loss: 0.036470234394073486, val loss: 0.1751330941915512\n",
      "Epoch: 130, batch loss: 0.026048777624964714, val loss: 0.1816917359828949\n",
      "Epoch: 131, batch loss: 0.03171103447675705, val loss: 0.1832435131072998\n",
      "Epoch: 132, batch loss: 0.0365791842341423, val loss: 0.17629200220108032\n",
      "Epoch: 133, batch loss: 0.02640489675104618, val loss: 0.17242862284183502\n",
      "Epoch: 134, batch loss: 0.02337813936173916, val loss: 0.17490795254707336\n",
      "Epoch: 135, batch loss: 0.03403100743889809, val loss: 0.16748112440109253\n",
      "Epoch: 136, batch loss: 0.025052640587091446, val loss: 0.16394145786762238\n",
      "Epoch: 137, batch loss: 0.02291472814977169, val loss: 0.16697175800800323\n",
      "Epoch: 138, batch loss: 0.02950916439294815, val loss: 0.16296593844890594\n",
      "Epoch: 139, batch loss: 0.023492584004998207, val loss: 0.15551286935806274\n",
      "Epoch: 140, batch loss: 0.01694968342781067, val loss: 0.15636733174324036\n",
      "Epoch: 141, batch loss: 0.022399161010980606, val loss: 0.1570892035961151\n",
      "Epoch: 142, batch loss: 0.023224245756864548, val loss: 0.15288037061691284\n",
      "Epoch: 143, batch loss: 0.018433351069688797, val loss: 0.15205633640289307\n",
      "Epoch: 144, batch loss: 0.0218112301081419, val loss: 0.15592147409915924\n",
      "Epoch: 145, batch loss: 0.02150128409266472, val loss: 0.15122990310192108\n",
      "Epoch: 146, batch loss: 0.018473556265234947, val loss: 0.14939165115356445\n",
      "Epoch: 147, batch loss: 0.019945576786994934, val loss: 0.1510707288980484\n",
      "Epoch: 148, batch loss: 0.020673207938671112, val loss: 0.15093894302845\n",
      "Epoch: 149, batch loss: 0.018268780782818794, val loss: 0.14677922427654266\n",
      "Epoch: 150, batch loss: 0.02472858875989914, val loss: 0.15094047784805298\n",
      "Epoch: 151, batch loss: 0.02292228862643242, val loss: 0.14689983427524567\n",
      "Epoch: 152, batch loss: 0.019954858347773552, val loss: 0.14727908372879028\n",
      "Epoch: 153, batch loss: 0.022217988967895508, val loss: 0.15049539506435394\n",
      "Epoch: 154, batch loss: 0.023326866328716278, val loss: 0.14923037588596344\n",
      "Epoch: 155, batch loss: 0.027690978720784187, val loss: 0.1456543505191803\n",
      "Epoch: 156, batch loss: 0.020588871091604233, val loss: 0.148079052567482\n",
      "Epoch: 157, batch loss: 0.030377043411135674, val loss: 0.15401197969913483\n",
      "Epoch: 158, batch loss: 0.032318364828825, val loss: 0.1506069302558899\n",
      "Epoch: 159, batch loss: 0.02266491763293743, val loss: 0.15411901473999023\n",
      "Epoch: 160, batch loss: 0.027757832780480385, val loss: 0.15263772010803223\n",
      "Epoch: 161, batch loss: 0.028503775596618652, val loss: 0.14783570170402527\n",
      "Epoch: 162, batch loss: 0.021970871835947037, val loss: 0.14595520496368408\n",
      "Epoch: 163, batch loss: 0.022836383432149887, val loss: 0.14837685227394104\n",
      "Epoch: 164, batch loss: 0.02995571866631508, val loss: 0.14506885409355164\n",
      "Epoch: 165, batch loss: 0.020336123183369637, val loss: 0.14437639713287354\n",
      "Epoch: 166, batch loss: 0.024009650573134422, val loss: 0.1449812948703766\n",
      "Epoch: 167, batch loss: 0.0295126810669899, val loss: 0.14039577543735504\n",
      "Epoch: 168, batch loss: 0.019663356244564056, val loss: 0.13378645479679108\n",
      "Epoch: 169, batch loss: 0.018132437020540237, val loss: 0.1373707354068756\n",
      "Epoch: 170, batch loss: 0.0224385317414999, val loss: 0.13485994935035706\n",
      "Epoch: 171, batch loss: 0.01989174447953701, val loss: 0.1350632756948471\n",
      "Epoch: 172, batch loss: 0.018323929980397224, val loss: 0.1353539377450943\n",
      "Epoch: 173, batch loss: 0.02529425173997879, val loss: 0.13805516064167023\n",
      "Epoch: 174, batch loss: 0.024395547807216644, val loss: 0.1390388309955597\n",
      "Epoch: 175, batch loss: 0.018739258870482445, val loss: 0.13787822425365448\n",
      "Epoch: 176, batch loss: 0.023179033771157265, val loss: 0.13941293954849243\n",
      "Epoch: 177, batch loss: 0.026606416329741478, val loss: 0.13769397139549255\n",
      "Epoch: 178, batch loss: 0.01938248984515667, val loss: 0.13466154038906097\n",
      "Epoch: 179, batch loss: 0.02289542369544506, val loss: 0.13552068173885345\n",
      "Epoch: 180, batch loss: 0.022110022604465485, val loss: 0.13543863594532013\n",
      "Epoch: 181, batch loss: 0.019015081226825714, val loss: 0.13340111076831818\n",
      "Epoch: 182, batch loss: 0.01889469474554062, val loss: 0.13334476947784424\n",
      "Epoch: 183, batch loss: 0.024865927174687386, val loss: 0.1328640729188919\n",
      "Epoch: 184, batch loss: 0.02074267342686653, val loss: 0.12798815965652466\n",
      "Epoch: 185, batch loss: 0.01853002980351448, val loss: 0.12565428018569946\n",
      "Epoch: 186, batch loss: 0.018664352595806122, val loss: 0.12936128675937653\n",
      "Epoch: 187, batch loss: 0.021565573289990425, val loss: 0.12595108151435852\n",
      "Epoch: 188, batch loss: 0.017804227769374847, val loss: 0.12433478236198425\n",
      "Epoch: 189, batch loss: 0.023646632209420204, val loss: 0.1256532371044159\n",
      "Epoch: 190, batch loss: 0.02535504661500454, val loss: 0.12362711876630783\n",
      "Epoch: 191, batch loss: 0.018365662544965744, val loss: 0.1214674785733223\n",
      "Epoch: 192, batch loss: 0.018594548106193542, val loss: 0.1229245588183403\n",
      "Epoch: 193, batch loss: 0.02215590327978134, val loss: 0.1223164051771164\n",
      "Epoch: 194, batch loss: 0.01780877634882927, val loss: 0.12159428745508194\n",
      "Epoch: 195, batch loss: 0.019180553033947945, val loss: 0.12147749215364456\n",
      "Epoch: 196, batch loss: 0.021562444046139717, val loss: 0.12276451289653778\n",
      "Epoch: 197, batch loss: 0.018526386469602585, val loss: 0.12082947790622711\n",
      "Epoch: 198, batch loss: 0.019912637770175934, val loss: 0.1211981549859047\n",
      "Epoch: 199, batch loss: 0.023908812552690506, val loss: 0.12185315042734146\n",
      "Epoch: 200, batch loss: 0.021312294527888298, val loss: 0.12013903260231018\n",
      "Epoch: 201, batch loss: 0.02515956200659275, val loss: 0.11932115256786346\n",
      "Epoch: 202, batch loss: 0.024475201964378357, val loss: 0.12370140850543976\n",
      "Epoch: 203, batch loss: 0.02761188894510269, val loss: 0.12575402855873108\n",
      "Epoch: 204, batch loss: 0.027193985879421234, val loss: 0.12042620033025742\n",
      "Epoch: 205, batch loss: 0.024051014333963394, val loss: 0.12785302102565765\n",
      "Epoch: 206, batch loss: 0.037553008645772934, val loss: 0.12947611510753632\n",
      "Epoch: 207, batch loss: 0.0316251702606678, val loss: 0.12414829432964325\n",
      "Epoch: 208, batch loss: 0.02040393464267254, val loss: 0.12483731657266617\n",
      "Epoch: 209, batch loss: 0.02571495994925499, val loss: 0.12862305343151093\n",
      "Epoch: 210, batch loss: 0.034332942217588425, val loss: 0.1268261969089508\n",
      "Epoch: 211, batch loss: 0.02664836496114731, val loss: 0.1233704686164856\n",
      "Epoch: 212, batch loss: 0.021117504686117172, val loss: 0.12176807969808578\n",
      "Epoch: 213, batch loss: 0.023676175624132156, val loss: 0.12383024394512177\n",
      "Epoch: 214, batch loss: 0.024540267884731293, val loss: 0.11917521059513092\n",
      "Epoch: 215, batch loss: 0.021767830476164818, val loss: 0.11810825765132904\n",
      "Epoch: 216, batch loss: 0.017125193029642105, val loss: 0.12006886303424835\n",
      "Epoch: 217, batch loss: 0.01934191584587097, val loss: 0.1207260712981224\n",
      "Epoch: 218, batch loss: 0.021298803389072418, val loss: 0.117693692445755\n",
      "Epoch: 219, batch loss: 0.020973751321434975, val loss: 0.11686855554580688\n",
      "Epoch: 220, batch loss: 0.021594151854515076, val loss: 0.11680455505847931\n",
      "Epoch: 221, batch loss: 0.02674682065844536, val loss: 0.11805130541324615\n",
      "Epoch: 222, batch loss: 0.01893998123705387, val loss: 0.11975591629743576\n",
      "Epoch: 223, batch loss: 0.01909766159951687, val loss: 0.11980964243412018\n",
      "Epoch: 224, batch loss: 0.02242671139538288, val loss: 0.11760898679494858\n",
      "Epoch: 225, batch loss: 0.019710710272192955, val loss: 0.11259208619594574\n",
      "Epoch: 226, batch loss: 0.021059369668364525, val loss: 0.11328612267971039\n",
      "Epoch: 227, batch loss: 0.022940564900636673, val loss: 0.11313498765230179\n",
      "Epoch: 228, batch loss: 0.022349528968334198, val loss: 0.11268302798271179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229, batch loss: 0.022017929702997208, val loss: 0.11124210059642792\n",
      "Epoch: 230, batch loss: 0.020934494212269783, val loss: 0.11442101001739502\n",
      "Epoch: 231, batch loss: 0.02643008530139923, val loss: 0.10964150726795197\n",
      "Epoch: 232, batch loss: 0.026246383786201477, val loss: 0.10622750967741013\n",
      "Epoch: 233, batch loss: 0.020680174231529236, val loss: 0.11256536841392517\n",
      "Epoch: 234, batch loss: 0.02836301364004612, val loss: 0.11456210166215897\n",
      "Epoch: 235, batch loss: 0.025639399886131287, val loss: 0.10924576967954636\n",
      "Epoch: 236, batch loss: 0.017469732090830803, val loss: 0.11072894930839539\n",
      "Epoch: 237, batch loss: 0.021990865468978882, val loss: 0.11341742426156998\n",
      "Epoch: 238, batch loss: 0.030506527051329613, val loss: 0.10698077082633972\n",
      "Epoch: 239, batch loss: 0.018403945490717888, val loss: 0.10411001741886139\n",
      "Epoch: 240, batch loss: 0.03423056751489639, val loss: 0.10986312478780746\n",
      "Epoch: 241, batch loss: 0.052869461476802826, val loss: 0.11349359899759293\n",
      "Epoch: 242, batch loss: 0.02658294327557087, val loss: 0.11015167832374573\n",
      "Epoch: 243, batch loss: 0.018749909475445747, val loss: 0.1064436286687851\n",
      "Epoch: 244, batch loss: 0.02433736063539982, val loss: 0.11047150194644928\n",
      "Epoch: 245, batch loss: 0.025909701362252235, val loss: 0.10797527432441711\n",
      "Epoch: 246, batch loss: 0.019944922998547554, val loss: 0.10490042716264725\n",
      "Epoch: 247, batch loss: 0.02094215899705887, val loss: 0.109222911298275\n",
      "Epoch: 248, batch loss: 0.0241205133497715, val loss: 0.11127039045095444\n",
      "Epoch: 249, batch loss: 0.02445898950099945, val loss: 0.10703305155038834\n",
      "Epoch: 250, batch loss: 0.018170952796936035, val loss: 0.10414581745862961\n",
      "Epoch: 251, batch loss: 0.017978381365537643, val loss: 0.10838130116462708\n",
      "Epoch: 252, batch loss: 0.021514132618904114, val loss: 0.10745079070329666\n",
      "Epoch: 253, batch loss: 0.02447594702243805, val loss: 0.10154976695775986\n",
      "Epoch: 254, batch loss: 0.020298264920711517, val loss: 0.10080577433109283\n",
      "Epoch: 255, batch loss: 0.018243547528982162, val loss: 0.10099244862794876\n",
      "Epoch: 256, batch loss: 0.023874396458268166, val loss: 0.10077321529388428\n",
      "Epoch: 257, batch loss: 0.024199072271585464, val loss: 0.09801752120256424\n",
      "Epoch: 258, batch loss: 0.018823059275746346, val loss: 0.10137498378753662\n",
      "Epoch: 259, batch loss: 0.021172380074858665, val loss: 0.10198323428630829\n",
      "Epoch: 260, batch loss: 0.02198558859527111, val loss: 0.09853528439998627\n",
      "Epoch: 261, batch loss: 0.01676803268492222, val loss: 0.0988280400633812\n",
      "Epoch: 262, batch loss: 0.020076608285307884, val loss: 0.10349991172552109\n",
      "Epoch: 263, batch loss: 0.024479707702994347, val loss: 0.10126771777868271\n",
      "Epoch: 264, batch loss: 0.017068391665816307, val loss: 0.09867183119058609\n",
      "Epoch: 265, batch loss: 0.018308507278561592, val loss: 0.10380212962627411\n",
      "Epoch: 266, batch loss: 0.02465319074690342, val loss: 0.10664833337068558\n",
      "Epoch: 267, batch loss: 0.026274463161826134, val loss: 0.10241425037384033\n",
      "Epoch: 268, batch loss: 0.019554901868104935, val loss: 0.09912215918302536\n",
      "Epoch: 269, batch loss: 0.019665902480483055, val loss: 0.10255829989910126\n",
      "Epoch: 270, batch loss: 0.022646144032478333, val loss: 0.1012803390622139\n",
      "Epoch: 271, batch loss: 0.019604278728365898, val loss: 0.09645126760005951\n",
      "Epoch: 272, batch loss: 0.018600180745124817, val loss: 0.09898678958415985\n",
      "Epoch: 273, batch loss: 0.023464934900403023, val loss: 0.10272987931966782\n",
      "Epoch: 274, batch loss: 0.023448476567864418, val loss: 0.10205946862697601\n",
      "Epoch: 275, batch loss: 0.016465162858366966, val loss: 0.10198761522769928\n",
      "Epoch: 276, batch loss: 0.023049021139740944, val loss: 0.10455332696437836\n",
      "Epoch: 277, batch loss: 0.025637121871113777, val loss: 0.10181061178445816\n",
      "Epoch: 278, batch loss: 0.018906131386756897, val loss: 0.09536689519882202\n",
      "Epoch: 279, batch loss: 0.020084084942936897, val loss: 0.09792724251747131\n",
      "Epoch: 280, batch loss: 0.023094255477190018, val loss: 0.1013028472661972\n",
      "Epoch: 281, batch loss: 0.0227353572845459, val loss: 0.1009039655327797\n",
      "Epoch: 282, batch loss: 0.02216888591647148, val loss: 0.09945564717054367\n",
      "Epoch: 283, batch loss: 0.02099878527224064, val loss: 0.1033296212553978\n",
      "Epoch: 284, batch loss: 0.024861358106136322, val loss: 0.10528265684843063\n",
      "Epoch: 285, batch loss: 0.024439914152026176, val loss: 0.09903205186128616\n",
      "Epoch: 286, batch loss: 0.020866412669420242, val loss: 0.09905527532100677\n",
      "Epoch: 287, batch loss: 0.02176985703408718, val loss: 0.1021086648106575\n",
      "Epoch: 288, batch loss: 0.023437539115548134, val loss: 0.10341031104326248\n",
      "Epoch: 289, batch loss: 0.018863791599869728, val loss: 0.10037556290626526\n",
      "Epoch: 290, batch loss: 0.02026393823325634, val loss: 0.0983615294098854\n",
      "Epoch: 291, batch loss: 0.02003229595720768, val loss: 0.09851699322462082\n",
      "Epoch: 292, batch loss: 0.01695231907069683, val loss: 0.09594715386629105\n",
      "Epoch: 293, batch loss: 0.020565001294016838, val loss: 0.09313754737377167\n",
      "Epoch: 294, batch loss: 0.017817508429288864, val loss: 0.0911940485239029\n",
      "Epoch: 295, batch loss: 0.015833796933293343, val loss: 0.09113126993179321\n",
      "Epoch: 296, batch loss: 0.019640373066067696, val loss: 0.09323757141828537\n",
      "Epoch: 297, batch loss: 0.017977330833673477, val loss: 0.09279923886060715\n",
      "Epoch: 298, batch loss: 0.01847943104803562, val loss: 0.08871135860681534\n",
      "Epoch: 299, batch loss: 0.01677968166768551, val loss: 0.08918049186468124\n",
      "Epoch: 300, batch loss: 0.01812969334423542, val loss: 0.09067153185606003\n",
      "Epoch: 301, batch loss: 0.020162934437394142, val loss: 0.08815484493970871\n",
      "Epoch: 302, batch loss: 0.01604795642197132, val loss: 0.0870864987373352\n",
      "Epoch: 303, batch loss: 0.017559068277478218, val loss: 0.09146988391876221\n",
      "Epoch: 304, batch loss: 0.02180045284330845, val loss: 0.09066113084554672\n",
      "Epoch: 305, batch loss: 0.015507555566728115, val loss: 0.08874154090881348\n",
      "Epoch: 306, batch loss: 0.019016209989786148, val loss: 0.09013305604457855\n",
      "Epoch: 307, batch loss: 0.0207532849162817, val loss: 0.08941615372896194\n",
      "Epoch: 308, batch loss: 0.021124914288520813, val loss: 0.08825269341468811\n",
      "Epoch: 309, batch loss: 0.018741216510534286, val loss: 0.08568843454122543\n",
      "Epoch: 310, batch loss: 0.01955215446650982, val loss: 0.09061963856220245\n",
      "Epoch: 311, batch loss: 0.021148286759853363, val loss: 0.09422475844621658\n",
      "Epoch: 312, batch loss: 0.026715580374002457, val loss: 0.09344735741615295\n",
      "Epoch: 313, batch loss: 0.0265578031539917, val loss: 0.09396987408399582\n",
      "Epoch: 314, batch loss: 0.022866632789373398, val loss: 0.09304946660995483\n",
      "Epoch: 315, batch loss: 0.022150468081235886, val loss: 0.09250235557556152\n",
      "Epoch: 316, batch loss: 0.02080695517361164, val loss: 0.08859704434871674\n",
      "Epoch: 317, batch loss: 0.019503362476825714, val loss: 0.09165627509355545\n",
      "Epoch: 318, batch loss: 0.020805876702070236, val loss: 0.08995261043310165\n",
      "Epoch: 319, batch loss: 0.01676325872540474, val loss: 0.08688171952962875\n",
      "Epoch: 320, batch loss: 0.019738715142011642, val loss: 0.09147076308727264\n",
      "Epoch: 321, batch loss: 0.020246973261237144, val loss: 0.08932783454656601\n",
      "Epoch: 322, batch loss: 0.01806321181356907, val loss: 0.0851842612028122\n",
      "Epoch: 323, batch loss: 0.018860535696148872, val loss: 0.08571141958236694\n",
      "Epoch: 324, batch loss: 0.016818011179566383, val loss: 0.08566432446241379\n",
      "Epoch: 325, batch loss: 0.017859308049082756, val loss: 0.0847216546535492\n",
      "Epoch: 326, batch loss: 0.01898154243826866, val loss: 0.08307916671037674\n",
      "Epoch: 327, batch loss: 0.017679471522569656, val loss: 0.08426497876644135\n",
      "Epoch: 328, batch loss: 0.020215116441249847, val loss: 0.08375820517539978\n",
      "Epoch: 329, batch loss: 0.017031461000442505, val loss: 0.08187815546989441\n",
      "Epoch: 330, batch loss: 0.01790962740778923, val loss: 0.08374713361263275\n",
      "Epoch: 331, batch loss: 0.020642941817641258, val loss: 0.0846361368894577\n",
      "Epoch: 332, batch loss: 0.01721818931400776, val loss: 0.08422455191612244\n",
      "Epoch: 333, batch loss: 0.01858319342136383, val loss: 0.0839606374502182\n",
      "Epoch: 334, batch loss: 0.02144184336066246, val loss: 0.0838373452425003\n",
      "Epoch: 335, batch loss: 0.017325319349765778, val loss: 0.08502496033906937\n",
      "Epoch: 336, batch loss: 0.02115415409207344, val loss: 0.08418942987918854\n",
      "Epoch: 337, batch loss: 0.020378144457936287, val loss: 0.08286716789007187\n",
      "Epoch: 338, batch loss: 0.01644667237997055, val loss: 0.08272641152143478\n",
      "Epoch: 339, batch loss: 0.018877338618040085, val loss: 0.08358581364154816\n",
      "Epoch: 340, batch loss: 0.023021863773465157, val loss: 0.08466412872076035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 341, batch loss: 0.01807287335395813, val loss: 0.08218368142843246\n",
      "Epoch: 342, batch loss: 0.016935614868998528, val loss: 0.08289841562509537\n",
      "Epoch: 343, batch loss: 0.01784176379442215, val loss: 0.08317013084888458\n",
      "Epoch: 344, batch loss: 0.016503265127539635, val loss: 0.08316757529973984\n",
      "Epoch: 345, batch loss: 0.01915113627910614, val loss: 0.08412788063287735\n",
      "Epoch: 346, batch loss: 0.017186565324664116, val loss: 0.08461830765008926\n",
      "Epoch: 347, batch loss: 0.018833424896001816, val loss: 0.08607728034257889\n",
      "Epoch: 348, batch loss: 0.01932700350880623, val loss: 0.08380240201950073\n",
      "Epoch: 349, batch loss: 0.018389031291007996, val loss: 0.08025672286748886\n",
      "Epoch: 350, batch loss: 0.019839327782392502, val loss: 0.08382752537727356\n",
      "Epoch: 351, batch loss: 0.022241979837417603, val loss: 0.08370348066091537\n",
      "Epoch: 352, batch loss: 0.01746547222137451, val loss: 0.07920020818710327\n",
      "Epoch: 353, batch loss: 0.016581658273935318, val loss: 0.08004280179738998\n",
      "Epoch: 354, batch loss: 0.01991361565887928, val loss: 0.08477261662483215\n",
      "Epoch: 355, batch loss: 0.01867077872157097, val loss: 0.08755002915859222\n",
      "Epoch: 356, batch loss: 0.023443063721060753, val loss: 0.0870758518576622\n",
      "Epoch: 357, batch loss: 0.019136618822813034, val loss: 0.08165644109249115\n",
      "Epoch: 358, batch loss: 0.0188733097165823, val loss: 0.08259497582912445\n",
      "Epoch: 359, batch loss: 0.020742811262607574, val loss: 0.07836463302373886\n",
      "Epoch: 360, batch loss: 0.01537187397480011, val loss: 0.07574709504842758\n",
      "Epoch: 361, batch loss: 0.01601361855864525, val loss: 0.07633338123559952\n",
      "Epoch: 362, batch loss: 0.01616646721959114, val loss: 0.07608190178871155\n",
      "Epoch: 363, batch loss: 0.016759619116783142, val loss: 0.07706046849489212\n",
      "Epoch: 364, batch loss: 0.019425224512815475, val loss: 0.07877953350543976\n",
      "Epoch: 365, batch loss: 0.020860953256487846, val loss: 0.08290795981884003\n",
      "Epoch: 366, batch loss: 0.016252778470516205, val loss: 0.0774887427687645\n",
      "Epoch: 367, batch loss: 0.0202214103192091, val loss: 0.0774943009018898\n",
      "Epoch: 368, batch loss: 0.020364265888929367, val loss: 0.07757192105054855\n",
      "Epoch: 369, batch loss: 0.017111413180828094, val loss: 0.07862144708633423\n",
      "Epoch: 370, batch loss: 0.019828887656331062, val loss: 0.0784110501408577\n",
      "Epoch: 371, batch loss: 0.019587140530347824, val loss: 0.07518080621957779\n",
      "Epoch: 372, batch loss: 0.021177303045988083, val loss: 0.0793665274977684\n",
      "Epoch: 373, batch loss: 0.018540875986218452, val loss: 0.08080371469259262\n",
      "Epoch: 374, batch loss: 0.018290139734745026, val loss: 0.07796113938093185\n",
      "Epoch: 375, batch loss: 0.02136368863284588, val loss: 0.07916705310344696\n",
      "Epoch: 376, batch loss: 0.024624787271022797, val loss: 0.08043196052312851\n",
      "Epoch: 377, batch loss: 0.019959522411227226, val loss: 0.07593511044979095\n",
      "Epoch: 378, batch loss: 0.019810393452644348, val loss: 0.07720071077346802\n",
      "Epoch: 379, batch loss: 0.02397155575454235, val loss: 0.08051523566246033\n",
      "Epoch: 380, batch loss: 0.02067788690328598, val loss: 0.08110587298870087\n",
      "Epoch: 381, batch loss: 0.02028065361082554, val loss: 0.08021228760480881\n",
      "Epoch: 382, batch loss: 0.017597438767552376, val loss: 0.08345667272806168\n",
      "Epoch: 383, batch loss: 0.03151051700115204, val loss: 0.08885754644870758\n",
      "Epoch: 384, batch loss: 0.027172032743692398, val loss: 0.08426021784543991\n",
      "Epoch: 385, batch loss: 0.026021962985396385, val loss: 0.07851919531822205\n",
      "Epoch: 386, batch loss: 0.021798668429255486, val loss: 0.08063729852437973\n",
      "Epoch: 387, batch loss: 0.021123282611370087, val loss: 0.08193250745534897\n",
      "Epoch: 388, batch loss: 0.021939368918538094, val loss: 0.07690924406051636\n",
      "Epoch: 389, batch loss: 0.017745714634656906, val loss: 0.07253162562847137\n",
      "Epoch: 390, batch loss: 0.016288362443447113, val loss: 0.07668141275644302\n",
      "Epoch: 391, batch loss: 0.019947882741689682, val loss: 0.07989650219678879\n",
      "Epoch: 392, batch loss: 0.016890093684196472, val loss: 0.08141879737377167\n",
      "Epoch: 393, batch loss: 0.028272485360503197, val loss: 0.08172448724508286\n",
      "Epoch: 394, batch loss: 0.04144970700144768, val loss: 0.0844009518623352\n",
      "Epoch: 395, batch loss: 0.025071443989872932, val loss: 0.08134420961141586\n",
      "Epoch: 396, batch loss: 0.024044562131166458, val loss: 0.07762320339679718\n",
      "Epoch: 397, batch loss: 0.01891510561108589, val loss: 0.07586222887039185\n",
      "Epoch: 398, batch loss: 0.01741039752960205, val loss: 0.07655050605535507\n",
      "Epoch: 399, batch loss: 0.022357458248734474, val loss: 0.07743673771619797\n",
      "Epoch: 400, batch loss: 0.02058854140341282, val loss: 0.07748240232467651\n",
      "Epoch: 401, batch loss: 0.019289296120405197, val loss: 0.07568017393350601\n",
      "Epoch: 402, batch loss: 0.01880108378827572, val loss: 0.07563895732164383\n",
      "Epoch: 403, batch loss: 0.01797393150627613, val loss: 0.0709550753235817\n",
      "Epoch: 404, batch loss: 0.01746281050145626, val loss: 0.07041643559932709\n",
      "Epoch: 405, batch loss: 0.017283957451581955, val loss: 0.07159905135631561\n",
      "Epoch: 406, batch loss: 0.014817631803452969, val loss: 0.07004356384277344\n",
      "Epoch: 407, batch loss: 0.017658527940511703, val loss: 0.06886886805295944\n",
      "Epoch: 408, batch loss: 0.0164939034730196, val loss: 0.06890447437763214\n",
      "Epoch: 409, batch loss: 0.016804441809654236, val loss: 0.06750105321407318\n",
      "Epoch: 410, batch loss: 0.01538542378693819, val loss: 0.06774822622537613\n",
      "Epoch: 411, batch loss: 0.015873635187745094, val loss: 0.06721217185258865\n",
      "Epoch: 412, batch loss: 0.015611016191542149, val loss: 0.06691370159387589\n",
      "Epoch: 413, batch loss: 0.01622546836733818, val loss: 0.06717787683010101\n",
      "Epoch: 414, batch loss: 0.016104307025671005, val loss: 0.06767108291387558\n",
      "Epoch: 415, batch loss: 0.01855548471212387, val loss: 0.06703667342662811\n",
      "Epoch: 416, batch loss: 0.014517477713525295, val loss: 0.06562811881303787\n",
      "Epoch: 417, batch loss: 0.016627058386802673, val loss: 0.06768623739480972\n",
      "Epoch: 418, batch loss: 0.017267394810914993, val loss: 0.06691465526819229\n",
      "Epoch: 419, batch loss: 0.014714175835251808, val loss: 0.0654640644788742\n",
      "Epoch: 420, batch loss: 0.015834059566259384, val loss: 0.06703320890665054\n",
      "Epoch: 421, batch loss: 0.01840590499341488, val loss: 0.06886984407901764\n",
      "Epoch: 422, batch loss: 0.01976427622139454, val loss: 0.07075148075819016\n",
      "Epoch: 423, batch loss: 0.021789880469441414, val loss: 0.0702684074640274\n",
      "Epoch: 424, batch loss: 0.017838751897215843, val loss: 0.07126311957836151\n",
      "Epoch: 425, batch loss: 0.017299195751547813, val loss: 0.07071944326162338\n",
      "Epoch: 426, batch loss: 0.020294535905122757, val loss: 0.06863044947385788\n",
      "Epoch: 427, batch loss: 0.016905995085835457, val loss: 0.0679783970117569\n",
      "Epoch: 428, batch loss: 0.01840956322848797, val loss: 0.06955398619174957\n",
      "Epoch: 429, batch loss: 0.01823994889855385, val loss: 0.07087746262550354\n",
      "Epoch: 430, batch loss: 0.01832001842558384, val loss: 0.07257633656263351\n",
      "Epoch: 431, batch loss: 0.01755010150372982, val loss: 0.0741436555981636\n",
      "Epoch: 432, batch loss: 0.024391083046793938, val loss: 0.08461017906665802\n",
      "Epoch: 433, batch loss: 0.03220021352171898, val loss: 0.08004039525985718\n",
      "Epoch: 434, batch loss: 0.0389106310904026, val loss: 0.0718880146741867\n",
      "Epoch: 435, batch loss: 0.021371066570281982, val loss: 0.07416900992393494\n",
      "Epoch: 436, batch loss: 0.018420511856675148, val loss: 0.07883895933628082\n",
      "Epoch: 437, batch loss: 0.02549109421670437, val loss: 0.08519536256790161\n",
      "Epoch: 438, batch loss: 0.023911844938993454, val loss: 0.07818007469177246\n",
      "Epoch: 439, batch loss: 0.016653578728437424, val loss: 0.07457353919744492\n",
      "Epoch: 440, batch loss: 0.017413320019841194, val loss: 0.07588205486536026\n",
      "Epoch: 441, batch loss: 0.01784016378223896, val loss: 0.07322005927562714\n",
      "Epoch: 442, batch loss: 0.017714647576212883, val loss: 0.06821353733539581\n",
      "Epoch: 443, batch loss: 0.018078021705150604, val loss: 0.06855718791484833\n",
      "Epoch: 444, batch loss: 0.01822129264473915, val loss: 0.0683039203286171\n",
      "Epoch: 445, batch loss: 0.016961589455604553, val loss: 0.06692171841859818\n",
      "Epoch: 446, batch loss: 0.01583324745297432, val loss: 0.06578341126441956\n",
      "Epoch: 447, batch loss: 0.017783312126994133, val loss: 0.06630756705999374\n",
      "Epoch: 448, batch loss: 0.01621554233133793, val loss: 0.06870297342538834\n",
      "Epoch: 449, batch loss: 0.016760768368840218, val loss: 0.06943225860595703\n",
      "Epoch: 450, batch loss: 0.01991916261613369, val loss: 0.06744928658008575\n",
      "Epoch: 451, batch loss: 0.017702465876936913, val loss: 0.06825053691864014\n",
      "Epoch: 452, batch loss: 0.016653848811984062, val loss: 0.06881149858236313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453, batch loss: 0.024149786680936813, val loss: 0.07088125497102737\n",
      "Epoch: 454, batch loss: 0.02120434120297432, val loss: 0.06744296848773956\n",
      "Epoch: 455, batch loss: 0.016972053796052933, val loss: 0.06334961950778961\n",
      "Epoch: 456, batch loss: 0.01649653911590576, val loss: 0.06590437144041061\n",
      "Epoch: 457, batch loss: 0.017306147143244743, val loss: 0.06649699807167053\n",
      "Epoch: 458, batch loss: 0.019292663782835007, val loss: 0.06555013358592987\n",
      "Epoch: 459, batch loss: 0.017061742022633553, val loss: 0.06380265206098557\n",
      "Epoch: 460, batch loss: 0.01706847734749317, val loss: 0.06402112543582916\n",
      "Epoch: 461, batch loss: 0.022561796009540558, val loss: 0.06683125346899033\n",
      "Epoch: 462, batch loss: 0.021770283579826355, val loss: 0.06625639647245407\n",
      "Epoch: 463, batch loss: 0.017375880852341652, val loss: 0.06262026727199554\n",
      "Epoch: 464, batch loss: 0.015782969072461128, val loss: 0.062323492020368576\n",
      "Epoch: 465, batch loss: 0.01583617925643921, val loss: 0.06391012668609619\n",
      "Epoch: 466, batch loss: 0.01829923689365387, val loss: 0.06387129426002502\n",
      "Epoch: 467, batch loss: 0.015673000365495682, val loss: 0.06177268549799919\n",
      "Epoch: 468, batch loss: 0.015644555911421776, val loss: 0.061142053455114365\n",
      "Epoch: 469, batch loss: 0.015493860468268394, val loss: 0.06393718719482422\n",
      "Epoch: 470, batch loss: 0.018941089510917664, val loss: 0.06677599251270294\n",
      "Epoch: 471, batch loss: 0.020657895132899284, val loss: 0.06322427839040756\n",
      "Epoch: 472, batch loss: 0.014571825973689556, val loss: 0.061480019241571426\n",
      "Epoch: 473, batch loss: 0.016992630437016487, val loss: 0.06435123831033707\n",
      "Epoch: 474, batch loss: 0.018366344273090363, val loss: 0.06404978781938553\n",
      "Epoch: 475, batch loss: 0.01895006000995636, val loss: 0.06400789320468903\n",
      "Epoch: 476, batch loss: 0.01606842875480652, val loss: 0.061478517949581146\n",
      "Epoch: 477, batch loss: 0.015936970710754395, val loss: 0.06423428654670715\n",
      "Epoch: 478, batch loss: 0.018125398084521294, val loss: 0.06633532792329788\n",
      "Epoch: 479, batch loss: 0.021132444962859154, val loss: 0.06689436733722687\n",
      "Epoch: 480, batch loss: 0.02076425589621067, val loss: 0.06669563055038452\n",
      "Epoch: 481, batch loss: 0.014636354520916939, val loss: 0.06326921284198761\n",
      "Epoch: 482, batch loss: 0.02108757570385933, val loss: 0.06535597145557404\n",
      "Epoch: 483, batch loss: 0.018092434853315353, val loss: 0.06140721216797829\n",
      "Epoch: 484, batch loss: 0.016411712393164635, val loss: 0.06052972748875618\n",
      "Epoch: 485, batch loss: 0.01843625120818615, val loss: 0.06219155713915825\n",
      "Epoch: 486, batch loss: 0.015170113183557987, val loss: 0.059307560324668884\n",
      "Epoch: 487, batch loss: 0.015617994591593742, val loss: 0.0618865005671978\n",
      "Epoch: 488, batch loss: 0.016038643196225166, val loss: 0.0611717514693737\n",
      "Epoch: 489, batch loss: 0.01809757947921753, val loss: 0.06122314929962158\n",
      "Epoch: 490, batch loss: 0.01701076328754425, val loss: 0.06029454991221428\n",
      "Epoch: 491, batch loss: 0.01803242228925228, val loss: 0.06154624745249748\n",
      "Epoch: 492, batch loss: 0.017867986112833023, val loss: 0.06004544720053673\n",
      "Epoch: 493, batch loss: 0.01569399982690811, val loss: 0.05861571803689003\n",
      "Epoch: 494, batch loss: 0.01593567617237568, val loss: 0.06099335104227066\n",
      "Epoch: 495, batch loss: 0.016936320811510086, val loss: 0.06450770050287247\n",
      "Epoch: 496, batch loss: 0.0169683825224638, val loss: 0.06514506787061691\n",
      "Epoch: 497, batch loss: 0.01952509582042694, val loss: 0.06600268185138702\n",
      "Epoch: 498, batch loss: 0.017729444429278374, val loss: 0.06722630560398102\n",
      "Epoch: 499, batch loss: 0.01725700870156288, val loss: 0.06583651155233383\n",
      "Epoch: 500, batch loss: 0.02325730212032795, val loss: 0.06549538671970367\n",
      "Epoch: 501, batch loss: 0.019603567197918892, val loss: 0.06293916702270508\n",
      "Epoch: 502, batch loss: 0.014486927539110184, val loss: 0.06137548014521599\n",
      "Epoch: 503, batch loss: 0.018444938585162163, val loss: 0.06412798166275024\n",
      "Epoch: 504, batch loss: 0.014491035602986813, val loss: 0.061929602175951004\n",
      "Epoch: 505, batch loss: 0.01691552810370922, val loss: 0.06280165165662766\n",
      "Epoch: 506, batch loss: 0.0154686588793993, val loss: 0.06007585674524307\n",
      "Epoch: 507, batch loss: 0.01550563145428896, val loss: 0.060225196182727814\n",
      "Epoch: 508, batch loss: 0.01674969308078289, val loss: 0.06137349084019661\n",
      "Epoch: 509, batch loss: 0.016332637518644333, val loss: 0.06397955119609833\n",
      "Epoch: 510, batch loss: 0.01837047189474106, val loss: 0.06248755007982254\n",
      "Epoch: 511, batch loss: 0.01454553660005331, val loss: 0.06159588694572449\n",
      "Epoch: 512, batch loss: 0.01790071278810501, val loss: 0.061683639883995056\n",
      "Epoch: 513, batch loss: 0.017216872423887253, val loss: 0.05704746022820473\n",
      "Epoch: 514, batch loss: 0.022447435185313225, val loss: 0.058514099568128586\n",
      "Epoch: 515, batch loss: 0.01860486902296543, val loss: 0.0581258200109005\n",
      "Epoch: 516, batch loss: 0.014162837527692318, val loss: 0.06058768182992935\n",
      "Epoch: 517, batch loss: 0.01774151436984539, val loss: 0.06463610380887985\n",
      "Epoch: 518, batch loss: 0.01777459681034088, val loss: 0.062040042132139206\n",
      "Epoch: 519, batch loss: 0.01939840242266655, val loss: 0.06458085775375366\n",
      "Epoch: 520, batch loss: 0.020075485110282898, val loss: 0.06538432091474533\n",
      "Epoch: 521, batch loss: 0.0175709780305624, val loss: 0.06740999221801758\n",
      "Epoch: 522, batch loss: 0.01798650249838829, val loss: 0.06491369009017944\n",
      "Epoch: 523, batch loss: 0.01595977507531643, val loss: 0.06130654737353325\n",
      "Epoch: 524, batch loss: 0.020067844539880753, val loss: 0.062265899032354355\n",
      "Epoch: 525, batch loss: 0.016980333253741264, val loss: 0.0625501424074173\n",
      "Epoch: 526, batch loss: 0.019431564956903458, val loss: 0.06721237301826477\n",
      "Epoch: 527, batch loss: 0.023270223289728165, val loss: 0.06811640411615372\n",
      "Epoch: 528, batch loss: 0.027545254677534103, val loss: 0.06296251714229584\n",
      "Epoch: 529, batch loss: 0.017196299508213997, val loss: 0.0620095431804657\n",
      "Epoch: 530, batch loss: 0.01733746938407421, val loss: 0.06231944262981415\n",
      "Epoch: 531, batch loss: 0.015273384749889374, val loss: 0.06281837821006775\n",
      "Epoch: 532, batch loss: 0.023041486740112305, val loss: 0.06444872915744781\n",
      "Epoch: 533, batch loss: 0.021780401468276978, val loss: 0.06496302783489227\n",
      "Epoch: 534, batch loss: 0.023166244849562645, val loss: 0.06549515575170517\n",
      "Epoch: 535, batch loss: 0.019201774150133133, val loss: 0.06396844238042831\n",
      "Epoch: 536, batch loss: 0.019118616357445717, val loss: 0.06357652693986893\n",
      "Epoch: 537, batch loss: 0.018125899136066437, val loss: 0.06121475622057915\n",
      "Epoch: 538, batch loss: 0.016627682372927666, val loss: 0.0591442734003067\n",
      "Epoch: 539, batch loss: 0.0153867918998003, val loss: 0.05657578632235527\n",
      "Epoch: 540, batch loss: 0.015329225920140743, val loss: 0.05442357063293457\n",
      "Epoch: 541, batch loss: 0.016460223123431206, val loss: 0.054080650210380554\n",
      "Epoch: 542, batch loss: 0.01451264601200819, val loss: 0.052676014602184296\n",
      "Epoch: 543, batch loss: 0.01242146547883749, val loss: 0.05116179585456848\n",
      "Epoch: 544, batch loss: 0.01349451020359993, val loss: 0.0504668764770031\n",
      "Epoch: 545, batch loss: 0.01279442012310028, val loss: 0.04910670965909958\n",
      "Epoch: 546, batch loss: 0.014300932176411152, val loss: 0.04969673231244087\n",
      "Epoch: 547, batch loss: 0.011987575329840183, val loss: 0.049306683242321014\n",
      "Epoch: 548, batch loss: 0.013187386095523834, val loss: 0.050805673003196716\n",
      "Epoch: 549, batch loss: 0.014353377744555473, val loss: 0.04967141151428223\n",
      "Epoch: 550, batch loss: 0.014567519538104534, val loss: 0.04844159260392189\n",
      "Epoch: 551, batch loss: 0.01218208484351635, val loss: 0.04832194000482559\n",
      "Epoch: 552, batch loss: 0.012423869222402573, val loss: 0.048865582793951035\n",
      "Epoch: 553, batch loss: 0.013400683179497719, val loss: 0.048779916018247604\n",
      "Epoch: 554, batch loss: 0.015007427893579006, val loss: 0.04905996099114418\n",
      "Epoch: 555, batch loss: 0.014258347451686859, val loss: 0.04896247014403343\n",
      "Epoch: 556, batch loss: 0.012704383581876755, val loss: 0.04974832013249397\n",
      "Epoch: 557, batch loss: 0.015233488753437996, val loss: 0.05052696540951729\n",
      "Epoch: 558, batch loss: 0.0173801276832819, val loss: 0.05018358305096626\n",
      "Epoch: 559, batch loss: 0.016363438218832016, val loss: 0.0512276291847229\n",
      "Epoch: 560, batch loss: 0.013443035073578358, val loss: 0.05125279352068901\n",
      "Epoch: 561, batch loss: 0.015010814182460308, val loss: 0.052229732275009155\n",
      "Epoch: 562, batch loss: 0.015191133134067059, val loss: 0.05264773592352867\n",
      "Epoch: 563, batch loss: 0.020823083817958832, val loss: 0.05437925085425377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 564, batch loss: 0.01611226052045822, val loss: 0.05332384258508682\n",
      "Epoch: 565, batch loss: 0.01653781160712242, val loss: 0.056835658848285675\n",
      "Epoch: 566, batch loss: 0.015585048124194145, val loss: 0.05454682558774948\n",
      "Epoch: 567, batch loss: 0.017924586310982704, val loss: 0.05415690317749977\n",
      "Epoch: 568, batch loss: 0.017064807936549187, val loss: 0.05691493675112724\n",
      "Epoch: 569, batch loss: 0.01612292416393757, val loss: 0.060459837317466736\n",
      "Epoch: 570, batch loss: 0.02219797484576702, val loss: 0.05967460200190544\n",
      "Epoch: 571, batch loss: 0.018763430416584015, val loss: 0.057196471840143204\n",
      "Epoch: 572, batch loss: 0.01766269840300083, val loss: 0.060201723128557205\n",
      "Epoch: 573, batch loss: 0.01703929901123047, val loss: 0.05974898487329483\n",
      "Epoch: 574, batch loss: 0.022642986848950386, val loss: 0.061126574873924255\n",
      "Epoch: 575, batch loss: 0.025153392925858498, val loss: 0.05680697038769722\n",
      "Epoch: 576, batch loss: 0.023395376279950142, val loss: 0.056302640587091446\n",
      "Epoch: 577, batch loss: 0.017411915585398674, val loss: 0.055241383612155914\n",
      "Epoch: 578, batch loss: 0.015980781987309456, val loss: 0.054621051996946335\n",
      "Epoch: 579, batch loss: 0.01787005551159382, val loss: 0.05330115184187889\n",
      "Epoch: 580, batch loss: 0.015321457758545876, val loss: 0.050216492265462875\n",
      "Epoch: 581, batch loss: 0.015099158510565758, val loss: 0.05081671476364136\n",
      "Epoch: 582, batch loss: 0.012766551226377487, val loss: 0.050690099596977234\n",
      "Epoch: 583, batch loss: 0.014500021003186703, val loss: 0.05117145925760269\n",
      "Epoch: 584, batch loss: 0.014923347160220146, val loss: 0.04978185519576073\n",
      "Epoch: 585, batch loss: 0.015081502497196198, val loss: 0.0516800619661808\n",
      "Epoch: 586, batch loss: 0.014179465360939503, val loss: 0.05199788138270378\n",
      "Epoch: 587, batch loss: 0.01642397604882717, val loss: 0.05132497102022171\n",
      "Epoch: 588, batch loss: 0.01567891240119934, val loss: 0.05113271623849869\n",
      "Epoch: 589, batch loss: 0.017008380964398384, val loss: 0.05083432421088219\n",
      "Epoch: 590, batch loss: 0.012004926800727844, val loss: 0.050252821296453476\n",
      "Epoch: 591, batch loss: 0.016704479232430458, val loss: 0.04984525591135025\n",
      "Epoch: 592, batch loss: 0.016330966725945473, val loss: 0.04876029118895531\n",
      "Epoch: 593, batch loss: 0.01691131293773651, val loss: 0.04926779866218567\n",
      "Epoch: 594, batch loss: 0.013907959684729576, val loss: 0.04771608114242554\n",
      "Epoch: 595, batch loss: 0.015601539053022861, val loss: 0.04676409810781479\n",
      "Epoch: 596, batch loss: 0.01466845627874136, val loss: 0.04797372967004776\n",
      "Epoch: 597, batch loss: 0.014322077855467796, val loss: 0.0487927608191967\n",
      "Epoch: 598, batch loss: 0.01334844995290041, val loss: 0.0473506785929203\n",
      "Epoch: 599, batch loss: 0.013938901014626026, val loss: 0.047039229422807693\n",
      "Epoch: 600, batch loss: 0.01584554649889469, val loss: 0.04865652322769165\n",
      "Epoch: 601, batch loss: 0.01334066316485405, val loss: 0.04862886667251587\n",
      "Epoch: 602, batch loss: 0.015984583646059036, val loss: 0.049462996423244476\n",
      "Epoch: 603, batch loss: 0.015833009034395218, val loss: 0.04939320310950279\n",
      "Epoch: 604, batch loss: 0.015739284455776215, val loss: 0.05036068707704544\n",
      "Epoch: 605, batch loss: 0.014419229701161385, val loss: 0.05160776525735855\n",
      "Epoch: 606, batch loss: 0.014714463613927364, val loss: 0.05149735510349274\n",
      "Epoch: 607, batch loss: 0.015836484730243683, val loss: 0.052418798208236694\n",
      "Epoch: 608, batch loss: 0.01783149503171444, val loss: 0.05121603608131409\n",
      "Epoch: 609, batch loss: 0.015346282161772251, val loss: 0.05412304028868675\n",
      "Epoch: 610, batch loss: 0.01615099608898163, val loss: 0.05654576048254967\n",
      "Epoch: 611, batch loss: 0.01639297790825367, val loss: 0.059449538588523865\n",
      "Epoch: 612, batch loss: 0.019316304475069046, val loss: 0.06186194345355034\n",
      "Epoch: 613, batch loss: 0.016787396743893623, val loss: 0.07060027867555618\n",
      "Epoch: 614, batch loss: 0.0180166307836771, val loss: 0.07040567696094513\n",
      "Epoch: 615, batch loss: 0.018267346546053886, val loss: 0.06717782467603683\n",
      "Epoch: 616, batch loss: 0.026449821889400482, val loss: 0.06633000075817108\n",
      "Epoch: 617, batch loss: 0.019039202481508255, val loss: 0.06382234394550323\n",
      "Epoch: 618, batch loss: 0.017156068235635757, val loss: 0.05915125831961632\n",
      "Epoch: 619, batch loss: 0.01712992414832115, val loss: 0.05383254215121269\n",
      "Epoch: 620, batch loss: 0.016397792845964432, val loss: 0.051526982337236404\n",
      "Epoch: 621, batch loss: 0.014373531565070152, val loss: 0.04915161430835724\n",
      "Epoch: 622, batch loss: 0.015615427866578102, val loss: 0.050831928849220276\n",
      "Epoch: 623, batch loss: 0.017617788165807724, val loss: 0.05240829288959503\n",
      "Epoch: 624, batch loss: 0.014525197446346283, val loss: 0.054507844150066376\n",
      "Epoch: 625, batch loss: 0.015080523677170277, val loss: 0.05329751595854759\n",
      "Epoch: 626, batch loss: 0.017430951818823814, val loss: 0.05114245042204857\n",
      "Epoch: 627, batch loss: 0.02405853196978569, val loss: 0.05118759721517563\n",
      "Epoch: 628, batch loss: 0.01729753240942955, val loss: 0.05113035812973976\n",
      "Epoch: 629, batch loss: 0.014179695397615433, val loss: 0.051388487219810486\n",
      "Epoch: 630, batch loss: 0.018277572467923164, val loss: 0.052201878279447556\n",
      "Epoch: 631, batch loss: 0.019573044031858444, val loss: 0.05320744961500168\n",
      "Epoch: 632, batch loss: 0.01873045600950718, val loss: 0.05340788885951042\n",
      "Epoch: 633, batch loss: 0.016082467511296272, val loss: 0.05360276624560356\n",
      "Epoch: 634, batch loss: 0.02027597278356552, val loss: 0.0528445839881897\n",
      "Epoch: 635, batch loss: 0.022168241441249847, val loss: 0.052591320127248764\n",
      "Epoch: 636, batch loss: 0.020955968648195267, val loss: 0.0554487444460392\n",
      "Epoch: 637, batch loss: 0.019304869696497917, val loss: 0.05372804403305054\n",
      "Epoch: 638, batch loss: 0.017657361924648285, val loss: 0.056404102593660355\n",
      "Epoch: 639, batch loss: 0.02195972576737404, val loss: 0.058405809104442596\n",
      "Epoch: 640, batch loss: 0.020155901089310646, val loss: 0.05507063865661621\n",
      "Epoch: 641, batch loss: 0.017621058970689774, val loss: 0.05537945032119751\n",
      "Epoch: 642, batch loss: 0.017206208780407906, val loss: 0.05102422460913658\n",
      "Epoch: 643, batch loss: 0.018982132896780968, val loss: 0.0523131899535656\n",
      "Epoch: 644, batch loss: 0.016805578023195267, val loss: 0.05304941534996033\n",
      "Epoch: 645, batch loss: 0.01665171980857849, val loss: 0.051212020218372345\n",
      "Epoch: 646, batch loss: 0.016787635162472725, val loss: 0.0509125180542469\n",
      "Epoch: 647, batch loss: 0.017451513558626175, val loss: 0.05257991701364517\n",
      "Epoch: 648, batch loss: 0.020373746752738953, val loss: 0.0503680594265461\n",
      "Epoch: 649, batch loss: 0.014271230436861515, val loss: 0.05100151523947716\n",
      "Epoch: 650, batch loss: 0.016685549169778824, val loss: 0.05179200693964958\n",
      "Epoch: 651, batch loss: 0.016034740954637527, val loss: 0.05043159797787666\n",
      "Epoch: 652, batch loss: 0.020022204145789146, val loss: 0.051347363740205765\n",
      "Epoch: 653, batch loss: 0.015051212161779404, val loss: 0.04808010905981064\n",
      "Epoch: 654, batch loss: 0.015209781937301159, val loss: 0.04898744449019432\n",
      "Epoch: 655, batch loss: 0.013209464959800243, val loss: 0.048414237797260284\n",
      "Epoch: 656, batch loss: 0.016936227679252625, val loss: 0.05035843700170517\n",
      "Epoch: 657, batch loss: 0.01924293301999569, val loss: 0.04987690597772598\n",
      "Epoch: 658, batch loss: 0.014778095297515392, val loss: 0.048798397183418274\n",
      "Epoch: 659, batch loss: 0.015288904309272766, val loss: 0.04864262416958809\n",
      "Epoch: 660, batch loss: 0.014791952446103096, val loss: 0.0477849543094635\n",
      "Epoch: 661, batch loss: 0.017305197194218636, val loss: 0.04765249788761139\n",
      "Epoch: 662, batch loss: 0.013647409155964851, val loss: 0.044849563390016556\n",
      "Epoch: 663, batch loss: 0.013423530384898186, val loss: 0.045102719217538834\n",
      "Epoch: 664, batch loss: 0.01358992699533701, val loss: 0.04533954709768295\n",
      "Epoch: 665, batch loss: 0.01667739823460579, val loss: 0.04674600064754486\n",
      "Epoch: 666, batch loss: 0.01542049553245306, val loss: 0.04524358734488487\n",
      "Epoch: 667, batch loss: 0.012694766744971275, val loss: 0.04688913747668266\n",
      "Epoch: 668, batch loss: 0.01576930284500122, val loss: 0.04821551963686943\n",
      "Epoch: 669, batch loss: 0.017248108983039856, val loss: 0.046369172632694244\n",
      "Epoch: 670, batch loss: 0.017389077693223953, val loss: 0.044880710542201996\n",
      "Epoch: 671, batch loss: 0.013027844950556755, val loss: 0.045192692428827286\n",
      "Epoch: 672, batch loss: 0.01542444434016943, val loss: 0.04755333438515663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 673, batch loss: 0.015292756259441376, val loss: 0.04921158403158188\n",
      "Epoch: 674, batch loss: 0.024441203102469444, val loss: 0.04958178475499153\n",
      "Epoch: 675, batch loss: 0.018913907930254936, val loss: 0.04854346811771393\n",
      "Epoch: 676, batch loss: 0.028848152607679367, val loss: 0.05019121617078781\n",
      "Epoch: 677, batch loss: 0.015440833754837513, val loss: 0.04701235890388489\n",
      "Epoch: 678, batch loss: 0.018885470926761627, val loss: 0.04623383656144142\n",
      "Epoch: 679, batch loss: 0.013291221112012863, val loss: 0.04454343765974045\n",
      "Epoch: 680, batch loss: 0.014894652180373669, val loss: 0.0445123128592968\n",
      "Epoch: 681, batch loss: 0.012692160904407501, val loss: 0.043458469212055206\n",
      "Epoch: 682, batch loss: 0.01699916273355484, val loss: 0.044243473559617996\n",
      "Epoch: 683, batch loss: 0.013980250805616379, val loss: 0.04338051378726959\n",
      "Epoch: 684, batch loss: 0.01449243351817131, val loss: 0.04485935717821121\n",
      "Epoch: 685, batch loss: 0.014522185549139977, val loss: 0.04360548034310341\n",
      "Epoch: 686, batch loss: 0.014480175450444221, val loss: 0.04278824105858803\n",
      "Epoch: 687, batch loss: 0.01306533720344305, val loss: 0.04256156459450722\n",
      "Epoch: 688, batch loss: 0.01229728851467371, val loss: 0.04265604540705681\n",
      "Epoch: 689, batch loss: 0.011717915534973145, val loss: 0.04093792289495468\n",
      "Epoch: 690, batch loss: 0.01385149173438549, val loss: 0.042222362011671066\n",
      "Epoch: 691, batch loss: 0.0133885582908988, val loss: 0.04215218871831894\n",
      "Epoch: 692, batch loss: 0.012020890600979328, val loss: 0.0426570326089859\n",
      "Epoch: 693, batch loss: 0.011202197521924973, val loss: 0.041560351848602295\n",
      "Epoch: 694, batch loss: 0.014886760152876377, val loss: 0.04184538871049881\n",
      "Epoch: 695, batch loss: 0.01369528565555811, val loss: 0.04208057373762131\n",
      "Epoch: 696, batch loss: 0.012195399031043053, val loss: 0.043933115899562836\n",
      "Epoch: 697, batch loss: 0.011510344222187996, val loss: 0.04381798207759857\n",
      "Epoch: 698, batch loss: 0.01406821608543396, val loss: 0.0453002043068409\n",
      "Epoch: 699, batch loss: 0.01296855416148901, val loss: 0.04567983001470566\n",
      "Epoch: 700, batch loss: 0.018171632662415504, val loss: 0.04607095941901207\n",
      "Epoch: 701, batch loss: 0.017862718552350998, val loss: 0.04568495228886604\n",
      "Epoch: 702, batch loss: 0.020895669236779213, val loss: 0.0451277419924736\n",
      "Epoch: 703, batch loss: 0.012055061757564545, val loss: 0.04526502266526222\n",
      "Epoch: 704, batch loss: 0.014189750887453556, val loss: 0.04505281522870064\n",
      "Epoch: 705, batch loss: 0.012792392633855343, val loss: 0.04211147874593735\n",
      "Epoch: 706, batch loss: 0.013887656852602959, val loss: 0.0425482802093029\n",
      "Epoch: 707, batch loss: 0.012586417607963085, val loss: 0.04256467893719673\n",
      "Epoch: 708, batch loss: 0.014071849174797535, val loss: 0.042687032371759415\n",
      "Epoch: 709, batch loss: 0.013447889126837254, val loss: 0.04156400263309479\n",
      "Epoch: 710, batch loss: 0.016112206503748894, val loss: 0.04213884472846985\n",
      "Epoch: 711, batch loss: 0.014271747320890427, val loss: 0.042124129831790924\n",
      "Epoch: 712, batch loss: 0.012491129338741302, val loss: 0.041666410863399506\n",
      "Epoch: 713, batch loss: 0.011966031044721603, val loss: 0.039789825677871704\n",
      "Epoch: 714, batch loss: 0.01409985776990652, val loss: 0.04006620869040489\n",
      "Epoch: 715, batch loss: 0.011968199163675308, val loss: 0.040552448481321335\n",
      "Epoch: 716, batch loss: 0.01232760027050972, val loss: 0.04134833440184593\n",
      "Epoch: 717, batch loss: 0.014071641489863396, val loss: 0.04172325134277344\n",
      "Epoch: 718, batch loss: 0.01540809404104948, val loss: 0.04184335097670555\n",
      "Epoch: 719, batch loss: 0.014023557305335999, val loss: 0.04269146919250488\n",
      "Epoch: 720, batch loss: 0.014104419387876987, val loss: 0.043548714369535446\n",
      "Epoch: 721, batch loss: 0.015170377679169178, val loss: 0.04442168027162552\n",
      "Epoch: 722, batch loss: 0.01717359386384487, val loss: 0.046979546546936035\n",
      "Epoch: 723, batch loss: 0.015074288472533226, val loss: 0.04858170077204704\n",
      "Epoch: 724, batch loss: 0.017298048362135887, val loss: 0.049431681632995605\n",
      "Epoch: 725, batch loss: 0.02024815045297146, val loss: 0.04774952679872513\n",
      "Epoch: 726, batch loss: 0.021534230560064316, val loss: 0.04698808491230011\n",
      "Epoch: 727, batch loss: 0.015520259737968445, val loss: 0.046867482364177704\n",
      "Epoch: 728, batch loss: 0.015576615929603577, val loss: 0.045561533421278\n",
      "Epoch: 729, batch loss: 0.017696499824523926, val loss: 0.04553275555372238\n",
      "Epoch: 730, batch loss: 0.017383798956871033, val loss: 0.04493654519319534\n",
      "Epoch: 731, batch loss: 0.014000894501805305, val loss: 0.047698285430669785\n",
      "Epoch: 732, batch loss: 0.01853538677096367, val loss: 0.048980411142110825\n",
      "Epoch: 733, batch loss: 0.02482501044869423, val loss: 0.049982525408267975\n",
      "Epoch: 734, batch loss: 0.02426465041935444, val loss: 0.05152597650885582\n",
      "Epoch: 735, batch loss: 0.019680187106132507, val loss: 0.05060386285185814\n",
      "Epoch: 736, batch loss: 0.026593178510665894, val loss: 0.052911426872015\n",
      "Epoch: 737, batch loss: 0.024738352745771408, val loss: 0.0485449880361557\n",
      "Epoch: 738, batch loss: 0.02230728790163994, val loss: 0.050262950360774994\n",
      "Epoch: 739, batch loss: 0.01620742492377758, val loss: 0.04944424331188202\n",
      "Epoch: 740, batch loss: 0.019539032131433487, val loss: 0.051134899258613586\n",
      "Epoch: 741, batch loss: 0.02455320581793785, val loss: 0.049139201641082764\n",
      "Epoch: 742, batch loss: 0.017243754118680954, val loss: 0.0487954244017601\n",
      "Epoch: 743, batch loss: 0.018879281356930733, val loss: 0.04932147264480591\n",
      "Epoch: 744, batch loss: 0.014446650631725788, val loss: 0.047041840851306915\n",
      "Epoch: 745, batch loss: 0.021117672324180603, val loss: 0.050524137914180756\n",
      "Epoch: 746, batch loss: 0.018210969865322113, val loss: 0.04615173116326332\n",
      "Epoch: 747, batch loss: 0.01744072698056698, val loss: 0.04657138139009476\n",
      "Epoch: 748, batch loss: 0.0166971106082201, val loss: 0.04333941265940666\n",
      "Epoch: 749, batch loss: 0.01802186109125614, val loss: 0.043247465044260025\n",
      "Epoch: 750, batch loss: 0.014053716324269772, val loss: 0.04234854504466057\n",
      "Epoch: 751, batch loss: 0.01653529703617096, val loss: 0.0427599772810936\n",
      "Epoch: 752, batch loss: 0.014767810702323914, val loss: 0.042127203196287155\n",
      "Epoch: 753, batch loss: 0.016455164179205894, val loss: 0.04369227960705757\n",
      "Epoch: 754, batch loss: 0.015412280336022377, val loss: 0.041650671511888504\n",
      "Epoch: 755, batch loss: 0.014238130301237106, val loss: 0.04379591718316078\n",
      "Epoch: 756, batch loss: 0.018083345144987106, val loss: 0.043679382652044296\n",
      "Epoch: 757, batch loss: 0.013585283420979977, val loss: 0.04340990632772446\n",
      "Epoch: 758, batch loss: 0.018620260059833527, val loss: 0.04132828488945961\n",
      "Epoch: 759, batch loss: 0.013663304969668388, val loss: 0.041936781257390976\n",
      "Epoch: 760, batch loss: 0.01678437739610672, val loss: 0.04185190424323082\n",
      "Epoch: 761, batch loss: 0.011054336093366146, val loss: 0.039096150547266006\n",
      "Epoch: 762, batch loss: 0.015118190087378025, val loss: 0.03988543525338173\n",
      "Epoch: 763, batch loss: 0.011178440414369106, val loss: 0.03872589021921158\n",
      "Epoch: 764, batch loss: 0.01621011272072792, val loss: 0.04088273271918297\n",
      "Epoch: 765, batch loss: 0.012284161522984505, val loss: 0.03810785338282585\n",
      "Epoch: 766, batch loss: 0.014156973920762539, val loss: 0.03886803984642029\n",
      "Epoch: 767, batch loss: 0.011681418865919113, val loss: 0.03847210481762886\n",
      "Epoch: 768, batch loss: 0.013551650568842888, val loss: 0.03954113647341728\n",
      "Epoch: 769, batch loss: 0.012740441597998142, val loss: 0.03756996616721153\n",
      "Epoch: 770, batch loss: 0.013054697774350643, val loss: 0.03831996023654938\n",
      "Epoch: 771, batch loss: 0.013282844796776772, val loss: 0.03881610929965973\n",
      "Epoch: 772, batch loss: 0.013292786665260792, val loss: 0.03920353949069977\n",
      "Epoch: 773, batch loss: 0.012297573499381542, val loss: 0.039078887552022934\n",
      "Epoch: 774, batch loss: 0.012289865873754025, val loss: 0.03868739306926727\n",
      "Epoch: 775, batch loss: 0.013243586756289005, val loss: 0.03939623385667801\n",
      "Epoch: 776, batch loss: 0.01219940185546875, val loss: 0.0395088866353035\n",
      "Epoch: 777, batch loss: 0.011740723624825478, val loss: 0.03858225792646408\n",
      "Epoch: 778, batch loss: 0.011679301969707012, val loss: 0.03941335901618004\n",
      "Epoch: 779, batch loss: 0.011355183087289333, val loss: 0.04074728861451149\n",
      "Epoch: 780, batch loss: 0.014418164268136024, val loss: 0.042452435940504074\n",
      "Epoch: 781, batch loss: 0.01299203373491764, val loss: 0.042819082736968994\n",
      "Epoch: 782, batch loss: 0.015367615036666393, val loss: 0.042695436626672745\n",
      "Epoch: 783, batch loss: 0.012958980165421963, val loss: 0.042464423924684525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 784, batch loss: 0.013715453445911407, val loss: 0.04378189146518707\n",
      "Epoch: 785, batch loss: 0.012762699276208878, val loss: 0.042725883424282074\n",
      "Epoch: 786, batch loss: 0.016053758561611176, val loss: 0.0421537309885025\n",
      "Epoch: 787, batch loss: 0.013492272235453129, val loss: 0.04232446849346161\n",
      "Epoch: 788, batch loss: 0.013701386749744415, val loss: 0.04285944998264313\n",
      "Epoch: 789, batch loss: 0.012052183039486408, val loss: 0.04141533747315407\n",
      "Epoch: 790, batch loss: 0.014357276260852814, val loss: 0.04059750959277153\n",
      "Epoch: 791, batch loss: 0.012620002031326294, val loss: 0.039660610258579254\n",
      "Epoch: 792, batch loss: 0.013122293166816235, val loss: 0.03944021090865135\n",
      "Epoch: 793, batch loss: 0.013551592826843262, val loss: 0.03832681477069855\n",
      "Epoch: 794, batch loss: 0.01279059611260891, val loss: 0.03834763169288635\n",
      "Epoch: 795, batch loss: 0.010644965805113316, val loss: 0.03781331703066826\n",
      "Epoch: 796, batch loss: 0.012914972379803658, val loss: 0.037465017288923264\n",
      "Epoch: 797, batch loss: 0.012606201693415642, val loss: 0.03657267615199089\n",
      "Epoch: 798, batch loss: 0.011645817197859287, val loss: 0.03789040446281433\n",
      "Epoch: 799, batch loss: 0.011002719402313232, val loss: 0.03718386963009834\n",
      "Epoch: 800, batch loss: 0.013779900968074799, val loss: 0.03708340600132942\n",
      "Epoch: 801, batch loss: 0.012774946168065071, val loss: 0.037131551653146744\n",
      "Epoch: 802, batch loss: 0.013829542323946953, val loss: 0.03783734515309334\n",
      "Epoch: 803, batch loss: 0.012955264188349247, val loss: 0.038454506546258926\n",
      "Epoch: 804, batch loss: 0.01653997227549553, val loss: 0.037274159491062164\n",
      "Epoch: 805, batch loss: 0.012169688940048218, val loss: 0.038252148777246475\n",
      "Epoch: 806, batch loss: 0.0141083849593997, val loss: 0.03896966949105263\n",
      "Epoch: 807, batch loss: 0.014495301060378551, val loss: 0.039374519139528275\n",
      "Epoch: 808, batch loss: 0.017442382872104645, val loss: 0.03860243409872055\n",
      "Epoch: 809, batch loss: 0.014782516285777092, val loss: 0.03985856473445892\n",
      "Epoch: 810, batch loss: 0.01370408944785595, val loss: 0.04002409428358078\n",
      "Epoch: 811, batch loss: 0.01782749965786934, val loss: 0.04158053547143936\n",
      "Epoch: 812, batch loss: 0.019171370193362236, val loss: 0.04000573232769966\n",
      "Epoch: 813, batch loss: 0.014771634712815285, val loss: 0.042111724615097046\n",
      "Epoch: 814, batch loss: 0.014019067399203777, val loss: 0.04296841472387314\n",
      "Epoch: 815, batch loss: 0.022362161427736282, val loss: 0.04381047189235687\n",
      "Epoch: 816, batch loss: 0.019200513139367104, val loss: 0.04399094358086586\n",
      "Epoch: 817, batch loss: 0.0181465782225132, val loss: 0.04173440486192703\n",
      "Epoch: 818, batch loss: 0.015321804210543633, val loss: 0.04273628443479538\n",
      "Epoch: 819, batch loss: 0.018595049157738686, val loss: 0.040899720042943954\n",
      "Epoch: 820, batch loss: 0.01608811877667904, val loss: 0.04230436310172081\n",
      "Epoch: 821, batch loss: 0.013098537921905518, val loss: 0.04251961037516594\n",
      "Epoch: 822, batch loss: 0.01702078990638256, val loss: 0.04344206675887108\n",
      "Epoch: 823, batch loss: 0.01716422475874424, val loss: 0.04256938025355339\n",
      "Epoch: 824, batch loss: 0.01363357063382864, val loss: 0.04570543020963669\n",
      "Epoch: 825, batch loss: 0.0131366653367877, val loss: 0.047554366290569305\n",
      "Epoch: 826, batch loss: 0.01896103471517563, val loss: 0.04766593128442764\n",
      "Epoch: 827, batch loss: 0.015507626347243786, val loss: 0.046442702412605286\n",
      "Epoch: 828, batch loss: 0.017810426652431488, val loss: 0.04786410182714462\n",
      "Epoch: 829, batch loss: 0.01393974106758833, val loss: 0.04608903452754021\n",
      "Epoch: 830, batch loss: 0.01973184198141098, val loss: 0.04543263465166092\n",
      "Epoch: 831, batch loss: 0.016541581600904465, val loss: 0.045194268226623535\n",
      "Epoch: 832, batch loss: 0.017132394015789032, val loss: 0.043396636843681335\n",
      "Epoch: 833, batch loss: 0.014070708304643631, val loss: 0.04261883720755577\n",
      "Epoch: 834, batch loss: 0.01849176175892353, val loss: 0.041356757283210754\n",
      "Epoch: 835, batch loss: 0.01230826135724783, val loss: 0.03947871923446655\n",
      "Epoch: 836, batch loss: 0.014905610121786594, val loss: 0.04016059264540672\n",
      "Epoch: 837, batch loss: 0.011941217817366123, val loss: 0.03854072093963623\n",
      "Epoch: 838, batch loss: 0.015310079790651798, val loss: 0.03854978829622269\n",
      "Epoch: 839, batch loss: 0.012723632156848907, val loss: 0.03673659265041351\n",
      "Epoch: 840, batch loss: 0.011524311266839504, val loss: 0.037282202392816544\n",
      "Epoch: 841, batch loss: 0.011487276293337345, val loss: 0.037338774651288986\n",
      "Epoch: 842, batch loss: 0.013554206117987633, val loss: 0.03851085901260376\n",
      "Epoch: 843, batch loss: 0.012757617980241776, val loss: 0.037769608199596405\n",
      "Epoch: 844, batch loss: 0.013627800159156322, val loss: 0.03829319402575493\n",
      "Epoch: 845, batch loss: 0.010506212711334229, val loss: 0.03650866076350212\n",
      "Epoch: 846, batch loss: 0.013849349692463875, val loss: 0.037872519344091415\n",
      "Epoch: 847, batch loss: 0.011933430097997189, val loss: 0.03656979650259018\n",
      "Epoch: 848, batch loss: 0.013800674118101597, val loss: 0.03835089132189751\n",
      "Epoch: 849, batch loss: 0.011528060771524906, val loss: 0.036652158945798874\n",
      "Epoch: 850, batch loss: 0.015461578033864498, val loss: 0.03761504963040352\n",
      "Epoch: 851, batch loss: 0.011881386861205101, val loss: 0.03670438006520271\n",
      "Epoch: 852, batch loss: 0.01637214981019497, val loss: 0.037866342812776566\n",
      "Epoch: 853, batch loss: 0.011461593210697174, val loss: 0.035467762500047684\n",
      "Epoch: 854, batch loss: 0.018570654094219208, val loss: 0.036607950925827026\n",
      "Epoch: 855, batch loss: 0.011707879602909088, val loss: 0.03667937219142914\n",
      "Epoch: 856, batch loss: 0.017373409122228622, val loss: 0.03812192752957344\n",
      "Epoch: 857, batch loss: 0.01272450853139162, val loss: 0.036051347851753235\n",
      "Epoch: 858, batch loss: 0.018045946955680847, val loss: 0.037378180772066116\n",
      "Epoch: 859, batch loss: 0.012176702730357647, val loss: 0.037170279771089554\n",
      "Epoch: 860, batch loss: 0.019361818209290504, val loss: 0.03755800053477287\n",
      "Epoch: 861, batch loss: 0.013973618857562542, val loss: 0.037459660321474075\n",
      "Epoch: 862, batch loss: 0.019790755584836006, val loss: 0.03824801743030548\n",
      "Epoch: 863, batch loss: 0.012780869379639626, val loss: 0.03815815597772598\n",
      "Epoch: 864, batch loss: 0.020104840397834778, val loss: 0.039644934237003326\n",
      "Epoch: 865, batch loss: 0.014160782098770142, val loss: 0.038259245455265045\n",
      "Epoch: 866, batch loss: 0.02045193873345852, val loss: 0.03971850872039795\n",
      "Epoch: 867, batch loss: 0.013966062106192112, val loss: 0.03777921199798584\n",
      "Epoch: 868, batch loss: 0.01930984854698181, val loss: 0.041473910212516785\n",
      "Epoch: 869, batch loss: 0.013213788159191608, val loss: 0.03997325524687767\n",
      "Epoch: 870, batch loss: 0.020822906866669655, val loss: 0.042849455028772354\n",
      "Epoch: 871, batch loss: 0.014023702591657639, val loss: 0.04079917073249817\n",
      "Epoch: 872, batch loss: 0.01718410663306713, val loss: 0.04315556585788727\n",
      "Epoch: 873, batch loss: 0.014019452035427094, val loss: 0.0404733270406723\n",
      "Epoch: 874, batch loss: 0.016015326604247093, val loss: 0.04061460867524147\n",
      "Epoch: 875, batch loss: 0.01694902405142784, val loss: 0.04021025449037552\n",
      "Epoch: 876, batch loss: 0.015205399133265018, val loss: 0.0406380221247673\n",
      "Epoch: 877, batch loss: 0.015837926417589188, val loss: 0.0401725098490715\n",
      "Epoch: 878, batch loss: 0.013814613223075867, val loss: 0.038839954882860184\n",
      "Epoch: 879, batch loss: 0.016720199957489967, val loss: 0.03876213729381561\n",
      "Epoch: 880, batch loss: 0.012296556495130062, val loss: 0.03833092004060745\n",
      "Epoch: 881, batch loss: 0.014481469988822937, val loss: 0.03824217990040779\n",
      "Epoch: 882, batch loss: 0.014536735601723194, val loss: 0.03759155049920082\n",
      "Epoch: 883, batch loss: 0.016894157975912094, val loss: 0.03822285309433937\n",
      "Epoch: 884, batch loss: 0.011521880514919758, val loss: 0.036709897220134735\n",
      "Epoch: 885, batch loss: 0.01113591343164444, val loss: 0.03796284273266792\n",
      "Epoch: 886, batch loss: 0.012389716692268848, val loss: 0.03849167004227638\n",
      "Epoch: 887, batch loss: 0.016820242628455162, val loss: 0.03988485410809517\n",
      "Epoch: 888, batch loss: 0.016821350902318954, val loss: 0.038223735988140106\n",
      "Epoch: 889, batch loss: 0.012145975604653358, val loss: 0.03745896741747856\n",
      "Epoch: 890, batch loss: 0.013710218481719494, val loss: 0.04078984633088112\n",
      "Epoch: 891, batch loss: 0.012660150416195393, val loss: 0.042514462023973465\n",
      "Epoch: 892, batch loss: 0.016624150797724724, val loss: 0.04220614954829216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893, batch loss: 0.013447637669742107, val loss: 0.04215581342577934\n",
      "Epoch: 894, batch loss: 0.01734970137476921, val loss: 0.04414539784193039\n",
      "Epoch: 895, batch loss: 0.014472388662397861, val loss: 0.041878774762153625\n",
      "Epoch: 896, batch loss: 0.015989523380994797, val loss: 0.04346844181418419\n",
      "Epoch: 897, batch loss: 0.016826801002025604, val loss: 0.04369572550058365\n",
      "Epoch: 898, batch loss: 0.014290599152445793, val loss: 0.04482750967144966\n",
      "Epoch: 899, batch loss: 0.012268603779375553, val loss: 0.045618705451488495\n",
      "Epoch: 900, batch loss: 0.016688259318470955, val loss: 0.04535169154405594\n",
      "Epoch: 901, batch loss: 0.02055279351770878, val loss: 0.044996511191129684\n",
      "Epoch: 902, batch loss: 0.018813226372003555, val loss: 0.04447184130549431\n",
      "Epoch: 903, batch loss: 0.015387882478535175, val loss: 0.042840518057346344\n",
      "Epoch: 904, batch loss: 0.013047504238784313, val loss: 0.04125746339559555\n",
      "Epoch: 905, batch loss: 0.013656701892614365, val loss: 0.04243861883878708\n",
      "Epoch: 906, batch loss: 0.014232750982046127, val loss: 0.04179298132658005\n",
      "Epoch: 907, batch loss: 0.011979998089373112, val loss: 0.04034925624728203\n",
      "Epoch: 908, batch loss: 0.013873008079826832, val loss: 0.03768892213702202\n",
      "Epoch: 909, batch loss: 0.011936192400753498, val loss: 0.03597624599933624\n",
      "Epoch: 910, batch loss: 0.012342297472059727, val loss: 0.037177834659814835\n",
      "Epoch: 911, batch loss: 0.010812250897288322, val loss: 0.0355246476829052\n",
      "Epoch: 912, batch loss: 0.010787038132548332, val loss: 0.03486979752779007\n",
      "Epoch: 913, batch loss: 0.011243016459047794, val loss: 0.03445764631032944\n",
      "Epoch: 914, batch loss: 0.010960210114717484, val loss: 0.033820535987615585\n",
      "Epoch: 915, batch loss: 0.009260446764528751, val loss: 0.033020105212926865\n",
      "Epoch: 916, batch loss: 0.010227118618786335, val loss: 0.03266657143831253\n",
      "Epoch: 917, batch loss: 0.010240265168249607, val loss: 0.032823558896780014\n",
      "Epoch: 918, batch loss: 0.011681986972689629, val loss: 0.03252771869301796\n",
      "Epoch: 919, batch loss: 0.010478549636900425, val loss: 0.032126884907484055\n",
      "Epoch: 920, batch loss: 0.009828626178205013, val loss: 0.03192392736673355\n",
      "Epoch: 921, batch loss: 0.009178128093481064, val loss: 0.03146906942129135\n",
      "Epoch: 922, batch loss: 0.01017308235168457, val loss: 0.031032873317599297\n",
      "Epoch: 923, batch loss: 0.009121420793235302, val loss: 0.03052690625190735\n",
      "Epoch: 924, batch loss: 0.009687609039247036, val loss: 0.030201543122529984\n",
      "Epoch: 925, batch loss: 0.009967755526304245, val loss: 0.030353274196386337\n",
      "Epoch: 926, batch loss: 0.01063403021544218, val loss: 0.03036600537598133\n",
      "Epoch: 927, batch loss: 0.009398792870342731, val loss: 0.03036661259829998\n",
      "Epoch: 928, batch loss: 0.009287561289966106, val loss: 0.030496902763843536\n",
      "Epoch: 929, batch loss: 0.009211956523358822, val loss: 0.03060748055577278\n",
      "Epoch: 930, batch loss: 0.010307544842362404, val loss: 0.030372433364391327\n",
      "Epoch: 931, batch loss: 0.010354626923799515, val loss: 0.030464917421340942\n",
      "Epoch: 932, batch loss: 0.01024697721004486, val loss: 0.030938006937503815\n",
      "Epoch: 933, batch loss: 0.010375955142080784, val loss: 0.030530957505106926\n",
      "Epoch: 934, batch loss: 0.009651870466768742, val loss: 0.030598532408475876\n",
      "Epoch: 935, batch loss: 0.010842585936188698, val loss: 0.030398711562156677\n",
      "Epoch: 936, batch loss: 0.009676109068095684, val loss: 0.030430670827627182\n",
      "Epoch: 937, batch loss: 0.010826562531292439, val loss: 0.030558614060282707\n",
      "Epoch: 938, batch loss: 0.010306078009307384, val loss: 0.030595162883400917\n",
      "Epoch: 939, batch loss: 0.009658810682594776, val loss: 0.030583640560507774\n",
      "Epoch: 940, batch loss: 0.011643831618130207, val loss: 0.031734712421894073\n",
      "Epoch: 941, batch loss: 0.011646974831819534, val loss: 0.031386665999889374\n",
      "Epoch: 942, batch loss: 0.011423114687204361, val loss: 0.03216223418712616\n",
      "Epoch: 943, batch loss: 0.010499590076506138, val loss: 0.03174956515431404\n",
      "Epoch: 944, batch loss: 0.014946379698812962, val loss: 0.033557526767253876\n",
      "Epoch: 945, batch loss: 0.013861419633030891, val loss: 0.03464079648256302\n",
      "Epoch: 946, batch loss: 0.015015752986073494, val loss: 0.03598497807979584\n",
      "Epoch: 947, batch loss: 0.01225474663078785, val loss: 0.038122501224279404\n",
      "Epoch: 948, batch loss: 0.01797708123922348, val loss: 0.038867589086294174\n",
      "Epoch: 949, batch loss: 0.017021415755152702, val loss: 0.042732592672109604\n",
      "Epoch: 950, batch loss: 0.018338380381464958, val loss: 0.04186110571026802\n",
      "Epoch: 951, batch loss: 0.025108570232987404, val loss: 0.047195982187986374\n",
      "Epoch: 952, batch loss: 0.023255765438079834, val loss: 0.04480094462633133\n",
      "Epoch: 953, batch loss: 0.02125636674463749, val loss: 0.05230065807700157\n",
      "Epoch: 954, batch loss: 0.01801966316998005, val loss: 0.04680974408984184\n",
      "Epoch: 955, batch loss: 0.027021076530218124, val loss: 0.05054464563727379\n",
      "Epoch: 956, batch loss: 0.025882188230752945, val loss: 0.04722776636481285\n",
      "Epoch: 957, batch loss: 0.023831477388739586, val loss: 0.04473491013050079\n",
      "Epoch: 958, batch loss: 0.01618437096476555, val loss: 0.04390326887369156\n",
      "Epoch: 959, batch loss: 0.022262636572122574, val loss: 0.043887220323085785\n",
      "Epoch: 960, batch loss: 0.018819576129317284, val loss: 0.040910765528678894\n",
      "Epoch: 961, batch loss: 0.01724918559193611, val loss: 0.03872263431549072\n",
      "Epoch: 962, batch loss: 0.015203449875116348, val loss: 0.03949575126171112\n",
      "Epoch: 963, batch loss: 0.015126633457839489, val loss: 0.03970540314912796\n",
      "Epoch: 964, batch loss: 0.0187095794826746, val loss: 0.03984781727194786\n",
      "Epoch: 965, batch loss: 0.013913357630372047, val loss: 0.0404532290995121\n",
      "Epoch: 966, batch loss: 0.015245207585394382, val loss: 0.0420052707195282\n",
      "Epoch: 967, batch loss: 0.013167303055524826, val loss: 0.0409216582775116\n",
      "Epoch: 968, batch loss: 0.017522456124424934, val loss: 0.037124115973711014\n",
      "Epoch: 969, batch loss: 0.012201864272356033, val loss: 0.037564441561698914\n",
      "Epoch: 970, batch loss: 0.018040494993329048, val loss: 0.03714403510093689\n",
      "Epoch: 971, batch loss: 0.010004127398133278, val loss: 0.03537292405962944\n",
      "Epoch: 972, batch loss: 0.014018087647855282, val loss: 0.03612542152404785\n",
      "Epoch: 973, batch loss: 0.012363716959953308, val loss: 0.034383442252874374\n",
      "Epoch: 974, batch loss: 0.013568736612796783, val loss: 0.03544735535979271\n",
      "Epoch: 975, batch loss: 0.010780221782624722, val loss: 0.03497998043894768\n",
      "Epoch: 976, batch loss: 0.01179201528429985, val loss: 0.034632984548807144\n",
      "Epoch: 977, batch loss: 0.013570530340075493, val loss: 0.03479553386569023\n",
      "Epoch: 978, batch loss: 0.011857152916491032, val loss: 0.033257875591516495\n",
      "Epoch: 979, batch loss: 0.009804944507777691, val loss: 0.0332811065018177\n",
      "Epoch: 980, batch loss: 0.00954875536262989, val loss: 0.0330655500292778\n",
      "Epoch: 981, batch loss: 0.012131055817008018, val loss: 0.03448118269443512\n",
      "Epoch: 982, batch loss: 0.011176750995218754, val loss: 0.03399236127734184\n",
      "Epoch: 983, batch loss: 0.009882491081953049, val loss: 0.03356587886810303\n",
      "Epoch: 984, batch loss: 0.010157356970012188, val loss: 0.03288688883185387\n",
      "Epoch: 985, batch loss: 0.010624286718666553, val loss: 0.03286798670887947\n",
      "Epoch: 986, batch loss: 0.011051291599869728, val loss: 0.03274019807577133\n",
      "Epoch: 987, batch loss: 0.008940489962697029, val loss: 0.03270350396633148\n",
      "Epoch: 988, batch loss: 0.010576826520264149, val loss: 0.032547518610954285\n",
      "Epoch: 989, batch loss: 0.010305841453373432, val loss: 0.031799282878637314\n",
      "Epoch: 990, batch loss: 0.009643628261983395, val loss: 0.032283853739500046\n",
      "Epoch: 991, batch loss: 0.009806005284190178, val loss: 0.03292122483253479\n",
      "Epoch: 992, batch loss: 0.010968250222504139, val loss: 0.03308167681097984\n",
      "Epoch: 993, batch loss: 0.010526500642299652, val loss: 0.03135557845234871\n",
      "Epoch: 994, batch loss: 0.010521113872528076, val loss: 0.03043946623802185\n",
      "Epoch: 995, batch loss: 0.010324648581445217, val loss: 0.030432188883423805\n",
      "Epoch: 996, batch loss: 0.010863523930311203, val loss: 0.03116053156554699\n",
      "Epoch: 997, batch loss: 0.00964893028140068, val loss: 0.029969381168484688\n",
      "Epoch: 998, batch loss: 0.010376997292041779, val loss: 0.030552851036190987\n",
      "Epoch: 999, batch loss: 0.01010857056826353, val loss: 0.029979921877384186\n",
      "Epoch: 1000, batch loss: 0.00903842132538557, val loss: 0.03093569166958332\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(no_batches):\n",
    "        # Extract and preprocess training data batch\n",
    "        if batch != no_batches:\n",
    "            data_batch = data_train.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
    "        else:\n",
    "            data_batch = data_train.iloc[batch * batch_size:]\n",
    "        batch_input = torch.from_numpy(data_batch.values)\n",
    "        \n",
    "        # Perform forward pass\n",
    "        batch_output = autoencoder(batch_input.float())\n",
    "        loss = criterion(batch_output, batch_input.float())\n",
    "        losses.append(loss)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_data = torch.from_numpy(data_train.values)\n",
    "    val_preds = autoencoder(val_data.float())\n",
    "    val_loss = criterion(val_preds, val_data.float())\n",
    "    \n",
    "    print(\"Epoch: {}, batch loss: {}, val loss: {}\".format(\n",
    "        epoch + 1, loss.item(), val_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch loss: 0.2800905108451843\n",
      "Epoch: 2, batch loss: 0.2796255052089691\n",
      "Epoch: 3, batch loss: 0.27705979347229004\n",
      "Epoch: 4, batch loss: 0.2742094397544861\n",
      "Epoch: 5, batch loss: 0.27015721797943115\n",
      "Epoch: 6, batch loss: 0.2672477960586548\n",
      "Epoch: 7, batch loss: 0.2645040452480316\n",
      "Epoch: 8, batch loss: 0.2621996998786926\n",
      "Epoch: 9, batch loss: 0.26083800196647644\n",
      "Epoch: 10, batch loss: 0.2592487633228302\n",
      "Epoch: 11, batch loss: 0.25840842723846436\n",
      "Epoch: 12, batch loss: 0.25732147693634033\n",
      "Epoch: 13, batch loss: 0.2563292682170868\n",
      "Epoch: 14, batch loss: 0.25555136799812317\n",
      "Epoch: 15, batch loss: 0.25454434752464294\n",
      "Epoch: 16, batch loss: 0.2540058195590973\n",
      "Epoch: 17, batch loss: 0.25331050157546997\n",
      "Epoch: 18, batch loss: 0.25296857953071594\n",
      "Epoch: 19, batch loss: 0.25276505947113037\n",
      "Epoch: 20, batch loss: 0.25261396169662476\n",
      "Epoch: 21, batch loss: 0.25269976258277893\n",
      "Epoch: 22, batch loss: 0.2526402175426483\n",
      "Epoch: 23, batch loss: 0.2527793347835541\n",
      "Epoch: 24, batch loss: 0.25280866026878357\n",
      "Epoch: 25, batch loss: 0.25287848711013794\n",
      "Epoch: 26, batch loss: 0.2529259920120239\n",
      "Epoch: 27, batch loss: 0.2528737783432007\n",
      "Epoch: 28, batch loss: 0.25287652015686035\n",
      "Epoch: 29, batch loss: 0.252771258354187\n",
      "Epoch: 30, batch loss: 0.25274524092674255\n",
      "Epoch: 31, batch loss: 0.25268319249153137\n",
      "Epoch: 32, batch loss: 0.2526525855064392\n",
      "Epoch: 33, batch loss: 0.2526395320892334\n",
      "Epoch: 34, batch loss: 0.25258517265319824\n",
      "Epoch: 35, batch loss: 0.25256362557411194\n",
      "Epoch: 36, batch loss: 0.2524811625480652\n",
      "Epoch: 37, batch loss: 0.252417653799057\n",
      "Epoch: 38, batch loss: 0.2523152232170105\n",
      "Epoch: 39, batch loss: 0.252208948135376\n",
      "Epoch: 40, batch loss: 0.25211209058761597\n",
      "Epoch: 41, batch loss: 0.25201117992401123\n",
      "Epoch: 42, batch loss: 0.25195300579071045\n",
      "Epoch: 43, batch loss: 0.25189587473869324\n",
      "Epoch: 44, batch loss: 0.2518759071826935\n",
      "Epoch: 45, batch loss: 0.251859188079834\n",
      "Epoch: 46, batch loss: 0.25185099244117737\n",
      "Epoch: 47, batch loss: 0.25184565782546997\n",
      "Epoch: 48, batch loss: 0.25182801485061646\n",
      "Epoch: 49, batch loss: 0.25181859731674194\n",
      "Epoch: 50, batch loss: 0.2517993450164795\n",
      "Epoch: 51, batch loss: 0.25179117918014526\n",
      "Epoch: 52, batch loss: 0.2517833709716797\n",
      "Epoch: 53, batch loss: 0.2517818510532379\n",
      "Epoch: 54, batch loss: 0.251787006855011\n",
      "Epoch: 55, batch loss: 0.25179049372673035\n",
      "Epoch: 56, batch loss: 0.2517995238304138\n",
      "Epoch: 57, batch loss: 0.2518024146556854\n",
      "Epoch: 58, batch loss: 0.2518066167831421\n",
      "Epoch: 59, batch loss: 0.2518061399459839\n",
      "Epoch: 60, batch loss: 0.25180357694625854\n",
      "Epoch: 61, batch loss: 0.25180068612098694\n",
      "Epoch: 62, batch loss: 0.2517947256565094\n",
      "Epoch: 63, batch loss: 0.25179094076156616\n",
      "Epoch: 64, batch loss: 0.25178468227386475\n",
      "Epoch: 65, batch loss: 0.25178030133247375\n",
      "Epoch: 66, batch loss: 0.2517753839492798\n",
      "Epoch: 67, batch loss: 0.2517711818218231\n",
      "Epoch: 68, batch loss: 0.2517682611942291\n",
      "Epoch: 69, batch loss: 0.25176480412483215\n",
      "Epoch: 70, batch loss: 0.25176310539245605\n",
      "Epoch: 71, batch loss: 0.2517606019973755\n",
      "Epoch: 72, batch loss: 0.2517591416835785\n",
      "Epoch: 73, batch loss: 0.2517572343349457\n",
      "Epoch: 74, batch loss: 0.2517554759979248\n",
      "Epoch: 75, batch loss: 0.25175410509109497\n",
      "Epoch: 76, batch loss: 0.2517525553703308\n",
      "Epoch: 77, batch loss: 0.25175192952156067\n",
      "Epoch: 78, batch loss: 0.2517511248588562\n",
      "Epoch: 79, batch loss: 0.25175103545188904\n",
      "Epoch: 80, batch loss: 0.2517509460449219\n",
      "Epoch: 81, batch loss: 0.25175100564956665\n",
      "Epoch: 82, batch loss: 0.25175121426582336\n",
      "Epoch: 83, batch loss: 0.2517511546611786\n",
      "Epoch: 84, batch loss: 0.2517513036727905\n",
      "Epoch: 85, batch loss: 0.25175103545188904\n",
      "Epoch: 86, batch loss: 0.25175097584724426\n",
      "Epoch: 87, batch loss: 0.25175076723098755\n",
      "Epoch: 88, batch loss: 0.251750648021698\n",
      "Epoch: 89, batch loss: 0.2517505884170532\n",
      "Epoch: 90, batch loss: 0.25175049901008606\n",
      "Epoch: 91, batch loss: 0.25175049901008606\n",
      "Epoch: 92, batch loss: 0.2517503499984741\n",
      "Epoch: 93, batch loss: 0.2517501711845398\n",
      "Epoch: 94, batch loss: 0.2517498731613159\n",
      "Epoch: 95, batch loss: 0.25174960494041443\n",
      "Epoch: 96, batch loss: 0.25174933671951294\n",
      "Epoch: 97, batch loss: 0.2517490088939667\n",
      "Epoch: 98, batch loss: 0.2517487704753876\n",
      "Epoch: 99, batch loss: 0.25174853205680847\n",
      "Epoch: 100, batch loss: 0.2517484426498413\n",
      "Epoch: 101, batch loss: 0.25174832344055176\n",
      "Epoch: 102, batch loss: 0.2517482340335846\n",
      "Epoch: 103, batch loss: 0.2517481744289398\n",
      "Epoch: 104, batch loss: 0.25174811482429504\n",
      "Epoch: 105, batch loss: 0.2517480254173279\n",
      "Epoch: 106, batch loss: 0.2517479658126831\n",
      "Epoch: 107, batch loss: 0.2517479658126831\n",
      "Epoch: 108, batch loss: 0.2517479360103607\n",
      "Epoch: 109, batch loss: 0.25174784660339355\n",
      "Epoch: 110, batch loss: 0.25174781680107117\n",
      "Epoch: 111, batch loss: 0.25174781680107117\n",
      "Epoch: 112, batch loss: 0.2517477869987488\n",
      "Epoch: 113, batch loss: 0.251747727394104\n",
      "Epoch: 114, batch loss: 0.2517477869987488\n",
      "Epoch: 115, batch loss: 0.251747727394104\n",
      "Epoch: 116, batch loss: 0.251747727394104\n",
      "Epoch: 117, batch loss: 0.2517476975917816\n",
      "Epoch: 118, batch loss: 0.2517476975917816\n",
      "Epoch: 119, batch loss: 0.25174766778945923\n",
      "Epoch: 120, batch loss: 0.25174766778945923\n",
      "Epoch: 121, batch loss: 0.25174763798713684\n",
      "Epoch: 122, batch loss: 0.25174757838249207\n",
      "Epoch: 123, batch loss: 0.25174757838249207\n",
      "Epoch: 124, batch loss: 0.2517475485801697\n",
      "Epoch: 125, batch loss: 0.2517475187778473\n",
      "Epoch: 126, batch loss: 0.2517475187778473\n",
      "Epoch: 127, batch loss: 0.2517474591732025\n",
      "Epoch: 128, batch loss: 0.2517474889755249\n",
      "Epoch: 129, batch loss: 0.2517474591732025\n",
      "Epoch: 130, batch loss: 0.2517474591732025\n",
      "Epoch: 131, batch loss: 0.2517474293708801\n",
      "Epoch: 132, batch loss: 0.25174739956855774\n",
      "Epoch: 133, batch loss: 0.25174739956855774\n",
      "Epoch: 134, batch loss: 0.25174739956855774\n",
      "Epoch: 135, batch loss: 0.25174736976623535\n",
      "Epoch: 136, batch loss: 0.25174733996391296\n",
      "Epoch: 137, batch loss: 0.25174736976623535\n",
      "Epoch: 138, batch loss: 0.25174736976623535\n",
      "Epoch: 139, batch loss: 0.25174736976623535\n",
      "Epoch: 140, batch loss: 0.2517473101615906\n",
      "Epoch: 141, batch loss: 0.2517473101615906\n",
      "Epoch: 142, batch loss: 0.25174733996391296\n",
      "Epoch: 143, batch loss: 0.25174733996391296\n",
      "Epoch: 144, batch loss: 0.2517473101615906\n",
      "Epoch: 145, batch loss: 0.2517473101615906\n",
      "Epoch: 146, batch loss: 0.2517473101615906\n",
      "Epoch: 147, batch loss: 0.2517473101615906\n",
      "Epoch: 148, batch loss: 0.2517472803592682\n",
      "Epoch: 149, batch loss: 0.2517472505569458\n",
      "Epoch: 150, batch loss: 0.2517472803592682\n",
      "Epoch: 151, batch loss: 0.2517472803592682\n",
      "Epoch: 152, batch loss: 0.2517472505569458\n",
      "Epoch: 153, batch loss: 0.2517472505569458\n",
      "Epoch: 154, batch loss: 0.2517472505569458\n",
      "Epoch: 155, batch loss: 0.2517472207546234\n",
      "Epoch: 156, batch loss: 0.2517472207546234\n",
      "Epoch: 157, batch loss: 0.2517472505569458\n",
      "Epoch: 158, batch loss: 0.2517472207546234\n",
      "Epoch: 159, batch loss: 0.2517472207546234\n",
      "Epoch: 160, batch loss: 0.2517472207546234\n",
      "Epoch: 161, batch loss: 0.2517472207546234\n",
      "Epoch: 162, batch loss: 0.2517472207546234\n",
      "Epoch: 163, batch loss: 0.2517472207546234\n",
      "Epoch: 164, batch loss: 0.2517472207546234\n",
      "Epoch: 165, batch loss: 0.251747190952301\n",
      "Epoch: 166, batch loss: 0.2517472207546234\n",
      "Epoch: 167, batch loss: 0.2517472207546234\n",
      "Epoch: 168, batch loss: 0.2517472207546234\n",
      "Epoch: 169, batch loss: 0.2517472207546234\n",
      "Epoch: 170, batch loss: 0.251747190952301\n",
      "Epoch: 171, batch loss: 0.2517472207546234\n",
      "Epoch: 172, batch loss: 0.251747190952301\n",
      "Epoch: 173, batch loss: 0.25174716114997864\n",
      "Epoch: 174, batch loss: 0.2517472207546234\n",
      "Epoch: 175, batch loss: 0.2517472207546234\n",
      "Epoch: 176, batch loss: 0.251747190952301\n",
      "Epoch: 177, batch loss: 0.2517472207546234\n",
      "Epoch: 178, batch loss: 0.251747190952301\n",
      "Epoch: 179, batch loss: 0.251747190952301\n",
      "Epoch: 180, batch loss: 0.251747190952301\n",
      "Epoch: 181, batch loss: 0.25174716114997864\n",
      "Epoch: 182, batch loss: 0.25174716114997864\n",
      "Epoch: 183, batch loss: 0.251747190952301\n",
      "Epoch: 184, batch loss: 0.25174716114997864\n",
      "Epoch: 185, batch loss: 0.25174716114997864\n",
      "Epoch: 186, batch loss: 0.251747190952301\n",
      "Epoch: 187, batch loss: 0.25174716114997864\n",
      "Epoch: 188, batch loss: 0.25174716114997864\n",
      "Epoch: 189, batch loss: 0.251747190952301\n",
      "Epoch: 190, batch loss: 0.25174716114997864\n",
      "Epoch: 191, batch loss: 0.2517472207546234\n",
      "Epoch: 192, batch loss: 0.25174716114997864\n",
      "Epoch: 193, batch loss: 0.25174716114997864\n",
      "Epoch: 194, batch loss: 0.25174716114997864\n",
      "Epoch: 195, batch loss: 0.25174716114997864\n",
      "Epoch: 196, batch loss: 0.25174716114997864\n",
      "Epoch: 197, batch loss: 0.25174716114997864\n",
      "Epoch: 198, batch loss: 0.251747190952301\n",
      "Epoch: 199, batch loss: 0.25174716114997864\n",
      "Epoch: 200, batch loss: 0.25174716114997864\n",
      "Epoch: 201, batch loss: 0.25174716114997864\n",
      "Epoch: 202, batch loss: 0.25174716114997864\n",
      "Epoch: 203, batch loss: 0.25174713134765625\n",
      "Epoch: 204, batch loss: 0.25174716114997864\n",
      "Epoch: 205, batch loss: 0.25174716114997864\n",
      "Epoch: 206, batch loss: 0.25174716114997864\n",
      "Epoch: 207, batch loss: 0.25174713134765625\n",
      "Epoch: 208, batch loss: 0.251747190952301\n",
      "Epoch: 209, batch loss: 0.25174716114997864\n",
      "Epoch: 210, batch loss: 0.25174713134765625\n",
      "Epoch: 211, batch loss: 0.251747190952301\n",
      "Epoch: 212, batch loss: 0.25174716114997864\n",
      "Epoch: 213, batch loss: 0.25174716114997864\n",
      "Epoch: 214, batch loss: 0.25174716114997864\n",
      "Epoch: 215, batch loss: 0.25174716114997864\n",
      "Epoch: 216, batch loss: 0.25174716114997864\n",
      "Epoch: 217, batch loss: 0.25174716114997864\n",
      "Epoch: 218, batch loss: 0.25174713134765625\n",
      "Epoch: 219, batch loss: 0.25174716114997864\n",
      "Epoch: 220, batch loss: 0.25174713134765625\n",
      "Epoch: 221, batch loss: 0.25174716114997864\n",
      "Epoch: 222, batch loss: 0.25174716114997864\n",
      "Epoch: 223, batch loss: 0.25174716114997864\n",
      "Epoch: 224, batch loss: 0.25174713134765625\n",
      "Epoch: 225, batch loss: 0.25174716114997864\n",
      "Epoch: 226, batch loss: 0.25174716114997864\n",
      "Epoch: 227, batch loss: 0.25174716114997864\n",
      "Epoch: 228, batch loss: 0.25174716114997864\n",
      "Epoch: 229, batch loss: 0.25174716114997864\n",
      "Epoch: 230, batch loss: 0.25174716114997864\n",
      "Epoch: 231, batch loss: 0.25174716114997864\n",
      "Epoch: 232, batch loss: 0.25174713134765625\n",
      "Epoch: 233, batch loss: 0.25174716114997864\n",
      "Epoch: 234, batch loss: 0.25174716114997864\n",
      "Epoch: 235, batch loss: 0.25174716114997864\n",
      "Epoch: 236, batch loss: 0.25174713134765625\n",
      "Epoch: 237, batch loss: 0.25174710154533386\n",
      "Epoch: 238, batch loss: 0.25174710154533386\n",
      "Epoch: 239, batch loss: 0.25174710154533386\n",
      "Epoch: 240, batch loss: 0.25174713134765625\n",
      "Epoch: 241, batch loss: 0.25174716114997864\n",
      "Epoch: 242, batch loss: 0.25174716114997864\n",
      "Epoch: 243, batch loss: 0.25174716114997864\n",
      "Epoch: 244, batch loss: 0.25174716114997864\n",
      "Epoch: 245, batch loss: 0.25174716114997864\n",
      "Epoch: 246, batch loss: 0.25174713134765625\n",
      "Epoch: 247, batch loss: 0.25174713134765625\n",
      "Epoch: 248, batch loss: 0.25174713134765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249, batch loss: 0.25174713134765625\n",
      "Epoch: 250, batch loss: 0.25174716114997864\n",
      "Epoch: 251, batch loss: 0.25174716114997864\n",
      "Epoch: 252, batch loss: 0.25174716114997864\n",
      "Epoch: 253, batch loss: 0.25174713134765625\n",
      "Epoch: 254, batch loss: 0.25174716114997864\n",
      "Epoch: 255, batch loss: 0.25174716114997864\n",
      "Epoch: 256, batch loss: 0.25174716114997864\n",
      "Epoch: 257, batch loss: 0.25174716114997864\n",
      "Epoch: 258, batch loss: 0.25174716114997864\n",
      "Epoch: 259, batch loss: 0.25174716114997864\n",
      "Epoch: 260, batch loss: 0.25174716114997864\n",
      "Epoch: 261, batch loss: 0.25174713134765625\n",
      "Epoch: 262, batch loss: 0.25174716114997864\n",
      "Epoch: 263, batch loss: 0.25174716114997864\n",
      "Epoch: 264, batch loss: 0.25174716114997864\n",
      "Epoch: 265, batch loss: 0.25174716114997864\n",
      "Epoch: 266, batch loss: 0.25174713134765625\n",
      "Epoch: 267, batch loss: 0.251747190952301\n",
      "Epoch: 268, batch loss: 0.25174716114997864\n",
      "Epoch: 269, batch loss: 0.25174716114997864\n",
      "Epoch: 270, batch loss: 0.25174716114997864\n",
      "Epoch: 271, batch loss: 0.25174710154533386\n",
      "Epoch: 272, batch loss: 0.25174716114997864\n",
      "Epoch: 273, batch loss: 0.25174716114997864\n",
      "Epoch: 274, batch loss: 0.25174716114997864\n",
      "Epoch: 275, batch loss: 0.25174713134765625\n",
      "Epoch: 276, batch loss: 0.25174716114997864\n",
      "Epoch: 277, batch loss: 0.25174710154533386\n",
      "Epoch: 278, batch loss: 0.25174716114997864\n",
      "Epoch: 279, batch loss: 0.25174716114997864\n",
      "Epoch: 280, batch loss: 0.25174716114997864\n",
      "Epoch: 281, batch loss: 0.25174710154533386\n",
      "Epoch: 282, batch loss: 0.25174713134765625\n",
      "Epoch: 283, batch loss: 0.25174713134765625\n",
      "Epoch: 284, batch loss: 0.25174713134765625\n",
      "Epoch: 285, batch loss: 0.25174713134765625\n",
      "Epoch: 286, batch loss: 0.25174710154533386\n",
      "Epoch: 287, batch loss: 0.25174716114997864\n",
      "Epoch: 288, batch loss: 0.25174716114997864\n",
      "Epoch: 289, batch loss: 0.25174710154533386\n",
      "Epoch: 290, batch loss: 0.25174713134765625\n",
      "Epoch: 291, batch loss: 0.25174710154533386\n",
      "Epoch: 292, batch loss: 0.25174710154533386\n",
      "Epoch: 293, batch loss: 0.25174713134765625\n",
      "Epoch: 294, batch loss: 0.25174713134765625\n",
      "Epoch: 295, batch loss: 0.25174716114997864\n",
      "Epoch: 296, batch loss: 0.25174716114997864\n",
      "Epoch: 297, batch loss: 0.25174716114997864\n",
      "Epoch: 298, batch loss: 0.25174713134765625\n",
      "Epoch: 299, batch loss: 0.25174713134765625\n",
      "Epoch: 300, batch loss: 0.25174716114997864\n",
      "Epoch: 301, batch loss: 0.25174713134765625\n",
      "Epoch: 302, batch loss: 0.25174716114997864\n",
      "Epoch: 303, batch loss: 0.25174716114997864\n",
      "Epoch: 304, batch loss: 0.25174716114997864\n",
      "Epoch: 305, batch loss: 0.25174710154533386\n",
      "Epoch: 306, batch loss: 0.25174716114997864\n",
      "Epoch: 307, batch loss: 0.25174716114997864\n",
      "Epoch: 308, batch loss: 0.251747190952301\n",
      "Epoch: 309, batch loss: 0.251747190952301\n",
      "Epoch: 310, batch loss: 0.251747190952301\n",
      "Epoch: 311, batch loss: 0.25174716114997864\n",
      "Epoch: 312, batch loss: 0.25174716114997864\n",
      "Epoch: 313, batch loss: 0.25174716114997864\n",
      "Epoch: 314, batch loss: 0.25174716114997864\n",
      "Epoch: 315, batch loss: 0.25174716114997864\n",
      "Epoch: 316, batch loss: 0.25174716114997864\n",
      "Epoch: 317, batch loss: 0.25174716114997864\n",
      "Epoch: 318, batch loss: 0.25174716114997864\n",
      "Epoch: 319, batch loss: 0.25174716114997864\n",
      "Epoch: 320, batch loss: 0.25174716114997864\n",
      "Epoch: 321, batch loss: 0.25174716114997864\n",
      "Epoch: 322, batch loss: 0.25174716114997864\n",
      "Epoch: 323, batch loss: 0.25174716114997864\n",
      "Epoch: 324, batch loss: 0.25174716114997864\n",
      "Epoch: 325, batch loss: 0.25174716114997864\n",
      "Epoch: 326, batch loss: 0.25174716114997864\n",
      "Epoch: 327, batch loss: 0.25174713134765625\n",
      "Epoch: 328, batch loss: 0.25174716114997864\n",
      "Epoch: 329, batch loss: 0.25174716114997864\n",
      "Epoch: 330, batch loss: 0.25174716114997864\n",
      "Epoch: 331, batch loss: 0.25174716114997864\n",
      "Epoch: 332, batch loss: 0.25174716114997864\n",
      "Epoch: 333, batch loss: 0.25174716114997864\n",
      "Epoch: 334, batch loss: 0.25174716114997864\n",
      "Epoch: 335, batch loss: 0.25174716114997864\n",
      "Epoch: 336, batch loss: 0.25174716114997864\n",
      "Epoch: 337, batch loss: 0.25174716114997864\n",
      "Epoch: 338, batch loss: 0.25174716114997864\n",
      "Epoch: 339, batch loss: 0.25174716114997864\n",
      "Epoch: 340, batch loss: 0.25174713134765625\n",
      "Epoch: 341, batch loss: 0.25174716114997864\n",
      "Epoch: 342, batch loss: 0.25174716114997864\n",
      "Epoch: 343, batch loss: 0.25174713134765625\n",
      "Epoch: 344, batch loss: 0.25174716114997864\n",
      "Epoch: 345, batch loss: 0.25174713134765625\n",
      "Epoch: 346, batch loss: 0.25174716114997864\n",
      "Epoch: 347, batch loss: 0.25174716114997864\n",
      "Epoch: 348, batch loss: 0.25174716114997864\n",
      "Epoch: 349, batch loss: 0.25174716114997864\n",
      "Epoch: 350, batch loss: 0.25174716114997864\n",
      "Epoch: 351, batch loss: 0.25174716114997864\n",
      "Epoch: 352, batch loss: 0.25174713134765625\n",
      "Epoch: 353, batch loss: 0.25174716114997864\n",
      "Epoch: 354, batch loss: 0.25174716114997864\n",
      "Epoch: 355, batch loss: 0.25174713134765625\n",
      "Epoch: 356, batch loss: 0.25174716114997864\n",
      "Epoch: 357, batch loss: 0.25174716114997864\n",
      "Epoch: 358, batch loss: 0.25174716114997864\n",
      "Epoch: 359, batch loss: 0.25174713134765625\n",
      "Epoch: 360, batch loss: 0.25174716114997864\n",
      "Epoch: 361, batch loss: 0.25174716114997864\n",
      "Epoch: 362, batch loss: 0.25174713134765625\n",
      "Epoch: 363, batch loss: 0.25174716114997864\n",
      "Epoch: 364, batch loss: 0.25174713134765625\n",
      "Epoch: 365, batch loss: 0.25174713134765625\n",
      "Epoch: 366, batch loss: 0.25174713134765625\n",
      "Epoch: 367, batch loss: 0.25174713134765625\n",
      "Epoch: 368, batch loss: 0.25174716114997864\n",
      "Epoch: 369, batch loss: 0.251747190952301\n",
      "Epoch: 370, batch loss: 0.25174713134765625\n",
      "Epoch: 371, batch loss: 0.25174716114997864\n",
      "Epoch: 372, batch loss: 0.25174713134765625\n",
      "Epoch: 373, batch loss: 0.25174713134765625\n",
      "Epoch: 374, batch loss: 0.25174713134765625\n",
      "Epoch: 375, batch loss: 0.25174713134765625\n",
      "Epoch: 376, batch loss: 0.25174713134765625\n",
      "Epoch: 377, batch loss: 0.25174713134765625\n",
      "Epoch: 378, batch loss: 0.25174713134765625\n",
      "Epoch: 379, batch loss: 0.25174713134765625\n",
      "Epoch: 380, batch loss: 0.25174713134765625\n",
      "Epoch: 381, batch loss: 0.25174713134765625\n",
      "Epoch: 382, batch loss: 0.25174713134765625\n",
      "Epoch: 383, batch loss: 0.25174713134765625\n",
      "Epoch: 384, batch loss: 0.25174713134765625\n",
      "Epoch: 385, batch loss: 0.25174713134765625\n",
      "Epoch: 386, batch loss: 0.25174716114997864\n",
      "Epoch: 387, batch loss: 0.25174716114997864\n",
      "Epoch: 388, batch loss: 0.25174716114997864\n",
      "Epoch: 389, batch loss: 0.25174716114997864\n",
      "Epoch: 390, batch loss: 0.25174716114997864\n",
      "Epoch: 391, batch loss: 0.25174716114997864\n",
      "Epoch: 392, batch loss: 0.25174716114997864\n",
      "Epoch: 393, batch loss: 0.25174716114997864\n",
      "Epoch: 394, batch loss: 0.25174716114997864\n",
      "Epoch: 395, batch loss: 0.25174716114997864\n",
      "Epoch: 396, batch loss: 0.25174716114997864\n",
      "Epoch: 397, batch loss: 0.25174716114997864\n",
      "Epoch: 398, batch loss: 0.25174716114997864\n",
      "Epoch: 399, batch loss: 0.25174716114997864\n",
      "Epoch: 400, batch loss: 0.25174716114997864\n",
      "Epoch: 401, batch loss: 0.25174716114997864\n",
      "Epoch: 402, batch loss: 0.25174716114997864\n",
      "Epoch: 403, batch loss: 0.25174716114997864\n",
      "Epoch: 404, batch loss: 0.25174716114997864\n",
      "Epoch: 405, batch loss: 0.25174716114997864\n",
      "Epoch: 406, batch loss: 0.25174716114997864\n",
      "Epoch: 407, batch loss: 0.25174716114997864\n",
      "Epoch: 408, batch loss: 0.25174716114997864\n",
      "Epoch: 409, batch loss: 0.25174716114997864\n",
      "Epoch: 410, batch loss: 0.25174713134765625\n",
      "Epoch: 411, batch loss: 0.25174716114997864\n",
      "Epoch: 412, batch loss: 0.25174716114997864\n",
      "Epoch: 413, batch loss: 0.25174713134765625\n",
      "Epoch: 414, batch loss: 0.25174713134765625\n",
      "Epoch: 415, batch loss: 0.25174716114997864\n",
      "Epoch: 416, batch loss: 0.25174716114997864\n",
      "Epoch: 417, batch loss: 0.25174716114997864\n",
      "Epoch: 418, batch loss: 0.25174716114997864\n",
      "Epoch: 419, batch loss: 0.25174716114997864\n",
      "Epoch: 420, batch loss: 0.25174716114997864\n",
      "Epoch: 421, batch loss: 0.25174716114997864\n",
      "Epoch: 422, batch loss: 0.25174716114997864\n",
      "Epoch: 423, batch loss: 0.25174716114997864\n",
      "Epoch: 424, batch loss: 0.25174713134765625\n",
      "Epoch: 425, batch loss: 0.25174716114997864\n",
      "Epoch: 426, batch loss: 0.25174716114997864\n",
      "Epoch: 427, batch loss: 0.25174716114997864\n",
      "Epoch: 428, batch loss: 0.25174713134765625\n",
      "Epoch: 429, batch loss: 0.25174716114997864\n",
      "Epoch: 430, batch loss: 0.25174713134765625\n",
      "Epoch: 431, batch loss: 0.25174713134765625\n",
      "Epoch: 432, batch loss: 0.25174716114997864\n",
      "Epoch: 433, batch loss: 0.25174716114997864\n",
      "Epoch: 434, batch loss: 0.25174716114997864\n",
      "Epoch: 435, batch loss: 0.25174716114997864\n",
      "Epoch: 436, batch loss: 0.25174716114997864\n",
      "Epoch: 437, batch loss: 0.25174716114997864\n",
      "Epoch: 438, batch loss: 0.25174713134765625\n",
      "Epoch: 439, batch loss: 0.25174716114997864\n",
      "Epoch: 440, batch loss: 0.25174716114997864\n",
      "Epoch: 441, batch loss: 0.25174713134765625\n",
      "Epoch: 442, batch loss: 0.25174713134765625\n",
      "Epoch: 443, batch loss: 0.25174716114997864\n",
      "Epoch: 444, batch loss: 0.25174716114997864\n",
      "Epoch: 445, batch loss: 0.25174716114997864\n",
      "Epoch: 446, batch loss: 0.25174716114997864\n",
      "Epoch: 447, batch loss: 0.25174716114997864\n",
      "Epoch: 448, batch loss: 0.25174713134765625\n",
      "Epoch: 449, batch loss: 0.25174716114997864\n",
      "Epoch: 450, batch loss: 0.25174716114997864\n",
      "Epoch: 451, batch loss: 0.25174716114997864\n",
      "Epoch: 452, batch loss: 0.25174716114997864\n",
      "Epoch: 453, batch loss: 0.25174713134765625\n",
      "Epoch: 454, batch loss: 0.25174716114997864\n",
      "Epoch: 455, batch loss: 0.25174716114997864\n",
      "Epoch: 456, batch loss: 0.25174713134765625\n",
      "Epoch: 457, batch loss: 0.25174716114997864\n",
      "Epoch: 458, batch loss: 0.25174713134765625\n",
      "Epoch: 459, batch loss: 0.25174716114997864\n",
      "Epoch: 460, batch loss: 0.25174716114997864\n",
      "Epoch: 461, batch loss: 0.25174716114997864\n",
      "Epoch: 462, batch loss: 0.25174716114997864\n",
      "Epoch: 463, batch loss: 0.25174716114997864\n",
      "Epoch: 464, batch loss: 0.25174716114997864\n",
      "Epoch: 465, batch loss: 0.25174713134765625\n",
      "Epoch: 466, batch loss: 0.25174716114997864\n",
      "Epoch: 467, batch loss: 0.25174716114997864\n",
      "Epoch: 468, batch loss: 0.25174716114997864\n",
      "Epoch: 469, batch loss: 0.25174716114997864\n",
      "Epoch: 470, batch loss: 0.25174716114997864\n",
      "Epoch: 471, batch loss: 0.25174713134765625\n",
      "Epoch: 472, batch loss: 0.25174713134765625\n",
      "Epoch: 473, batch loss: 0.25174713134765625\n",
      "Epoch: 474, batch loss: 0.25174713134765625\n",
      "Epoch: 475, batch loss: 0.25174713134765625\n",
      "Epoch: 476, batch loss: 0.25174713134765625\n",
      "Epoch: 477, batch loss: 0.25174713134765625\n",
      "Epoch: 478, batch loss: 0.25174713134765625\n",
      "Epoch: 479, batch loss: 0.25174716114997864\n",
      "Epoch: 480, batch loss: 0.25174710154533386\n",
      "Epoch: 481, batch loss: 0.25174716114997864\n",
      "Epoch: 482, batch loss: 0.25174713134765625\n",
      "Epoch: 483, batch loss: 0.25174710154533386\n",
      "Epoch: 484, batch loss: 0.25174716114997864\n",
      "Epoch: 485, batch loss: 0.25174713134765625\n",
      "Epoch: 486, batch loss: 0.25174713134765625\n",
      "Epoch: 487, batch loss: 0.25174710154533386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 488, batch loss: 0.25174713134765625\n",
      "Epoch: 489, batch loss: 0.25174710154533386\n",
      "Epoch: 490, batch loss: 0.25174716114997864\n",
      "Epoch: 491, batch loss: 0.25174713134765625\n",
      "Epoch: 492, batch loss: 0.25174716114997864\n",
      "Epoch: 493, batch loss: 0.25174713134765625\n",
      "Epoch: 494, batch loss: 0.25174713134765625\n",
      "Epoch: 495, batch loss: 0.25174716114997864\n",
      "Epoch: 496, batch loss: 0.25174713134765625\n",
      "Epoch: 497, batch loss: 0.25174716114997864\n",
      "Epoch: 498, batch loss: 0.25174716114997864\n",
      "Epoch: 499, batch loss: 0.25174716114997864\n",
      "Epoch: 500, batch loss: 0.25174716114997864\n",
      "Epoch: 501, batch loss: 0.25174710154533386\n",
      "Epoch: 502, batch loss: 0.25174713134765625\n",
      "Epoch: 503, batch loss: 0.25174716114997864\n",
      "Epoch: 504, batch loss: 0.25174716114997864\n",
      "Epoch: 505, batch loss: 0.25174713134765625\n",
      "Epoch: 506, batch loss: 0.25174716114997864\n",
      "Epoch: 507, batch loss: 0.25174710154533386\n",
      "Epoch: 508, batch loss: 0.25174710154533386\n",
      "Epoch: 509, batch loss: 0.25174713134765625\n",
      "Epoch: 510, batch loss: 0.25174713134765625\n",
      "Epoch: 511, batch loss: 0.25174713134765625\n",
      "Epoch: 512, batch loss: 0.25174713134765625\n",
      "Epoch: 513, batch loss: 0.25174713134765625\n",
      "Epoch: 514, batch loss: 0.25174710154533386\n",
      "Epoch: 515, batch loss: 0.25174710154533386\n",
      "Epoch: 516, batch loss: 0.25174710154533386\n",
      "Epoch: 517, batch loss: 0.25174710154533386\n",
      "Epoch: 518, batch loss: 0.25174713134765625\n",
      "Epoch: 519, batch loss: 0.25174713134765625\n",
      "Epoch: 520, batch loss: 0.25174716114997864\n",
      "Epoch: 521, batch loss: 0.25174713134765625\n",
      "Epoch: 522, batch loss: 0.25174713134765625\n",
      "Epoch: 523, batch loss: 0.25174713134765625\n",
      "Epoch: 524, batch loss: 0.25174716114997864\n",
      "Epoch: 525, batch loss: 0.25174716114997864\n",
      "Epoch: 526, batch loss: 0.25174716114997864\n",
      "Epoch: 527, batch loss: 0.25174713134765625\n",
      "Epoch: 528, batch loss: 0.25174713134765625\n",
      "Epoch: 529, batch loss: 0.25174713134765625\n",
      "Epoch: 530, batch loss: 0.25174713134765625\n",
      "Epoch: 531, batch loss: 0.25174710154533386\n",
      "Epoch: 532, batch loss: 0.25174710154533386\n",
      "Epoch: 533, batch loss: 0.25174713134765625\n",
      "Epoch: 534, batch loss: 0.25174713134765625\n",
      "Epoch: 535, batch loss: 0.25174713134765625\n",
      "Epoch: 536, batch loss: 0.25174710154533386\n",
      "Epoch: 537, batch loss: 0.25174710154533386\n",
      "Epoch: 538, batch loss: 0.25174713134765625\n",
      "Epoch: 539, batch loss: 0.25174713134765625\n",
      "Epoch: 540, batch loss: 0.25174713134765625\n",
      "Epoch: 541, batch loss: 0.25174710154533386\n",
      "Epoch: 542, batch loss: 0.25174713134765625\n",
      "Epoch: 543, batch loss: 0.25174716114997864\n",
      "Epoch: 544, batch loss: 0.25174713134765625\n",
      "Epoch: 545, batch loss: 0.25174713134765625\n",
      "Epoch: 546, batch loss: 0.25174713134765625\n",
      "Epoch: 547, batch loss: 0.25174713134765625\n",
      "Epoch: 548, batch loss: 0.25174713134765625\n",
      "Epoch: 549, batch loss: 0.25174716114997864\n",
      "Epoch: 550, batch loss: 0.25174713134765625\n",
      "Epoch: 551, batch loss: 0.25174713134765625\n",
      "Epoch: 552, batch loss: 0.25174716114997864\n",
      "Epoch: 553, batch loss: 0.25174716114997864\n",
      "Epoch: 554, batch loss: 0.25174716114997864\n",
      "Epoch: 555, batch loss: 0.25174713134765625\n",
      "Epoch: 556, batch loss: 0.25174716114997864\n",
      "Epoch: 557, batch loss: 0.25174716114997864\n",
      "Epoch: 558, batch loss: 0.25174716114997864\n",
      "Epoch: 559, batch loss: 0.25174716114997864\n",
      "Epoch: 560, batch loss: 0.25174713134765625\n",
      "Epoch: 561, batch loss: 0.25174713134765625\n",
      "Epoch: 562, batch loss: 0.25174716114997864\n",
      "Epoch: 563, batch loss: 0.25174716114997864\n",
      "Epoch: 564, batch loss: 0.25174716114997864\n",
      "Epoch: 565, batch loss: 0.25174716114997864\n",
      "Epoch: 566, batch loss: 0.25174713134765625\n",
      "Epoch: 567, batch loss: 0.25174713134765625\n",
      "Epoch: 568, batch loss: 0.25174713134765625\n",
      "Epoch: 569, batch loss: 0.25174716114997864\n",
      "Epoch: 570, batch loss: 0.25174713134765625\n",
      "Epoch: 571, batch loss: 0.25174713134765625\n",
      "Epoch: 572, batch loss: 0.25174713134765625\n",
      "Epoch: 573, batch loss: 0.25174713134765625\n",
      "Epoch: 574, batch loss: 0.25174713134765625\n",
      "Epoch: 575, batch loss: 0.25174710154533386\n",
      "Epoch: 576, batch loss: 0.25174713134765625\n",
      "Epoch: 577, batch loss: 0.25174713134765625\n",
      "Epoch: 578, batch loss: 0.25174713134765625\n",
      "Epoch: 579, batch loss: 0.25174710154533386\n",
      "Epoch: 580, batch loss: 0.25174713134765625\n",
      "Epoch: 581, batch loss: 0.25174710154533386\n",
      "Epoch: 582, batch loss: 0.25174716114997864\n",
      "Epoch: 583, batch loss: 0.25174716114997864\n",
      "Epoch: 584, batch loss: 0.25174713134765625\n",
      "Epoch: 585, batch loss: 0.25174713134765625\n",
      "Epoch: 586, batch loss: 0.25174713134765625\n",
      "Epoch: 587, batch loss: 0.25174710154533386\n",
      "Epoch: 588, batch loss: 0.25174716114997864\n",
      "Epoch: 589, batch loss: 0.25174710154533386\n",
      "Epoch: 590, batch loss: 0.25174710154533386\n",
      "Epoch: 591, batch loss: 0.25174713134765625\n",
      "Epoch: 592, batch loss: 0.25174713134765625\n",
      "Epoch: 593, batch loss: 0.25174713134765625\n",
      "Epoch: 594, batch loss: 0.25174713134765625\n",
      "Epoch: 595, batch loss: 0.25174716114997864\n",
      "Epoch: 596, batch loss: 0.25174716114997864\n",
      "Epoch: 597, batch loss: 0.25174713134765625\n",
      "Epoch: 598, batch loss: 0.25174713134765625\n",
      "Epoch: 599, batch loss: 0.25174713134765625\n",
      "Epoch: 600, batch loss: 0.25174716114997864\n",
      "Epoch: 601, batch loss: 0.25174716114997864\n",
      "Epoch: 602, batch loss: 0.25174716114997864\n",
      "Epoch: 603, batch loss: 0.25174716114997864\n",
      "Epoch: 604, batch loss: 0.25174716114997864\n",
      "Epoch: 605, batch loss: 0.25174713134765625\n",
      "Epoch: 606, batch loss: 0.25174716114997864\n",
      "Epoch: 607, batch loss: 0.25174716114997864\n",
      "Epoch: 608, batch loss: 0.25174713134765625\n",
      "Epoch: 609, batch loss: 0.25174713134765625\n",
      "Epoch: 610, batch loss: 0.25174713134765625\n",
      "Epoch: 611, batch loss: 0.25174713134765625\n",
      "Epoch: 612, batch loss: 0.25174713134765625\n",
      "Epoch: 613, batch loss: 0.25174716114997864\n",
      "Epoch: 614, batch loss: 0.25174713134765625\n",
      "Epoch: 615, batch loss: 0.25174713134765625\n",
      "Epoch: 616, batch loss: 0.25174713134765625\n",
      "Epoch: 617, batch loss: 0.25174713134765625\n",
      "Epoch: 618, batch loss: 0.25174713134765625\n",
      "Epoch: 619, batch loss: 0.25174713134765625\n",
      "Epoch: 620, batch loss: 0.25174713134765625\n",
      "Epoch: 621, batch loss: 0.25174713134765625\n",
      "Epoch: 622, batch loss: 0.25174713134765625\n",
      "Epoch: 623, batch loss: 0.25174713134765625\n",
      "Epoch: 624, batch loss: 0.25174713134765625\n",
      "Epoch: 625, batch loss: 0.25174710154533386\n",
      "Epoch: 626, batch loss: 0.25174713134765625\n",
      "Epoch: 627, batch loss: 0.25174710154533386\n",
      "Epoch: 628, batch loss: 0.25174716114997864\n",
      "Epoch: 629, batch loss: 0.25174713134765625\n",
      "Epoch: 630, batch loss: 0.25174716114997864\n",
      "Epoch: 631, batch loss: 0.25174713134765625\n",
      "Epoch: 632, batch loss: 0.25174710154533386\n",
      "Epoch: 633, batch loss: 0.25174710154533386\n",
      "Epoch: 634, batch loss: 0.25174713134765625\n",
      "Epoch: 635, batch loss: 0.25174713134765625\n",
      "Epoch: 636, batch loss: 0.25174713134765625\n",
      "Epoch: 637, batch loss: 0.25174713134765625\n",
      "Epoch: 638, batch loss: 0.25174716114997864\n",
      "Epoch: 639, batch loss: 0.25174710154533386\n",
      "Epoch: 640, batch loss: 0.25174713134765625\n",
      "Epoch: 641, batch loss: 0.25174716114997864\n",
      "Epoch: 642, batch loss: 0.25174713134765625\n",
      "Epoch: 643, batch loss: 0.25174713134765625\n",
      "Epoch: 644, batch loss: 0.25174713134765625\n",
      "Epoch: 645, batch loss: 0.25174713134765625\n",
      "Epoch: 646, batch loss: 0.25174716114997864\n",
      "Epoch: 647, batch loss: 0.25174716114997864\n",
      "Epoch: 648, batch loss: 0.25174713134765625\n",
      "Epoch: 649, batch loss: 0.25174713134765625\n",
      "Epoch: 650, batch loss: 0.25174713134765625\n",
      "Epoch: 651, batch loss: 0.25174713134765625\n",
      "Epoch: 652, batch loss: 0.25174710154533386\n",
      "Epoch: 653, batch loss: 0.25174713134765625\n",
      "Epoch: 654, batch loss: 0.25174713134765625\n",
      "Epoch: 655, batch loss: 0.25174716114997864\n",
      "Epoch: 656, batch loss: 0.25174716114997864\n",
      "Epoch: 657, batch loss: 0.25174713134765625\n",
      "Epoch: 658, batch loss: 0.25174716114997864\n",
      "Epoch: 659, batch loss: 0.25174713134765625\n",
      "Epoch: 660, batch loss: 0.25174713134765625\n",
      "Epoch: 661, batch loss: 0.25174713134765625\n",
      "Epoch: 662, batch loss: 0.25174716114997864\n",
      "Epoch: 663, batch loss: 0.25174713134765625\n",
      "Epoch: 664, batch loss: 0.25174716114997864\n",
      "Epoch: 665, batch loss: 0.25174713134765625\n",
      "Epoch: 666, batch loss: 0.25174713134765625\n",
      "Epoch: 667, batch loss: 0.25174713134765625\n",
      "Epoch: 668, batch loss: 0.25174713134765625\n",
      "Epoch: 669, batch loss: 0.25174716114997864\n",
      "Epoch: 670, batch loss: 0.25174716114997864\n",
      "Epoch: 671, batch loss: 0.25174713134765625\n",
      "Epoch: 672, batch loss: 0.25174713134765625\n",
      "Epoch: 673, batch loss: 0.25174710154533386\n",
      "Epoch: 674, batch loss: 0.25174716114997864\n",
      "Epoch: 675, batch loss: 0.25174716114997864\n",
      "Epoch: 676, batch loss: 0.25174716114997864\n",
      "Epoch: 677, batch loss: 0.25174716114997864\n",
      "Epoch: 678, batch loss: 0.25174716114997864\n",
      "Epoch: 679, batch loss: 0.25174716114997864\n",
      "Epoch: 680, batch loss: 0.25174710154533386\n",
      "Epoch: 681, batch loss: 0.25174713134765625\n",
      "Epoch: 682, batch loss: 0.25174716114997864\n",
      "Epoch: 683, batch loss: 0.25174716114997864\n",
      "Epoch: 684, batch loss: 0.25174716114997864\n",
      "Epoch: 685, batch loss: 0.25174713134765625\n",
      "Epoch: 686, batch loss: 0.25174713134765625\n",
      "Epoch: 687, batch loss: 0.25174710154533386\n",
      "Epoch: 688, batch loss: 0.25174713134765625\n",
      "Epoch: 689, batch loss: 0.25174713134765625\n",
      "Epoch: 690, batch loss: 0.25174716114997864\n",
      "Epoch: 691, batch loss: 0.25174710154533386\n",
      "Epoch: 692, batch loss: 0.25174716114997864\n",
      "Epoch: 693, batch loss: 0.25174713134765625\n",
      "Epoch: 694, batch loss: 0.25174713134765625\n",
      "Epoch: 695, batch loss: 0.25174713134765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 696, batch loss: 0.25174713134765625\n",
      "Epoch: 697, batch loss: 0.25174716114997864\n",
      "Epoch: 698, batch loss: 0.25174716114997864\n",
      "Epoch: 699, batch loss: 0.25174713134765625\n",
      "Epoch: 700, batch loss: 0.25174713134765625\n",
      "Epoch: 701, batch loss: 0.25174713134765625\n",
      "Epoch: 702, batch loss: 0.25174716114997864\n",
      "Epoch: 703, batch loss: 0.25174716114997864\n",
      "Epoch: 704, batch loss: 0.25174710154533386\n",
      "Epoch: 705, batch loss: 0.25174713134765625\n",
      "Epoch: 706, batch loss: 0.25174713134765625\n",
      "Epoch: 707, batch loss: 0.25174713134765625\n",
      "Epoch: 708, batch loss: 0.25174710154533386\n",
      "Epoch: 709, batch loss: 0.25174713134765625\n",
      "Epoch: 710, batch loss: 0.25174713134765625\n",
      "Epoch: 711, batch loss: 0.25174713134765625\n",
      "Epoch: 712, batch loss: 0.25174713134765625\n",
      "Epoch: 713, batch loss: 0.25174716114997864\n",
      "Epoch: 714, batch loss: 0.25174716114997864\n",
      "Epoch: 715, batch loss: 0.25174713134765625\n",
      "Epoch: 716, batch loss: 0.25174713134765625\n",
      "Epoch: 717, batch loss: 0.25174716114997864\n",
      "Epoch: 718, batch loss: 0.25174716114997864\n",
      "Epoch: 719, batch loss: 0.25174716114997864\n",
      "Epoch: 720, batch loss: 0.25174713134765625\n",
      "Epoch: 721, batch loss: 0.25174713134765625\n",
      "Epoch: 722, batch loss: 0.25174716114997864\n",
      "Epoch: 723, batch loss: 0.25174713134765625\n",
      "Epoch: 724, batch loss: 0.25174713134765625\n",
      "Epoch: 725, batch loss: 0.25174716114997864\n",
      "Epoch: 726, batch loss: 0.25174713134765625\n",
      "Epoch: 727, batch loss: 0.25174713134765625\n",
      "Epoch: 728, batch loss: 0.25174716114997864\n",
      "Epoch: 729, batch loss: 0.25174713134765625\n",
      "Epoch: 730, batch loss: 0.25174716114997864\n",
      "Epoch: 731, batch loss: 0.25174713134765625\n",
      "Epoch: 732, batch loss: 0.25174713134765625\n",
      "Epoch: 733, batch loss: 0.25174713134765625\n",
      "Epoch: 734, batch loss: 0.25174710154533386\n",
      "Epoch: 735, batch loss: 0.25174716114997864\n",
      "Epoch: 736, batch loss: 0.25174716114997864\n",
      "Epoch: 737, batch loss: 0.25174716114997864\n",
      "Epoch: 738, batch loss: 0.25174713134765625\n",
      "Epoch: 739, batch loss: 0.25174713134765625\n",
      "Epoch: 740, batch loss: 0.25174713134765625\n",
      "Epoch: 741, batch loss: 0.25174713134765625\n",
      "Epoch: 742, batch loss: 0.25174716114997864\n",
      "Epoch: 743, batch loss: 0.25174710154533386\n",
      "Epoch: 744, batch loss: 0.25174713134765625\n",
      "Epoch: 745, batch loss: 0.25174713134765625\n",
      "Epoch: 746, batch loss: 0.25174713134765625\n",
      "Epoch: 747, batch loss: 0.25174713134765625\n",
      "Epoch: 748, batch loss: 0.25174713134765625\n",
      "Epoch: 749, batch loss: 0.25174716114997864\n",
      "Epoch: 750, batch loss: 0.25174713134765625\n",
      "Epoch: 751, batch loss: 0.25174713134765625\n",
      "Epoch: 752, batch loss: 0.25174713134765625\n",
      "Epoch: 753, batch loss: 0.25174713134765625\n",
      "Epoch: 754, batch loss: 0.25174713134765625\n",
      "Epoch: 755, batch loss: 0.25174713134765625\n",
      "Epoch: 756, batch loss: 0.25174713134765625\n",
      "Epoch: 757, batch loss: 0.25174713134765625\n",
      "Epoch: 758, batch loss: 0.25174713134765625\n",
      "Epoch: 759, batch loss: 0.25174713134765625\n",
      "Epoch: 760, batch loss: 0.25174713134765625\n",
      "Epoch: 761, batch loss: 0.25174713134765625\n",
      "Epoch: 762, batch loss: 0.25174713134765625\n",
      "Epoch: 763, batch loss: 0.25174713134765625\n",
      "Epoch: 764, batch loss: 0.25174713134765625\n",
      "Epoch: 765, batch loss: 0.25174713134765625\n",
      "Epoch: 766, batch loss: 0.25174713134765625\n",
      "Epoch: 767, batch loss: 0.25174713134765625\n",
      "Epoch: 768, batch loss: 0.25174713134765625\n",
      "Epoch: 769, batch loss: 0.25174713134765625\n",
      "Epoch: 770, batch loss: 0.25174713134765625\n",
      "Epoch: 771, batch loss: 0.25174713134765625\n",
      "Epoch: 772, batch loss: 0.25174713134765625\n",
      "Epoch: 773, batch loss: 0.25174713134765625\n",
      "Epoch: 774, batch loss: 0.25174716114997864\n",
      "Epoch: 775, batch loss: 0.25174710154533386\n",
      "Epoch: 776, batch loss: 0.25174713134765625\n",
      "Epoch: 777, batch loss: 0.25174713134765625\n",
      "Epoch: 778, batch loss: 0.25174713134765625\n",
      "Epoch: 779, batch loss: 0.25174716114997864\n",
      "Epoch: 780, batch loss: 0.25174713134765625\n",
      "Epoch: 781, batch loss: 0.25174713134765625\n",
      "Epoch: 782, batch loss: 0.25174713134765625\n",
      "Epoch: 783, batch loss: 0.25174713134765625\n",
      "Epoch: 784, batch loss: 0.25174716114997864\n",
      "Epoch: 785, batch loss: 0.25174716114997864\n",
      "Epoch: 786, batch loss: 0.25174713134765625\n",
      "Epoch: 787, batch loss: 0.25174713134765625\n",
      "Epoch: 788, batch loss: 0.25174716114997864\n",
      "Epoch: 789, batch loss: 0.25174716114997864\n",
      "Epoch: 790, batch loss: 0.25174716114997864\n",
      "Epoch: 791, batch loss: 0.25174713134765625\n",
      "Epoch: 792, batch loss: 0.25174716114997864\n",
      "Epoch: 793, batch loss: 0.25174716114997864\n",
      "Epoch: 794, batch loss: 0.25174716114997864\n",
      "Epoch: 795, batch loss: 0.25174713134765625\n",
      "Epoch: 796, batch loss: 0.25174710154533386\n",
      "Epoch: 797, batch loss: 0.25174713134765625\n",
      "Epoch: 798, batch loss: 0.25174710154533386\n",
      "Epoch: 799, batch loss: 0.25174710154533386\n",
      "Epoch: 800, batch loss: 0.25174710154533386\n",
      "Epoch: 801, batch loss: 0.25174710154533386\n",
      "Epoch: 802, batch loss: 0.25174716114997864\n",
      "Epoch: 803, batch loss: 0.25174713134765625\n",
      "Epoch: 804, batch loss: 0.25174713134765625\n",
      "Epoch: 805, batch loss: 0.25174713134765625\n",
      "Epoch: 806, batch loss: 0.25174716114997864\n",
      "Epoch: 807, batch loss: 0.25174716114997864\n",
      "Epoch: 808, batch loss: 0.25174710154533386\n",
      "Epoch: 809, batch loss: 0.25174713134765625\n",
      "Epoch: 810, batch loss: 0.25174716114997864\n",
      "Epoch: 811, batch loss: 0.25174713134765625\n",
      "Epoch: 812, batch loss: 0.25174710154533386\n",
      "Epoch: 813, batch loss: 0.25174710154533386\n",
      "Epoch: 814, batch loss: 0.25174710154533386\n",
      "Epoch: 815, batch loss: 0.25174710154533386\n",
      "Epoch: 816, batch loss: 0.25174713134765625\n",
      "Epoch: 817, batch loss: 0.25174716114997864\n",
      "Epoch: 818, batch loss: 0.25174713134765625\n",
      "Epoch: 819, batch loss: 0.25174713134765625\n",
      "Epoch: 820, batch loss: 0.25174713134765625\n",
      "Epoch: 821, batch loss: 0.25174716114997864\n",
      "Epoch: 822, batch loss: 0.25174713134765625\n",
      "Epoch: 823, batch loss: 0.25174713134765625\n",
      "Epoch: 824, batch loss: 0.25174716114997864\n",
      "Epoch: 825, batch loss: 0.25174716114997864\n",
      "Epoch: 826, batch loss: 0.25174713134765625\n",
      "Epoch: 827, batch loss: 0.25174713134765625\n",
      "Epoch: 828, batch loss: 0.25174713134765625\n",
      "Epoch: 829, batch loss: 0.25174713134765625\n",
      "Epoch: 830, batch loss: 0.25174713134765625\n",
      "Epoch: 831, batch loss: 0.25174713134765625\n",
      "Epoch: 832, batch loss: 0.25174713134765625\n",
      "Epoch: 833, batch loss: 0.25174716114997864\n",
      "Epoch: 834, batch loss: 0.25174716114997864\n",
      "Epoch: 835, batch loss: 0.25174716114997864\n",
      "Epoch: 836, batch loss: 0.25174713134765625\n",
      "Epoch: 837, batch loss: 0.25174713134765625\n",
      "Epoch: 838, batch loss: 0.25174713134765625\n",
      "Epoch: 839, batch loss: 0.25174713134765625\n",
      "Epoch: 840, batch loss: 0.25174713134765625\n",
      "Epoch: 841, batch loss: 0.25174713134765625\n",
      "Epoch: 842, batch loss: 0.25174713134765625\n",
      "Epoch: 843, batch loss: 0.25174716114997864\n",
      "Epoch: 844, batch loss: 0.25174713134765625\n",
      "Epoch: 845, batch loss: 0.25174713134765625\n",
      "Epoch: 846, batch loss: 0.25174716114997864\n",
      "Epoch: 847, batch loss: 0.25174716114997864\n",
      "Epoch: 848, batch loss: 0.25174716114997864\n",
      "Epoch: 849, batch loss: 0.25174716114997864\n",
      "Epoch: 850, batch loss: 0.25174710154533386\n",
      "Epoch: 851, batch loss: 0.25174713134765625\n",
      "Epoch: 852, batch loss: 0.25174716114997864\n",
      "Epoch: 853, batch loss: 0.25174713134765625\n",
      "Epoch: 854, batch loss: 0.25174716114997864\n",
      "Epoch: 855, batch loss: 0.25174713134765625\n",
      "Epoch: 856, batch loss: 0.25174713134765625\n",
      "Epoch: 857, batch loss: 0.25174713134765625\n",
      "Epoch: 858, batch loss: 0.25174713134765625\n",
      "Epoch: 859, batch loss: 0.25174716114997864\n",
      "Epoch: 860, batch loss: 0.25174716114997864\n",
      "Epoch: 861, batch loss: 0.25174713134765625\n",
      "Epoch: 862, batch loss: 0.25174716114997864\n",
      "Epoch: 863, batch loss: 0.25174716114997864\n",
      "Epoch: 864, batch loss: 0.25174713134765625\n",
      "Epoch: 865, batch loss: 0.25174713134765625\n",
      "Epoch: 866, batch loss: 0.25174713134765625\n",
      "Epoch: 867, batch loss: 0.25174713134765625\n",
      "Epoch: 868, batch loss: 0.25174713134765625\n",
      "Epoch: 869, batch loss: 0.25174716114997864\n",
      "Epoch: 870, batch loss: 0.25174713134765625\n",
      "Epoch: 871, batch loss: 0.25174713134765625\n",
      "Epoch: 872, batch loss: 0.25174716114997864\n",
      "Epoch: 873, batch loss: 0.25174713134765625\n",
      "Epoch: 874, batch loss: 0.25174713134765625\n",
      "Epoch: 875, batch loss: 0.25174713134765625\n",
      "Epoch: 876, batch loss: 0.25174716114997864\n",
      "Epoch: 877, batch loss: 0.25174713134765625\n",
      "Epoch: 878, batch loss: 0.25174713134765625\n",
      "Epoch: 879, batch loss: 0.25174713134765625\n",
      "Epoch: 880, batch loss: 0.25174713134765625\n",
      "Epoch: 881, batch loss: 0.25174713134765625\n",
      "Epoch: 882, batch loss: 0.25174713134765625\n",
      "Epoch: 883, batch loss: 0.25174713134765625\n",
      "Epoch: 884, batch loss: 0.25174713134765625\n",
      "Epoch: 885, batch loss: 0.25174713134765625\n",
      "Epoch: 886, batch loss: 0.25174713134765625\n",
      "Epoch: 887, batch loss: 0.25174716114997864\n",
      "Epoch: 888, batch loss: 0.25174713134765625\n",
      "Epoch: 889, batch loss: 0.25174713134765625\n",
      "Epoch: 890, batch loss: 0.25174713134765625\n",
      "Epoch: 891, batch loss: 0.25174716114997864\n",
      "Epoch: 892, batch loss: 0.25174713134765625\n",
      "Epoch: 893, batch loss: 0.25174713134765625\n",
      "Epoch: 894, batch loss: 0.25174713134765625\n",
      "Epoch: 895, batch loss: 0.25174713134765625\n",
      "Epoch: 896, batch loss: 0.25174713134765625\n",
      "Epoch: 897, batch loss: 0.25174713134765625\n",
      "Epoch: 898, batch loss: 0.25174713134765625\n",
      "Epoch: 899, batch loss: 0.25174713134765625\n",
      "Epoch: 900, batch loss: 0.25174713134765625\n",
      "Epoch: 901, batch loss: 0.25174713134765625\n",
      "Epoch: 902, batch loss: 0.25174716114997864\n",
      "Epoch: 903, batch loss: 0.25174713134765625\n",
      "Epoch: 904, batch loss: 0.25174713134765625\n",
      "Epoch: 905, batch loss: 0.25174716114997864\n",
      "Epoch: 906, batch loss: 0.25174716114997864\n",
      "Epoch: 907, batch loss: 0.25174716114997864\n",
      "Epoch: 908, batch loss: 0.25174713134765625\n",
      "Epoch: 909, batch loss: 0.25174716114997864\n",
      "Epoch: 910, batch loss: 0.25174716114997864\n",
      "Epoch: 911, batch loss: 0.25174713134765625\n",
      "Epoch: 912, batch loss: 0.25174716114997864\n",
      "Epoch: 913, batch loss: 0.25174713134765625\n",
      "Epoch: 914, batch loss: 0.25174716114997864\n",
      "Epoch: 915, batch loss: 0.25174713134765625\n",
      "Epoch: 916, batch loss: 0.25174716114997864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 917, batch loss: 0.25174713134765625\n",
      "Epoch: 918, batch loss: 0.25174713134765625\n",
      "Epoch: 919, batch loss: 0.25174716114997864\n",
      "Epoch: 920, batch loss: 0.25174713134765625\n",
      "Epoch: 921, batch loss: 0.25174716114997864\n",
      "Epoch: 922, batch loss: 0.25174713134765625\n",
      "Epoch: 923, batch loss: 0.25174716114997864\n",
      "Epoch: 924, batch loss: 0.25174716114997864\n",
      "Epoch: 925, batch loss: 0.25174716114997864\n",
      "Epoch: 926, batch loss: 0.25174713134765625\n",
      "Epoch: 927, batch loss: 0.25174716114997864\n",
      "Epoch: 928, batch loss: 0.25174716114997864\n",
      "Epoch: 929, batch loss: 0.25174713134765625\n",
      "Epoch: 930, batch loss: 0.25174713134765625\n",
      "Epoch: 931, batch loss: 0.25174713134765625\n",
      "Epoch: 932, batch loss: 0.25174716114997864\n",
      "Epoch: 933, batch loss: 0.25174713134765625\n",
      "Epoch: 934, batch loss: 0.25174713134765625\n",
      "Epoch: 935, batch loss: 0.25174713134765625\n",
      "Epoch: 936, batch loss: 0.25174713134765625\n",
      "Epoch: 937, batch loss: 0.25174713134765625\n",
      "Epoch: 938, batch loss: 0.25174713134765625\n",
      "Epoch: 939, batch loss: 0.25174713134765625\n",
      "Epoch: 940, batch loss: 0.25174710154533386\n",
      "Epoch: 941, batch loss: 0.25174713134765625\n",
      "Epoch: 942, batch loss: 0.25174713134765625\n",
      "Epoch: 943, batch loss: 0.25174713134765625\n",
      "Epoch: 944, batch loss: 0.25174710154533386\n",
      "Epoch: 945, batch loss: 0.25174710154533386\n",
      "Epoch: 946, batch loss: 0.25174713134765625\n",
      "Epoch: 947, batch loss: 0.25174713134765625\n",
      "Epoch: 948, batch loss: 0.25174713134765625\n",
      "Epoch: 949, batch loss: 0.25174710154533386\n",
      "Epoch: 950, batch loss: 0.25174713134765625\n",
      "Epoch: 951, batch loss: 0.25174713134765625\n",
      "Epoch: 952, batch loss: 0.25174710154533386\n",
      "Epoch: 953, batch loss: 0.25174713134765625\n",
      "Epoch: 954, batch loss: 0.25174710154533386\n",
      "Epoch: 955, batch loss: 0.25174713134765625\n",
      "Epoch: 956, batch loss: 0.25174713134765625\n",
      "Epoch: 957, batch loss: 0.25174713134765625\n",
      "Epoch: 958, batch loss: 0.25174713134765625\n",
      "Epoch: 959, batch loss: 0.25174713134765625\n",
      "Epoch: 960, batch loss: 0.25174713134765625\n",
      "Epoch: 961, batch loss: 0.25174713134765625\n",
      "Epoch: 962, batch loss: 0.25174713134765625\n",
      "Epoch: 963, batch loss: 0.25174716114997864\n",
      "Epoch: 964, batch loss: 0.25174716114997864\n",
      "Epoch: 965, batch loss: 0.25174710154533386\n",
      "Epoch: 966, batch loss: 0.25174713134765625\n",
      "Epoch: 967, batch loss: 0.25174710154533386\n",
      "Epoch: 968, batch loss: 0.25174713134765625\n",
      "Epoch: 969, batch loss: 0.25174713134765625\n",
      "Epoch: 970, batch loss: 0.25174716114997864\n",
      "Epoch: 971, batch loss: 0.25174713134765625\n",
      "Epoch: 972, batch loss: 0.25174713134765625\n",
      "Epoch: 973, batch loss: 0.25174713134765625\n",
      "Epoch: 974, batch loss: 0.25174716114997864\n",
      "Epoch: 975, batch loss: 0.25174713134765625\n",
      "Epoch: 976, batch loss: 0.25174713134765625\n",
      "Epoch: 977, batch loss: 0.25174716114997864\n",
      "Epoch: 978, batch loss: 0.25174713134765625\n",
      "Epoch: 979, batch loss: 0.25174716114997864\n",
      "Epoch: 980, batch loss: 0.25174716114997864\n",
      "Epoch: 981, batch loss: 0.25174716114997864\n",
      "Epoch: 982, batch loss: 0.25174716114997864\n",
      "Epoch: 983, batch loss: 0.25174713134765625\n",
      "Epoch: 984, batch loss: 0.25174716114997864\n",
      "Epoch: 985, batch loss: 0.25174713134765625\n",
      "Epoch: 986, batch loss: 0.25174716114997864\n",
      "Epoch: 987, batch loss: 0.25174716114997864\n",
      "Epoch: 988, batch loss: 0.25174716114997864\n",
      "Epoch: 989, batch loss: 0.25174716114997864\n",
      "Epoch: 990, batch loss: 0.25174713134765625\n",
      "Epoch: 991, batch loss: 0.25174713134765625\n",
      "Epoch: 992, batch loss: 0.25174713134765625\n",
      "Epoch: 993, batch loss: 0.25174713134765625\n",
      "Epoch: 994, batch loss: 0.25174713134765625\n",
      "Epoch: 995, batch loss: 0.25174713134765625\n",
      "Epoch: 996, batch loss: 0.25174713134765625\n",
      "Epoch: 997, batch loss: 0.25174716114997864\n",
      "Epoch: 998, batch loss: 0.25174716114997864\n",
      "Epoch: 999, batch loss: 0.25174713134765625\n",
      "Epoch: 1000, batch loss: 0.25174713134765625\n"
     ]
    }
   ],
   "source": [
    "# Batch gradient descent\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Preprocess the training data\n",
    "    data_batch = data_train.values\n",
    "    batch_input = torch.from_numpy(data_batch)\n",
    "    # Perform forward pass\n",
    "    batch_output = autoencoder(batch_input.float())\n",
    "    loss = criterion(batch_output, batch_input.float())\n",
    "    losses.append(loss)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: {}, batch loss: {}\".format(\n",
    "        epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val RMSE and correlation: 0.03093569166958332 (0.9842515965397906, 0.0)\n"
     ]
    }
   ],
   "source": [
    "val_data = torch.from_numpy(data_train.values)\n",
    "val_preds = autoencoder(val_data.float())\n",
    "\n",
    "val_loss = criterion(val_preds, val_data.float())\n",
    "val_corr = pearsonr(val_data.flatten(), val_preds.detach().numpy().flatten())\n",
    "print(\"Val RMSE and correlation:\", val_loss.item(), val_corr)\n",
    "\n",
    "rmses_per_drug = []\n",
    "corrs_per_drug = []\n",
    "for d in range(val_data.shape[0]):\n",
    "    original_input = val_data[d].numpy()\n",
    "    predicted_input = val_preds[d].detach().numpy()\n",
    "    rmses_per_drug.append(metrics.mean_squared_error(original_input, predicted_input) ** 0.5)\n",
    "    corrs_per_drug.append(pearsonr(original_input, predicted_input)[0])\n",
    "    \n",
    "results_per_drug_df = pd.DataFrame()\n",
    "results_per_drug_df[\"Drug ID\"] = drug_data_df.index\n",
    "results_per_drug_df[\"Training RMSE\"] = rmses_per_drug\n",
    "results_per_drug_df[\"Training correlation\"] = corrs_per_drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 3)\n"
     ]
    }
   ],
   "source": [
    "print(results_per_drug_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821717378646001"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_per_drug_df[\"Training correlation\"].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2737, -3.0208, -1.6157, -2.5957, -0.2376, -0.3696, -1.2811, -0.3108,\n",
       "         0.5440, -1.5509], dtype=torch.float64)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0160, -2.9993, -1.4550, -2.6885, -0.2197, -0.0436, -1.6678, -0.4368,\n",
       "         0.7624, -1.4990], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds[10, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check deep autoencoder alone on cell line data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate autoencoder into main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kinases Dataset + Remaning GDSC drug's putative targets <class 'modeling.Dataset'>\n",
      "\n",
      "Dataset containing 74 common drugs of GDSC and HMS LINCS Kinome scan dataset.\n",
      "Cell lines data types: expression, coding variant and tissue type. Expressions and coding variants are \n",
      "present only for proteins present in both GDSC and KINOMEscan data, resulting in expression of 188 genes and\n",
      "nmutations in 18 genes.\n",
      "In addition, expressions and mutations (17 new features) of remaining target genes from GDSC are included\n",
      "Tissue types are dummy encoded GDSC Tissue Descriptions 1 (18 features).\n",
      "Drugs representation: inhibition scores (% control) of 294 proteins. Set of proteins is the intersection of \n",
      "proteins screened for each of 74 drugs.\n",
      "Drug response data: drug reponse data contains AUC metrics across cell lines for 74 drugs considered.\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../../Data/Preprocessed Datasets/\"\n",
    "with open(filepath + \"GDSC-KINOMEscan_proteins_intersection_+_remaining_GDSC_target_genes_dataset.pkl\", \"rb\") as f:\n",
    "    full_dataset = dill.load(f)\n",
    "print(full_dataset.name, type(full_dataset))\n",
    "print()\n",
    "print(full_dataset.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(922, 241) (74, 294) (52730, 3)\n"
     ]
    }
   ],
   "source": [
    "# Establish response data for samples (drug-cell line pairs)\n",
    "response_df = full_dataset.response_data.copy()\n",
    "\n",
    "# Establish cell line features data\n",
    "cell_line_data_original_df = full_dataset.full_cell_lines_data.copy()\n",
    "\n",
    "# Search for cell lines present in response data, but missing the genomic features\n",
    "missing_cell_lines = []\n",
    "for cosmic_id in response_df.COSMIC_ID.unique():\n",
    "    if cosmic_id not in cell_line_data_original_df.cell_line_id.unique():\n",
    "        missing_cell_lines.append(cosmic_id)\n",
    "# Put cell line IDs into index and drop cell line IDs columns\n",
    "cell_line_data_original_df.index = cell_line_data_original_df.cell_line_id\n",
    "cell_line_data_original_df = cell_line_data_original_df.drop(\"cell_line_id\", axis=1)\n",
    "\n",
    "# Extract response only for cell lines for which features are present\n",
    "response_df = response_df[~response_df.COSMIC_ID.isin(missing_cell_lines)]\n",
    "\n",
    "# Establish drug features data\n",
    "drug_data_original_df = full_dataset.drugs_data.copy()\n",
    "\n",
    "# Convert drug index from LINCS name to GDSC drug ID\n",
    "drug_data_original_df.index = drug_data_original_df.index.map(full_dataset.kinomescan_name_to_gdsc_id_mapper)\n",
    "print(cell_line_data_original_df.shape, drug_data_original_df.shape, response_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, name, network):\n",
    "        self.name = name\n",
    "        self.network = network\n",
    "        \n",
    "    def train(self, train_samples, cell_line_features, drug_features,\n",
    "             batch_size, optimizer, criterion, reg_lambda=0, log=True):\n",
    "        \"\"\"Perform training process by looping over training set in batches (one epoch) of the\n",
    "        training.\"\"\"\n",
    "        no_batches = train_samples.shape[0] // batch_size + 1\n",
    "        \n",
    "        # Training the model\n",
    "        self.network.train()\n",
    "        for batch in range(no_batches):\n",
    "            # Separate response variable batch\n",
    "            if batch != no_batches:\n",
    "                samples_batch = train_samples.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
    "            else:\n",
    "                samples_batch = train_samples.iloc[batch * batch_size:]\n",
    "\n",
    "            # Extract output variable batch\n",
    "            y_batch = torch.from_numpy(samples_batch[\"AUC\"].values).view(-1, 1)\n",
    "\n",
    "            # Extract cell lines IDs for which data shall be extracted\n",
    "            cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
    "            # Extract corresponding cell line data\n",
    "            cell_line_input_batch = cell_line_features.loc[cl_ids].values\n",
    "            cell_line_input_batch = torch.from_numpy(cell_line_input_batch)\n",
    "\n",
    "            # Extract drug IDs for which data shall be extracted\n",
    "            drug_ids = samples_batch[\"DRUG_ID\"].values\n",
    "            # Extract corresponding drug data\n",
    "            drug_input_batch = drug_features.loc[drug_ids].values\n",
    "            drug_input_batch = torch.from_numpy(drug_input_batch)\n",
    "\n",
    "            # Clear gradient buffers because we don't want to accummulate gradients \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            batch_output = self.network(drug_input_batch.float(), cell_line_input_batch.float())\n",
    "\n",
    "            reg_sum = 0\n",
    "            for param in self.network.parameters():\n",
    "                reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
    "\n",
    "            # Compute the loss for this batch\n",
    "            loss = criterion(batch_output, y_batch.float()) + reg_lambda * reg_sum\n",
    "            # Get the gradients w.r.t. the parameters\n",
    "            loss.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, samples, cell_line_features, drug_features):\n",
    "        \"\"\"Predict response on a given set of samples\"\"\"\n",
    "        y_true = samples[\"AUC\"].values\n",
    "\n",
    "        cl_input = cell_line_features.loc[samples[\"COSMIC_ID\"].values].values\n",
    "        drug_input = drug_features.loc[samples[\"DRUG_ID\"].values].values\n",
    "\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted = self.network(torch.from_numpy(drug_input).float(), \n",
    "                             torch.from_numpy(cl_input).float())\n",
    "        return predicted, y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def per_drug_performance_df(samples, predicted, mean_training_auc=None):\n",
    "        \"\"\"Compute evaluation metrics per drug and return them in a DataFrame\"\"\"\n",
    "        sample_with_predictions = samples.copy()\n",
    "        sample_with_predictions[\"Predicted AUC\"] = predicted.numpy()\n",
    "\n",
    "        drugs = []\n",
    "        model_corrs = []\n",
    "        model_rmses = []\n",
    "        dummy_corrs = []\n",
    "        dummy_rmses = []\n",
    "        no_samples = []\n",
    "\n",
    "        for drug in sample_with_predictions.DRUG_ID.unique():\n",
    "            df = sample_with_predictions[sample_with_predictions.DRUG_ID == drug]\n",
    "            if df.shape[0] < 2:\n",
    "                continue\n",
    "            if mean_training_auc:\n",
    "                dummy_preds = [mean_training_auc] * df.shape[0]\n",
    "            else:\n",
    "                dummy_preds = [df[\"AUC\"].mean()] * df.shape[0]\n",
    "            dummy_rmse = metrics.mean_squared_error(df[\"AUC\"], dummy_preds) ** 0.5\n",
    "            dummy_corr = pearsonr(df[\"AUC\"], dummy_preds)\n",
    "\n",
    "            try:\n",
    "                model_rmse = metrics.mean_squared_error(df[\"AUC\"], df[\"Predicted AUC\"]) ** 0.5\n",
    "                model_corr = pearsonr(df[\"AUC\"], df[\"Predicted AUC\"])\n",
    "            except ValueError:\n",
    "                model_rmse, model_corr = np.nan, (np.nan, np.nan)\n",
    "\n",
    "            drugs.append(drug)\n",
    "            dummy_rmses.append(dummy_rmse)\n",
    "            dummy_corrs.append(dummy_corr[0])\n",
    "\n",
    "            model_rmses.append(model_rmse)\n",
    "            model_corrs.append(model_corr[0])\n",
    "\n",
    "            no_samples.append(df.COSMIC_ID.nunique())\n",
    "\n",
    "        performance_per_drug = pd.DataFrame()\n",
    "        performance_per_drug[\"Drug ID\"] = drugs\n",
    "        performance_per_drug[\"Model RMSE\"] = model_rmses\n",
    "        performance_per_drug[\"Model correlation\"] = model_corrs\n",
    "\n",
    "        performance_per_drug[\"Dummy RMSE\"] = dummy_rmses\n",
    "        performance_per_drug[\"Dummy correlation\"] = dummy_corrs\n",
    "        performance_per_drug[\"No. samples\"] = no_samples\n",
    "\n",
    "        return performance_per_drug\n",
    "        \n",
    "    @staticmethod\n",
    "    def evaluate_predictions(y_true, preds):\n",
    "        \"\"\"Compute RMSE and correlation with true values for model predictions\"\"\"\n",
    "        return metrics.mean_squared_error(y_true, preds) ** 0.5, pearsonr(y_true, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "class LinearMatrixFactorizationWithFeatures(torch.nn.Module):\n",
    "    def __init__(self, drug_input_dim, cell_line_input_dim, output_dim, \n",
    "                 out_activation_func=None,\n",
    "                 drug_bias=True,\n",
    "                 cell_line_bias=True):\n",
    "        super(LinearMatrixFactorizationWithFeatures, self).__init__()\n",
    "        self.drug_linear = torch.nn.Linear(drug_input_dim, output_dim, bias=drug_bias)\n",
    "        self.cell_line_linear = torch.nn.Linear(cell_line_input_dim, output_dim, bias=cell_line_bias)\n",
    "        self.out_activation = out_activation_func\n",
    "        \n",
    "    def forward(self, drug_features, cell_line_features):\n",
    "        drug_outputs = self.drug_linear(drug_features)\n",
    "        cell_line_outputs = self.cell_line_linear(cell_line_features)\n",
    "        \n",
    "        final_outputs = torch.sum(torch.mul(drug_outputs, cell_line_outputs), dim=1).view(-1, 1)\n",
    "        if self.out_activation:\n",
    "            return self.out_activation(final_outputs)\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/val/test sets\n",
    "num_val_cell_lines = 100\n",
    "num_test_cell_lines = 100\n",
    "split_seed = 11\n",
    "samples_train, samples_val, samples_test, cell_lines_test, cell_lines_val = Dataset.samples_train_test_split(\n",
    "                                                                        response_df,\n",
    "                                                                        num_val_cell_lines,\n",
    "                                                                        num_test_cell_lines,\n",
    "                                                                        split_seed,\n",
    "                                                                        shuffle=True)\n",
    "# Normalize the data\n",
    "# Cell line data\n",
    "cols_subset = [col for col in list(cell_line_data_original_df) if col.endswith(\"_exp\")]\n",
    "rows_subset = [x for x in cell_line_data_original_df.index if x not in cell_lines_test + cell_lines_val]\n",
    "\n",
    "cell_line_data_df = Dataset.standardize_data(cell_line_data_original_df, cols_subset=cols_subset,\n",
    "                                            rows_subset=rows_subset)\n",
    "# Drug data\n",
    "drug_data_df = Dataset.standardize_data(drug_data_original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantianate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "drug_dim, cell_line_dim = drug_data_df.shape[1], cell_line_data_df.shape[1]\n",
    "drug_bias, cell_line_bias = True, True\n",
    "out_activation_func = torch.sigmoid\n",
    "hidden_dim = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = LinearMatrixFactorizationWithFeatures(drug_dim, cell_line_dim, hidden_dim,\n",
    "                                                 drug_bias=drug_bias,\n",
    "                                                 cell_line_bias=drug_bias,\n",
    "                                                 out_activation_func=out_activation_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify training parameters\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "no_batches = samples_train.shape[0] // batch_size + 1\n",
    "reg_lambda = 1e-5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    network.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch loss: 0.02867189608514309, val_loss: 0.1880162827121493, val_corr: 0.6301681074304405\n",
      "Epoch: 2, batch loss: 0.01765073649585247, val_loss: 0.1319277847301788, val_corr: 0.6895879230655876\n",
      "Epoch: 3, batch loss: 0.019665133208036423, val_loss: 0.13272284354148725, val_corr: 0.682123831877956\n",
      "Epoch: 4, batch loss: 0.01923956535756588, val_loss: 0.12984282335650055, val_corr: 0.6973952805015904\n",
      "Epoch: 5, batch loss: 0.01693965494632721, val_loss: 0.12740322727643005, val_corr: 0.7099484253363766\n",
      "Epoch: 6, batch loss: 0.014819617383182049, val_loss: 0.12616778199531395, val_corr: 0.7168634422249307\n",
      "Epoch: 7, batch loss: 0.01356389932334423, val_loss: 0.12559166710524233, val_corr: 0.7207460785752255\n",
      "Epoch: 8, batch loss: 0.012879186309874058, val_loss: 0.125116283070562, val_corr: 0.7241535722294504\n",
      "Epoch: 9, batch loss: 0.01247566007077694, val_loss: 0.12454829465312152, val_corr: 0.7278920286016871\n",
      "Epoch: 10, batch loss: 0.012217331677675247, val_loss: 0.12391884623742921, val_corr: 0.7317445552934134\n",
      "Epoch: 11, batch loss: 0.012040075846016407, val_loss: 0.123294621841978, val_corr: 0.7353814217384677\n",
      "Epoch: 12, batch loss: 0.011909665539860725, val_loss: 0.12273785419613491, val_corr: 0.7385218336386639\n",
      "Epoch: 13, batch loss: 0.011808853596448898, val_loss: 0.1222853120327139, val_corr: 0.741015425365969\n",
      "Epoch: 14, batch loss: 0.011731881648302078, val_loss: 0.12191846399233369, val_corr: 0.7429878744336419\n",
      "Epoch: 15, batch loss: 0.011676903814077377, val_loss: 0.12158679526940228, val_corr: 0.7447240226318362\n",
      "Epoch: 16, batch loss: 0.011640387587249279, val_loss: 0.12125713201547211, val_corr: 0.7464137467956611\n",
      "Epoch: 17, batch loss: 0.011617542244493961, val_loss: 0.12092249174648906, val_corr: 0.748106021144017\n",
      "Epoch: 18, batch loss: 0.01160391978919506, val_loss: 0.12058701961587212, val_corr: 0.7497869836847192\n",
      "Epoch: 19, batch loss: 0.011595621705055237, val_loss: 0.12025637829655265, val_corr: 0.7514298733538679\n",
      "Epoch: 20, batch loss: 0.011589203029870987, val_loss: 0.11993454340329823, val_corr: 0.7530122486046609\n",
      "Epoch: 21, batch loss: 0.011582222767174244, val_loss: 0.11962322294656032, val_corr: 0.754521907002397\n",
      "Epoch: 22, batch loss: 0.011573735624551773, val_loss: 0.11932183991532, val_corr: 0.7559604327289172\n",
      "Epoch: 23, batch loss: 0.011564156040549278, val_loss: 0.11902848216691465, val_corr: 0.7573393251783799\n",
      "Epoch: 24, batch loss: 0.01155449915677309, val_loss: 0.11874144458964297, val_corr: 0.7586714002939035\n",
      "Epoch: 25, batch loss: 0.011545687913894653, val_loss: 0.1184600351433746, val_corr: 0.759964539869348\n",
      "Epoch: 26, batch loss: 0.011538268998265266, val_loss: 0.1181846105553567, val_corr: 0.761220598589709\n",
      "Epoch: 27, batch loss: 0.011532464995980263, val_loss: 0.11791635968695754, val_corr: 0.7624365306711957\n",
      "Epoch: 28, batch loss: 0.011528290808200836, val_loss: 0.11765658786445471, val_corr: 0.7636078990278458\n",
      "Epoch: 29, batch loss: 0.011525657027959824, val_loss: 0.11740645343845364, val_corr: 0.7647306307379985\n",
      "Epoch: 30, batch loss: 0.011524445377290249, val_loss: 0.1171667938634556, val_corr: 0.7658018012780581\n",
      "Epoch: 31, batch loss: 0.01152453850954771, val_loss: 0.11693815418886304, val_corr: 0.7668197576748192\n",
      "Epoch: 32, batch loss: 0.011525820940732956, val_loss: 0.1167208360221753, val_corr: 0.7677838461610909\n",
      "Epoch: 33, batch loss: 0.011528190225362778, val_loss: 0.11651496525556629, val_corr: 0.7686941875282681\n",
      "Epoch: 34, batch loss: 0.011531547643244267, val_loss: 0.11632048850449152, val_corr: 0.7695516075668741\n",
      "Epoch: 35, batch loss: 0.011535807512700558, val_loss: 0.11613733879419469, val_corr: 0.7703570563799506\n",
      "Epoch: 36, batch loss: 0.011540893465280533, val_loss: 0.11596527738798819, val_corr: 0.7711121224517591\n",
      "Epoch: 37, batch loss: 0.011546727269887924, val_loss: 0.11580405153331992, val_corr: 0.7718184696776362\n",
      "Epoch: 38, batch loss: 0.011553232558071613, val_loss: 0.11565327567592491, val_corr: 0.772478302738286\n",
      "Epoch: 39, batch loss: 0.011560334824025631, val_loss: 0.11551260125104944, val_corr: 0.7730936142664698\n",
      "Epoch: 40, batch loss: 0.011567977257072926, val_loss: 0.11538149093174693, val_corr: 0.7736671166348006\n",
      "Epoch: 41, batch loss: 0.011576056480407715, val_loss: 0.11525933411670213, val_corr: 0.7742017898770909\n",
      "Epoch: 42, batch loss: 0.011584528721868992, val_loss: 0.1151454816569876, val_corr: 0.7747005693957687\n",
      "Epoch: 43, batch loss: 0.011593305505812168, val_loss: 0.11503926701682875, val_corr: 0.7751666106674464\n",
      "Epoch: 44, batch loss: 0.011602356098592281, val_loss: 0.1149400227097302, val_corr: 0.775602692011609\n",
      "Epoch: 45, batch loss: 0.01161157712340355, val_loss: 0.11484710008025967, val_corr: 0.7760118159473989\n",
      "Epoch: 46, batch loss: 0.011620995588600636, val_loss: 0.11476003267791808, val_corr: 0.7763956455380394\n",
      "Epoch: 47, batch loss: 0.011630483902990818, val_loss: 0.11467817898050806, val_corr: 0.776757427820477\n",
      "Epoch: 48, batch loss: 0.011640145443379879, val_loss: 0.11460149351924503, val_corr: 0.7770965036984636\n",
      "Epoch: 49, batch loss: 0.01164980698376894, val_loss: 0.11452905985823943, val_corr: 0.7774181892692101\n",
      "Epoch: 50, batch loss: 0.011659687384963036, val_loss: 0.11446167848127453, val_corr: 0.7777168460631183\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch gradient descent\n",
    "network.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(no_batches):\n",
    "        # Separate response variable batch\n",
    "        if batch != no_batches:\n",
    "            samples_batch = samples_train.iloc[batch * batch_size:(batch + 1) * batch_size]\n",
    "        else:\n",
    "            samples_batch = samples_train.iloc[batch * batch_size:]\n",
    "\n",
    "        # Extract output variable batch\n",
    "        y_batch = torch.from_numpy(samples_batch[\"AUC\"].values).view(-1, 1)\n",
    "\n",
    "        # Extract cell lines IDs for which data shall be extracted\n",
    "        cl_ids = samples_batch[\"COSMIC_ID\"].values\n",
    "        # Extract corresponding cell line data\n",
    "        cell_line_input_batch = cell_line_data_df.loc[cl_ids].values\n",
    "        cell_line_input_batch = torch.from_numpy(cell_line_input_batch)\n",
    "\n",
    "        # Extract drug IDs for which data shall be extracted\n",
    "        drug_ids = samples_batch[\"DRUG_ID\"].values\n",
    "        # Extract corresponding drug data\n",
    "        drug_input_batch = drug_data_df.loc[drug_ids].values\n",
    "        drug_input_batch = torch.from_numpy(drug_input_batch)\n",
    "\n",
    "        # Clear gradient buffers because we don't want to accummulate gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        batch_output = network(drug_input_batch.float(), cell_line_input_batch.float())\n",
    "\n",
    "        reg_sum = 0\n",
    "        for param in network.parameters():\n",
    "            reg_sum += 0.5 * (param ** 2).sum()  # L2 norm\n",
    "\n",
    "        # Compute the loss for this batch\n",
    "        loss = criterion(batch_output, y_batch.float()) + reg_lambda * reg_sum\n",
    "        # Get the gradients w.r.t. the parameters\n",
    "        loss.backward()\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Evaluate on validation set\n",
    "    validation_pairs = samples_train.iloc[:100]\n",
    "    validation_true_responses = torch.from_numpy(validation_pairs[\"AUC\"].values).view(-1, 1)\n",
    "    # Extract cell lines IDs for which data shall be extracted\n",
    "    cl_ids = validation_pairs[\"COSMIC_ID\"].values\n",
    "    # Extract corresponding cell line data\n",
    "    cell_line_input_validation = cell_line_data_df.loc[cl_ids].values\n",
    "    cell_line_input_validation = torch.from_numpy(cell_line_input_validation)\n",
    "    # Extract drug IDs for which data shall be extracted\n",
    "    drug_ids = validation_pairs[\"DRUG_ID\"].values\n",
    "    # Extract corresponding drug data\n",
    "    drug_input_validation = drug_data_df.loc[drug_ids].values\n",
    "    drug_input_validation = torch.from_numpy(drug_input_validation)\n",
    "    \n",
    "    network.eval()\n",
    "    validation_output = network(drug_input_validation.float(), cell_line_input_validation.float())\n",
    "    \n",
    "    val_rmse, val_corr = Model.evaluate_predictions(validation_true_responses.numpy().flatten(), \n",
    "                                                    validation_output.detach().numpy().flatten())\n",
    "    \n",
    "    print(\"Epoch: {}, batch loss: {}, val_loss: {}, val_corr: {}\".format(\n",
    "        epoch + 1, loss.item(), val_rmse, val_corr[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model's definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear system\n",
    "class LinearMatrixFactorizationWithFeatures(torch.nn.Module):\n",
    "    def __init__(self, drug_input_dim, cell_line_input_dim, output_dim, \n",
    "                 out_activation_func=None,\n",
    "                 drug_bias=True,\n",
    "                 cell_line_bias=True):\n",
    "        super(LinearMatrixFactorizationWithFeatures, self).__init__()\n",
    "        self.drug_linear = torch.nn.Linear(drug_input_dim, output_dim, bias=drug_bias)\n",
    "        self.cell_line_linear = torch.nn.Linear(cell_line_input_dim, output_dim, bias=cell_line_bias)\n",
    "        self.out_activation = out_activation_func\n",
    "        \n",
    "    def forward(self, drug_features, cell_line_features):\n",
    "        drug_outputs = self.drug_linear(drug_features)\n",
    "        cell_line_outputs = self.cell_line_linear(cell_line_features)\n",
    "        \n",
    "        final_outputs = torch.sum(torch.mul(drug_outputs, cell_line_outputs), dim=1).view(-1, 1)\n",
    "        if self.out_activation:\n",
    "            return self.out_activation(final_outputs)\n",
    "        return final_outputs\n",
    "\n",
    "class RecSystemWithAutoencoders(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 drug_autoencoder,\n",
    "                 cell_line_autoencoder,\n",
    "                 out_activation=None):\n",
    "        \n",
    "        super(RecSystemWithAutoencoders, self).__init__()\n",
    "        self.drug_autoencoder = drug_autoencoder\n",
    "        self.cell_line_autoencoder = cell_line_autoencoder\n",
    "        self.out_activation = out_activation\n",
    "        \n",
    "    def forward(self, drug_features, cell_line_features):\n",
    "        drug_code, drug_reconstruction = self.drug_autoencoder(drug_features)\n",
    "        cell_line_code, cell_line_reconstruction = self.cell_line_autoencoder(cell_line_features)\n",
    "        \n",
    "        final_outputs = torch.sum(torch.mul(drug_code, cell_line_code), dim=1).view(-1, 1)\n",
    "        if self.out_activation:\n",
    "            return self.out_activation(final_outputs)\n",
    "        return final_outputs, drug_reconstruction, cell_line_reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep autoencoder with one hidden layer\n",
    "class DeepAutoencoderOneHiddenLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, code_dim, activation_func=nn.ReLU):\n",
    "        super(DeepAutoencoderOneHiddenLayer, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation_func(),\n",
    "            nn.Linear(hidden_dim, code_dim),\n",
    "            activation_func())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(code_dim, hidden_dim),\n",
    "            activation_func(),\n",
    "            nn.Linear(hidden_dim, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        code = x\n",
    "        x = self.decoder(x)\n",
    "        return code, x\n",
    "\n",
    "\n",
    "# Simple linear autoencoder\n",
    "class LinearAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinearAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Tanh())\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "# Simple linear autoencoder\n",
    "class LinearAutoencoderWithoutActivation(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(LinearAutoencoderWithoutActivation, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.456547008749136e-15 294.0 0.30624881699008477 206.979763195284\n"
     ]
    }
   ],
   "source": [
    "# Split data into train/val/test sets\n",
    "num_val_cell_lines = 100\n",
    "num_test_cell_lines = 100\n",
    "split_seed = 11\n",
    "samples_train, samples_val, samples_test, cell_lines_test, cell_lines_val = Dataset.samples_train_test_split(\n",
    "                                                                        response_df,\n",
    "                                                                        num_val_cell_lines,\n",
    "                                                                        num_test_cell_lines,\n",
    "                                                                        split_seed,\n",
    "                                                                        shuffle=True)\n",
    "# Normalize the data\n",
    "# Cell line data\n",
    "cols_subset = [col for col in list(cell_line_data_df) if col.endswith(\"_exp\")]\n",
    "rows_subset = [x for x in cell_line_data_df.index if x not in cell_lines_test + cell_lines_val]\n",
    "\n",
    "cell_line_data_df = Dataset.standardize_data(cell_line_data_original_df, cols_subset=cols_subset,\n",
    "                                            rows_subset=rows_subset)\n",
    "# Drug data\n",
    "drug_data_df = Dataset.standardize_data(drug_data_original_df)\n",
    "\n",
    "print(drug_data_df.mean().sum(), drug_data_df.std().sum(), \n",
    "      cell_line_data_df.mean().sum(), cell_line_data_df.std().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantianate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate drug and cell line autoencoders\n",
    "drug_input_dim, cell_line_input_dim = drug_data_df.shape[1], cell_line_data_df.shape[1]\n",
    "code_dim = 5    # Dimension of lower-dimensional drugs and cell lines representation\n",
    "drug_hidden_dim = 128   # Dimension of middle (hidden layer) in encoding network\n",
    "cell_line_hidden_dim = 128\n",
    "\n",
    "drug_autoencoder = DeepAutoencoderOneHiddenLayer(drug_input_dim, drug_hidden_dim, code_dim,\n",
    "                                                activation_func=nn.ReLU)\n",
    "\n",
    "cell_line_autoencoder = DeepAutoencoderOneHiddenLayer(cell_line_input_dim, cell_line_hidden_dim, code_dim,\n",
    "                                                activation_func=nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate complete model\n",
    "rec_system = RecSystemWithAutoencoders(drug_autoencoder, cell_line_autoencoder,\n",
    "                                      out_activation=torch.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 294]) True\n",
      "torch.Size([128]) True\n",
      "torch.Size([5, 128]) True\n",
      "torch.Size([5]) True\n",
      "torch.Size([128, 5]) True\n",
      "torch.Size([128]) True\n",
      "torch.Size([294, 128]) True\n",
      "torch.Size([294]) True\n",
      "torch.Size([128, 241]) True\n",
      "torch.Size([128]) True\n",
      "torch.Size([5, 128]) True\n",
      "torch.Size([5]) True\n",
      "torch.Size([128, 5]) True\n",
      "torch.Size([128]) True\n",
      "torch.Size([241, 128]) True\n",
      "torch.Size([241]) True\n"
     ]
    }
   ],
   "source": [
    "for p in rec_system.parameters():\n",
    "    print(p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 294])\n"
     ]
    }
   ],
   "source": [
    "drug_feats_test = drug_data_df.iloc[0].values.reshape(-1, 294)\n",
    "drug_feats_test = torch.from_numpy(drug_feats_test)\n",
    "print(drug_feats_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 241])\n"
     ]
    }
   ],
   "source": [
    "cl_feats_test = cell_line_data_df.iloc[0].values.reshape(-1, 241)\n",
    "cl_feats_test = torch.from_numpy(cl_feats_test)\n",
    "print(cl_feats_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = rec_system(drug_feats_test.float(), cl_feats_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU(inplace=True)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.ReLU"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReLU(inplace=True)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.ReLU(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (modeling_drug_efficacy)",
   "language": "python",
   "name": "modeling_drug_efficacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
